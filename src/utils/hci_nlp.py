# -*- coding: utf-8 -*-
"""HCI+NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Du_aYWLtAaMF01sqtpMwfDdxIslRzlNS
"""

import tensorflow as tf
import transformers
import numpy as np
import os
import glob
import json

from transformers import BertConfig, BertTokenizer, TFBertModel

bert = TFBertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

sample_text = 'With Harry Kane watching Rodri, when City’s centre-backs were in possession, either Bentancur or Hojbjerg would happily push forward 20 yards out of their midfield line to prevent forward passes, with the other dropping in. Tottenham were deep, but they weren’t passive.'

tokens = tokenizer(sample_text, return_tensors="tf")
model_output = bert(tokens)
cls = model_output.last_hidden_state[0, 0, :]

cls.numpy().shape

"""## Running PushShift"""

import requests
import time

sleep_time = 1
subreddit = 'fantasyfootball'
time_filter = int(time.time())
oldest_created_utc = time_filter
more_posts_exist = True
posts_per_file = 1000
output_dir = '/scratch/summit/dasr8731/needfinder'

url = "https://api.pushshift.io/reddit/search/submission/"

def get_string_representation(string) : 
	tokens = tokenizer(string, return_tensors="tf")
	model_output = bert(tokens)
	cls = model_output.last_hidden_state[0, 0, :]
	return str(list(cls.numpy()))

def parse_submission(submission) :
    attributes = ('url', 'upvote_ratio', 'title', 'subreddit', 'selftext', 'score', 'num_comments', 'created', 'created_utc', 'author', 'full_link')
    parsed = {}

    for attribute in attributes :
        
        if attribute in submission :  
            
            if type(submission[attribute]) == float: 
                parsed[attribute] = str(submission[attribute])
            else : 
                parsed[attribute] = submission[attribute]
            
        else : 
            parsed[attribute] = None

    parsed['bert_title'] = get_string_representation(submission['title'])
    parsed_str = json.dumps(parsed)
    return parsed_str

def get_filename(start_time, end_time) : 

	start_time = time.strftime("%Y%m%d%H%M%S",  time.localtime(start_time))
	end_time = time.strftime("%Y%m%d%H%M%S",  time.localtime(end_time))
	return os.path.join(output_dir , "{}_{}.jsonl".format(start_time, end_time))

def save_to_file(start_time, end_time, string_list) : 

	filename = get_filename(start_time, end_time)

	with open(filename, 'w') as f : 
		f.write("\n".join(string_list))

	return True

while more_posts_exist : 

    response = requests.get(url, params={
                            'subreddit':subreddit,
                            'before':time_filter, 
                            'limit':posts_per_file})

    parsed_results = []

    num_submissions = 0
    
    for submission in response.json()['data'] : 

        parsed_results.append(parse_submission(submission))

        if int(submission['created_utc']) < oldest_created_utc:
            oldest_created_utc = int(submission['created_utc'])

        num_submissions += 1
    
    print("Number of submissions : " ,  num_submissions)
    
   
    if num_submissions == 0 :
        more_posts_exist = False

    save_to_file(oldest_created_utc, time_filter, parsed_results)
    time_filter = oldest_created_utc
    print("Finished processing upto : " , time.strftime("%Y-%m-%d | %H:%M:%S",  time.localtime(time_filter)))

    time.sleep(sleep_time)

