<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao-Fei</forename><surname>Cheng</surname></persName>
							<email>haofeic@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Logan</forename><surname>Stapleton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Kawakami</surname></persName>
							<email>akawakam@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Venkatesh</forename><surname>Sivaraman</surname></persName>
							<email>vsivaram@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yanghuidi</forename><surname>Cheng</surname></persName>
							<email>yangcheng@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Diana</forename><surname>Qing</surname></persName>
							<email>dianaqing@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><forename type="middle">Perer</forename><surname>Kenneth</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Holstein</forename><surname>Zhiwei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Wu</surname></persName>
							<email>zstevenwu@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
							<email>haiyiz@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Perer</surname></persName>
							<email>adamperer@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kenneth</forename><surname>Holstein</surname></persName>
							<email>kjholste@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Steven</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University University of Minnesota Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<settlement>Minneapolis, Pittsburgh</settlement>
									<country>USA, USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Carnegie Mellon University University of California</orgName>
								<address>
									<settlement>Berkeley Pittsburgh, Pittsburgh, Berkeley</settlement>
									<country>USA, USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<settlement>Pittsburgh, Pittsburgh</settlement>
									<country>USA, USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3491102.3501831</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-07-22T05:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human-centered AI</term>
					<term>machine learning</term>
					<term>algorithmic biases</term>
					<term>algorithmassisted decision-making</term>
					<term>child welfare</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning tools have been deployed in various contexts to support human decision-making, in the hope that human-algorithm collaboration can improve decision quality. However, the question of whether such collaborations reduce or exacerbate biases in decision-making remains underexplored. In this work, we conducted a mixed-methods study, analyzing child welfare call screen workers' decision-making over a span of four years, and interviewing them on how they incorporate algorithmic predictions into their decision-making process. Our data analysis shows that, compared to the algorithm alone, workers reduced the disparity in screen-in rate between Black and white children from 20% to 9%. Our qualitative data show that workers achieved this by making holistic risk assessments and adjusting for the algorithm's limitations. Our analyses also show more nuanced results about how human-algorithm collaboration afects prediction accuracy, and how to measure these efects. These results shed light on potential mechanisms for improving human-algorithm collaboration in high-risk decision-making contexts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have seen the deployment of AI-based tools either to augment or replace human judgments across a growing range of high-impact decision-making contexts, such as social work, criminal justice, hiring, healthcare, and education <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b74">72,</ref><ref type="bibr" target="#b100">95,</ref><ref type="bibr" target="#b103">98]</ref>. These technologies have often been adopted under the logic that they are more accurate and equitable than human decision makers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b58">58]</ref>. Prior work suggests that on various predictive tasks, AI systems are more accurate than human decision makers <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b93">89]</ref>. However, in many social decision-making contexts, such as recidivism risk assessment, AI systems have been shown to inherit human biases from historical data, and perpetuate discrimination against already vulnerable populations, e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b92">88]</ref>. Prior attempts to make these algorithms less discriminatory have largely focused on the technical design of the algorithms-a central focus of the area of algorithmic fairness, e.g. <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b41">41]</ref>. Another possible approach to improving fairness in decision-making may be through human-AI collaborations, aimed at combining strengths and mitigating limitations in both AI-based and human decisions <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b89">86,</ref><ref type="bibr" target="#b102">97]</ref>. In some contexts, human-AI collaboration has demonstrated potential to improve fairness and efectiveness of decision-making, compared with human or AI decision-making alone, e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b73">71]</ref>. However, empirical results have been mixed. For example, in a real-world pretrial criminal context, human-AI collaboration was shown to exacerbate discriminatory decision-making <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">84]</ref>.</p><p>In this paper, we examine how call screen workers in Allegheny County, Pennsylvania use the AI-based Allegheny Family Screening Tool (AFST) to make decisions about which reports of child abuse or neglect (henceforth referrals) to investigate. Similar to De-Arteaga et al. <ref type="bibr" target="#b23">[23]</ref>, we compare automating versus augmenting decision-making with the AFST. Note that the AFST was designed to "augment the human decision whether to investigate a call" <ref type="bibr" target="#b14">[14]</ref> and Allegheny County Ofce of Children, Youth and Families leadership assures that the tool is never used to completely automate decisions <ref type="bibr" target="#b4">[5]</ref>. However, critics such as Eubanks <ref type="bibr" target="#b31">[31]</ref> and the National Coalition for Child Protection Reform <ref type="bibr" target="#b66">[66]</ref> worry that the AFST, and other tools like it, may someday be used to automate decisions, for example, as an austerity measure. In this paper, we investigate the efect that such a hypothetical automated decisionmaking policy would have on racial disparities in child welfare call screening. We then compare this policy to the current standard decision-making process, where workers make decisions with the assistance of AFST. Since child welfare workers themselves are known to make racially disparate decisions <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b54">54]</ref>, it is unclear whether adding them back "in the loop" will do any good in this regard. The central question of this paper is thus: when people work with algorithms in a child welfare context that is known to have racial disparities, will they serve to mitigate or exacerbate disparities? The answer to this question can inform the responsible design and use of AI tools in the child welfare context, as well as other high-stakes social decision-making contexts.</p><p>The primary racial disparity measure we use in this paper is the diference in the screen-in rates across Black and white children.</p><p>Where prior work has emphasized the impact that the AFST has had on caseworkers' workloads <ref type="bibr" target="#b35">[35]</ref>, we think it is also important to focus on the impact that the AFST has on families in Allegheny County. Call screening is most often the frst point of contact with the child welfare system, where an agency decides whether to intervene into the life of a family by investigating them. Being screened in may lead to more child welfare involvement into a family's life. AFST documentation states that "screening in and a child protection investigation has some potentially deleterious efects on families. If screening in, however, is a prerequisite to being ofered higher quality services or being prioritized for a slot in a desired program, one can argue the benefts of an investigation" <ref type="bibr" target="#b96">[92]</ref>. Higher screenin rates indicate higher levels of state intervention into families' lives, starting with investigation. Racial disparities in screening rates indicate uneven application of interventions or investigations of Black and white families and potentially uneven distribution of the potential harms and benefts of them. Disparities in screening rates may be unjustifed if they occur because of unwarranted intervention or lack of intervention, e.g. investigating a family when their child is not at imminent risk of abuse or neglect. As Dorothy Roberts <ref type="bibr" target="#b26">[26]</ref> suggests, "[t]he disproportionate number of Black children under state supervision results from discriminatory decision-making within the system as well as racist institutions in the broader society. " However, as we will discuss below, disparities may be justifed: higher need or higher risk of abuse or neglect among children in one group of people could warrant a higher screen-in rate.</p><p>In summary:</p><p>• Through quantitative analysis based on the two years of data immediately following the introduction of the AFST-from August 2016 to July 2018-we evaluated racial disparities in AFST-only and worker-AFST screening decisions. Our results show that worker-AFST decision-making served to reduce the disparity in screen-in rate between Black and white children compared to algorithm-only decisions. • We conducted a contextual inquiry by observing how call screen workers use the AFST to help them make decisions, and we interviewed workers about their experiences working with the algorithm. Through qualitative analysis, we fnd that by assessing referrals holistically using all of the information available to them and by adjusting for the algorithm's limitations, call screen workers disagreed with the AFST in ways that serve, in aggregate, to reduced the impact of racial disparities in the algorithm. Our fndings suggest that the AFST did not supplant call screening discretion and decision processes and that workers were not blindly following the AFST, consistent with the statement from Allegheny County Department of Human Services <ref type="bibr" target="#b4">[5]</ref> responding to Eubanks <ref type="bibr" target="#b31">[31]</ref>. • We also analyzed the accuracy of AFST-only and worker-AFST decisions. Although the AFST is better than workers at predicting the outcomes that it is trained to predict, our qualitative fndings indicate that workers make screening decisions to optimize for fundamentally diferent outcomes than the AFST. • Finally, we provide design implications for potential ways to improve the collaboration between call screen workers and the AFST in improving the decision-making process.</p><p>The AFST is just one algorithmic system used in child welfare; there are many similar systems used in child welfare agencies across the U.S. <ref type="bibr" target="#b80">[78,</ref><ref type="bibr" target="#b84">81]</ref>. We anticipate that several of our fndings may generalize to other public sector, algorithm-assisted decision-making contexts. However, agencies are often reticent about their internal policies, decision-making, and even public information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b82">79]</ref>, making it challenging to conduct similar analyses across other contexts. We thank the Allegheny County Ofce of Children, Youth and Families (CYF) for their continued interest in working with external researchers, and for their transparency in providing us data and access to their facilities. We also thank the workers in the Intake Department for taking the time to speak with us, and for sharing their insights. We hope that more agencies will follow Allegheny County's lead in opening opportunities for public and research accountability.</p><p>Finally, the data used throughout this paper contained information on 39,429 children who were referred to CYF. We acknowledge all 39,429 of these children and their families, on whom this data was collected and for whom this data refects potentially consequential interactions with CYF.</p><p>2 RELATED WORK 2.1 Algorithm-in-the-loop decision-making "Algorithm-in-the-loop" decision-making is commonly characterized by frst having an algorithm-produced prediction or classifcation, with a human making the fnal decision after considering an algorithmic recommendation <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b88">85]</ref>. Algorithm-in-the-loop decision-making has been observed in multiple high-stakes scenarios, including pretrial bail decisions <ref type="bibr" target="#b2">[3]</ref>, recidivism predictions <ref type="bibr" target="#b28">[28]</ref>, predictive policing <ref type="bibr" target="#b75">[73]</ref>, and diagnosing patients in clinical settings <ref type="bibr" target="#b62">[62]</ref>.</p><p>As algorithm-in-the-loop decision-making becomes increasingly common in practice, recent research has started to look at how humans work with algorithms when making decisions and at the relative contributions of humans versus algorithms to overall performance. Many studies have focused on prediction accuracy, fnding that on many tasks, algorithms can outperform humans in terms of prediction accuracy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b104">99]</ref>. However, recognizing that human experts and algorithms may have complementary strengths and limitations, a line of research has sought to understand how to combine the capabilities of each <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b89">86,</ref><ref type="bibr" target="#b102">97]</ref>. Some studies have demonstrated that combinations of human and algorithmic judgment can improve prediction and/or decision-making (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b73">71]</ref>). Yet empirical results in this space have been varied so far. In other studies, human-algorithm decisionmaking has failed to improve or has even harmed performance, compared with either human or algorithmic decisions alone (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b76">74,</ref><ref type="bibr">84,</ref><ref type="bibr" target="#b104">99]</ref>).</p><p>Beyond accuracy, other metrics have been used to evaluate between decisions made by humans, AI, and human-AI combined <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b76">74]</ref>. Much of this work focuses on minimizing the error of the decisions compared to the ground truth. Most relevant to our work, Albright <ref type="bibr" target="#b2">[3]</ref> compares racial disparities in human-only and human-AI decisions in the context of pretrial bail hearings. Here, judges are presented with a risk score and recommendation from a risk assessment algorithm (similar to the AFST) and then must decide whether to give a person who is charged with a crime bail (and keep them in jail until they can pay) or let them go free without paying bail until their trial date. Albright <ref type="bibr" target="#b2">[3]</ref> suggested that judges disagreed with the algorithmic recommendations for certain types of defendants such that the judge-algorithm bail decisions were more racially disparate-giving Black people bail rather than letting them free without bail at a higher rate than white peoplethan both the past judge-only decisions (before the algorithm was implemented) and the algorithm-only recommendations in the same time period. In our work, we present empirical results in the opposite direction: in the context of child welfare call screening, we found that human-algorithm decisions were less racially disparate than algorithm-only decisions would have been, and somewhat less racially disparate than past human-only decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Risk Assessment Tools in Child Welfare</head><p>For an overview of predictive algorithms used in child welfare, see <ref type="bibr" target="#b84">[81]</ref> and <ref type="bibr" target="#b80">[78]</ref>. We provide a brief history here. For decades, child welfare agencies across the U.S. and abroad have been using risk assessment instruments (RAIs) to assist child social workers in making decisions, such as whether or not to investigate a family or whether to remove a child from their family. Most RAIs have been checklists that workers fll out in order to estimate the risk of child maltreatment. For example, see the Structured Decision Making (SDM) tools used in the California Child Protective Services system and a number of other locales <ref type="bibr" target="#b71">[70]</ref>. For a case study of other kinds of RAIs and algorithms used in child welfare, see <ref type="bibr" target="#b83">[80]</ref>. However, newer RAIs include automated tools, commonly called predictive analytics or data-driven predictive tools, which use statistical modeling and machine learning to estimate risk based on historical administrative data. Earlier iterations of these were developed by private companies, such as Eckerd Connects [16], MindShare Technology <ref type="bibr" target="#b90">[87]</ref>, or SAS <ref type="bibr" target="#b46">[46]</ref>. Due to high error rates and their "black box" nature, the Los Angeles County and Illinois child welfare systems dropped private algorithms after brief trials <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b64">64]</ref>. As such, these private tools have fallen out of favor, though some were still in use in other locations at the time of publication. Other data-driven predictive tools have been or are being developed through public-academic partnerships-such as the Allegheny Family Screening Tool (AFST) in Allegheny County, Pennsylvania <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b96">92]</ref> or other tools being implemented by the same designers in Douglas County, Colorado <ref type="bibr" target="#b95">[91]</ref> and Los Angeles County <ref type="bibr" target="#b78">[76]</ref>. 1 These publicly-developed algorithms have proved more resilient, with the AFST being the longest-lasting and most prominent predictive tool in use today. Proponents of these newer automated risk assessment tools, such as the AFST, argue that they make more accurate decisions than both child social workers and standard checklist-based RAIs; and that they make more consistent, objective, and equitable decisions <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b85">82]</ref>. Some critics argue that these automated tools are still too inaccurate, that they do not predict true child abuse or neglect, and that they still make biased decisions because they are trained on biased data <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b65">65]</ref>.</p><p>In order to evaluate the AFST based on these opposing concerns, the Allegheny County CYF commissioned Goldhaber-Fiebert and Prince <ref type="bibr" target="#b35">[35]</ref> to conduct an Impact Evaluation of the AFST. Among the results, the most relevant to this paper were the following: 1) the AFST "increased accuracy for children screened-in for investigation and may have slightly decreased accuracy for children screened-out;" 2) the AFST did not decrease the screen-in rate overall; and 3) the AFST reduced racial disparities in terms of the screen-in rate, but possibly worsened them in terms of accuracy <ref type="bibr" target="#b35">[35]</ref>. These fndings called into question whether the AFST improves either accuracy or equity. These results were based on an analysis of two years of data immediately following the introduction of the AFST-from August 2016 to July 2018. Our quantitative fndings are based on the same data as Goldhaber-Fiebert and Prince <ref type="bibr" target="#b35">[35]</ref>, which we preprocessed and analyzed to match their work, as well. Rather than reiterate their fndings, we use this data to evaluate the efects that automating screening decisions would have on racial disparities. We also expand upon Goldhaber-Fiebert and Prince <ref type="bibr" target="#b35">[35]</ref>'s fndings with a new mixed-methods approach. It should also be noted that some critics oppose not only automated predictive tools in child welfare, but also the discourse around accuracy, fairness, accountability and transparency that our paper contributes to, which "does not address the core structural issues at work" with these tools <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b79">77]</ref>. Abdurahman <ref type="bibr" target="#b1">[2]</ref> explicitly names and critiques the central question of our paper-namely, "Does the Allegheny Family Screening Tool (AFST) produce fair outcomes?" We still see some merit in "adjudicating [the] downstream impact" of the AFST <ref type="bibr" target="#b1">[2]</ref>. However, we recognize that our paper is limited in that it does consider the larger political economic or social contexts in which these tools are deployed, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b79">77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Study Context</head><p>In this paper, we studied a high-stakes scenario of child maltreatment referral screening decisions. The Allegheny County Ofce of Children, Youth, and Families (CYF) has been using an algorithmic tool, the Allegheny Family Screening Tool (AFST), to assist with child maltreatment call screening process since August 2016. The AFST is a machine learning-based predictive risk modeling tool that analyzes county data to predict outcomes related to child abuse or neglect. The AFST Version 1 used demographic data related to the alleged victims, caretakers, alleged perpetrators, prior child welfare history, criminal history, public behavioral health history, and use of public assistance <ref type="bibr" target="#b96">[92]</ref>. 2 Prior to the introduction of the AFST, call screen workers made all referral decisions without any algorithmic aids. Since its deployment, workers have been presented with an AFST risk score to assist with their call screening decisions for all referrals which were not automatically screened in or out (henceforth referrals or discretionary referrals). 3 Call screen workers still make the fnal decisions-they have the option to either agree with the algorithm recommendation, or to disagree and go with their own decisions. We adopted a mixed-methods approach to investigate how CYF call screen workers work with the AFST, and how the resulting human-AI decisions afect disparities in decision outcomes. We analyzed historical data on call screening decisions prior to and after the deployment of the AFST algorithm. We also conducted 2 This was true for the frst deployed version of the tool. The second version (in use from November 2018 until the time of publication) stopped using public assistance as a predictive feature and started using birth records <ref type="bibr" target="#b97">[93]</ref>. For a full list of variables used by the AFST Version 1, see pages 37 to 44 of the documentation <ref type="bibr" target="#b96">[92]</ref>. 3 We provide more detail about which referrals are discretionary or not in Section 3.2.1. contextual interviews with caseworkers and supervisors to support interpretation of fndings from these quantitative analyses.</p><p>3.1.1 Use of the AFST in call screening. The AFST Version 1 (used from August 2016 to November 2018) was made up of two modelsthe re-referral model was trained to predict whether a child would be reported again within two years of being screened out; the placement model was trained to predict whether a child would be removed from their home and placed in foster care within two years of being screened in. Each model produces a risk score ranging from 1 (lowest risk) to 20 (highest risk) associated with the likelihood of the corresponding predicted outcome (re-referral or placement) for each child in the referral. The score is categorized visually into 3 bins: Low risk (score 1-9), Medium risk (score 10-14), and High risk (score <ref type="bibr" target="#b15">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref>. The caseworker and supervisor sees a single risk score (see Figure <ref type="figure" target="#fig_1">2</ref>) associated with the referral. The presented score is the higher of the scores across the two models. If the referral involves multiple children, the score of the child with the highest score in the referral would be shown. 4  Workers used the AFST as follows: each caseworker frst gathered information about the referral, made assessments of risk and safety, 5 made a screening recommendation for the referral, then ran the AFST to generate a risk score and passed the report to a supervisor. The supervisor then reviewed the report on the referral, which included its AFST score, then made the fnal decision to screen in and investigate the family or not. For all discretionary referrals, the AFST score served as a recommendation; workers had the authority to either agree or disagree with that recommendation when making the fnal screening decision. In Appendix D, we suggest that CYF workers consider High risk referrals <ref type="bibr" target="#b15">(15)</ref><ref type="bibr">(16)</ref><ref type="bibr" target="#b17">(17)</ref><ref type="bibr" target="#b18">(18)</ref><ref type="bibr" target="#b19">(19)</ref><ref type="bibr" target="#b20">(20)</ref> to be recommended screen-in, Low risk (1-9) recommended screen-out, and Medium risk (10-14) sans recommendation. However, 29.3% of children in discretionary referrals from August 2016 to May 2018 had a placement model score of 18 or above and were fagged as mandatory screen-in <ref type="bibr" target="#b96">[92]</ref>. For these referrals, workers were shown the AFST interface on the right side of Figure <ref type="figure" target="#fig_1">2</ref>. These referrals were "required to be screened in," but supervisors were able to "override this requirement at their discretion" provided that they "documented and reviewed" their reasons for overriding this requirement <ref type="bibr" target="#b96">[92]</ref>. Supervisors overrode these decisions and screened out 21.0% of children labeled mandatory screen-in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data analysis</head><p>3.2.1 Data. We acquired data from Allegheny County CYF about all children who were referred to CYF from January 2015 to July 2018. The data contains referrals from both before the deployment of the AFST Version 1 (January 2015 -July 2016) and after (August 2016 -July 2018). We excluded all referrals which were automatically screened in or out, since these non-discretionary referrals would not have been infuenced by the AFST. Thus, we excluded all Child Protective Services (CPS) referrals, 6 referrals with active cases, and referrals with completed cases. We excluded referrals which were labeled both CPS and General Protective Services (GPS) in the data. We also excluded all cases which did not include white nor Black children, according to our defnitions in Section 4.2. After preprocessing, the data used in our quantitative analyses included GPS referrals without active or completed cases which included white or Black children from January 1, 2015 to <ref type="bibr">May 13, 2018.</ref> We also used AFST scores which were generated retrospectively for the entire time period, which-due to a technical glitch that led the AFST to produce erroneous scores for a subset of referrals during the frst year and a half of deployment-means that in some cases the scores we use in our analysis were not the scores that workers were shown from August 2016 until December 2017 <ref type="bibr" target="#b23">[23]</ref>. Also, workers were not shown AFST scores from any referral from January 2015 to July 2016, since the AFST was not deployed until August 1, 2016. After December 2017, the AFST scores used in our analysis were the same as those shown to workers. We use the corrected AFST scores instead of the scores shown to workers to more accurately portray the screen-in rate of a hypothetical automated AFST-only policy.</p><p>Each entry in the data corresponds to an individual child who was referred at one time. Each child and each referral were associated with unique IDs. If a family with only one child was referred to CYF three times, this would correspond to three diferent entries in the data with the same child ID but three diferent referral IDs. If a family with two children was referred to CYF once, this would also correspond to two diferent entries with two diferent child IDs but only one referral ID. Each entry included the AFST risk score generated for each child in each referral, and the fnal call screening decisions made by the call workers for that referral. 7 Throughout our analysis, we report statistics and percentages in terms of entries, where one entry represents a unique child in a unique referral. 8 For shorthand throughout the paper, however, we describe numbers and percentages in terms of children. For example, when we write that 71.0% of Black children were screened-in, this really means that 71.0% of entries containing Black children in discretionary referrals were not labeled screen out. After preprocessing, our data contains 31,025 entries before the deployment of the AFST Version 1, which include information on 23,230 unique children in 15,179 unique referrals; the preprocessed data from after the deployment of the AFST contained 51,750 entries on 33,613 children and 24,250 referrals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contextual Inquiry and Interviews</head><p>To support interpretation of our quantitative fndings, we also conducted contextual interviews with call screen caseworkers and supervisors. A group of researchers visited the Intake Department of Allegheny County CYF in July 2021. The visit consisted of two parts: 1) contextual inquiries, where the researchers observed how call screen workers worked with the AFST when making screening decisions; and 2) semi-structured interviews where researchers were able to ask more in-depth questions. We observed and interviewed 13 participants in total: 9 call screen caseworkers and 4 supervisors over 2 separate visits in a span of two weeks. All participants worked full time as call screen caseworkers or supervisors. We checked with workers before and during our visits to make sure that we did not burden them too much while they were busy with work. To prevent workers from being identifed in the workplace, all responses in the paper are anonymous and we report only minimal demographic information. Participants included, but may not have been limited to, white workers, Black workers, women, and men. At the request of the ofce, we did not provide monetary compensation to the participants. This study was approved by the Institutional Review Board of Carnegie Mellon University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Contextual Inquiry.</head><p>After obtaining consent from participants, we observed call screen caseworkers and supervisors in their normal workfow, and participants were encouraged to think aloud as they performed their tasks, to make more of their thinking and reasoning visible. See Figure <ref type="figure" target="#fig_0">1</ref> for a visual diagram of the overall workfow. For caseworkers, this included taking phone calls from reporting sources, gathering information for reports, running the AFST to produce a risk score and recommendation, and making the call screening recommendation. For supervisors, this included reviewing the reports made by caseworkers, correcting information in reports (if need be), requesting feld screening to gather missing information about a referral (if need be), making the fnal screening decisions, and overriding the mandatory screen-in referrals that the algorithm enforces. 9 Each contextual inquiry session took about three hours. Researchers took notes on the actions and thought processes of the participants, while asking brief follow-up questions as needed. Due to the sensitive nature of the work, we neither audio recorded the contextual inquiry nor took notes on any personally identifable information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Semi-structured Interviews.</head><p>After the contextual inquiry, we invited each caseworker and supervisor for a semi-structured interview. The goal of the interview was to understand how participants incorporated the AFST in their decision-making process, and in particular, to gain further insight into possible mechanisms underlying our quantitative fndings. At the beginning of each interview, we discussed participants' background and experience in child welfare. We also asked any follow-up questions that arose from our observations during the contextual inquiry, including clarifying questions about specifc referrals or about their day-to-day workfow. We shared statistics about racial disparities in call screening similar to those in Figures <ref type="figure" target="#fig_3">3, 4</ref>, and 5 which compared the AFST recommendations to actual decisions from 2016 to 2018, and asked workers' thoughts about these numbers. We then asked participants how they worked with the AFST to ensure fairness in screening decisions. Lastly, we discussed potential improvements to the design and use of the AFST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Qalitative Analysis.</head><p>We frst transcribed all interview recordings into text, and used thematic analysis <ref type="bibr" target="#b6">[7]</ref> to analyze our data, a constructivist approach inspired by grounded theory <ref type="bibr" target="#b13">[13]</ref>. We combined the data collected from both the contextual inquiry and interviews, which contains interview transcripts and feld notes. The authors collaboratively conducted open coding on the data, which generated over 1500 open codes. The authors then conducted an iterative afnity mapping process to the open codes, performing constant comparisons and iteratively clustering related codes. In the end, the authors refned the themes that emerged from the afnity mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Positionality</head><p>We authors acknowledge that our positionality shapes our approaches to research, as well as how we interpret and present our fndings. Given that the subject of research involves how Black and white families have been treated by the child welfare system in Allegheny County, Pennsylvania, we think it is especially important to acknowledge our racial/ethnic backgrounds, where we live, and our relationships to child welfare. The two lead authors are Asian and white, respectively. The rest of the authors self-described their racial/ethnic backgrounds as Asian, Asian American, Caucasian, Chinese, Filipino and White, in alphabetical order. None of us authors are Black. All but two of us live in Allegheny County; the other two live in Minnesota and California. None of us have been investigated by a child welfare agency, nor were any of us adopted nor involved in the foster care system as children. Throughout this work, we collaborated with Allegheny County CYF in order to gain access to data and to talk with workers, although the analysis and writing were conducted independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DECISION-MAKING PARADIGMS AND TERMINOLOGY</head><p>In this section, we defne terminology used throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decision-making paradigms</head><p>In this paper, we compare hypothetical screening decisions made by the AFST alone versus actual decisions made by child welfare workers when using the AFST. We defne these two decision-making paradigms as follows:</p><p>(1) AFST-only decisions: The hypothetical screening decisions that the AFST algorithm would make if it were the only decision-maker (without workers). For clarity, we suppose that the AFST would screen in any High risk referrals (with a score of 15 to 20) and screen out any Medium and Low risk referrals (scores 1 to 14). We choose a threshold of 15 for our AFST-only policy because it is the threshold between Medium and High risk AFST labels presented to the call screen workers, and because its hypothetical screen-in rate would be close to the actual screen-in rate from 2016 to 2018. This split between High and Low risk referrals follows ofcial AFST documentation, which discusses disparities: "up until the end of 2017, 47% of black children received a 'high'-range score <ref type="bibr" target="#b15">(15)</ref><ref type="bibr">(16)</ref><ref type="bibr" target="#b17">(17)</ref><ref type="bibr" target="#b18">(18)</ref><ref type="bibr" target="#b19">(19)</ref><ref type="bibr" target="#b20">(20)</ref>, compared to 39% of white children. Conversely, 29.6% of white children have received a 'low'-range score (1-9), compared to 10% of black children" <ref type="bibr">[68, p.11]</ref>. See Appendix D for evidence that call screen workers see High risk labels as screen-in recommendations and Low risk as screen-out. However, we also conducted sensitivity analyses by replicating our empirical results across alternative thresholds. For example, Figure <ref type="figure" target="#fig_6">6a</ref> shows similar screen-in rate disparities across thresholds from 10 to 20. (2) AFST-assisted worker decisions: The actual call screening decisions made by child welfare workers from 2016 to 2018, assisted by the AFST (see Section 3.1.1 for a detailed description of the decision-making process). For brevity, we refer to these decisions as worker-AFST decisions throughout the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Defnitions of Black and white children</head><p>In the data, each child had one or more of the CYF race labels: "Black, " "'white, " "Hispanic, " "Asian, " "Native American, " "other, " or "unknown. " For our quantitative analyses in this paper, we considered a child Black if they were assigned the CYF race label "Black" alone or "Black" with any other CYF race label. We considered a child white only if they were labeled "white" only -i.e. if the child were labeled "white" plus any other CYF race label, they were not considered white in our quantitative analyses. We considered a child with the CYF labels "Black" and "white" as Black. This follows the same racial classifcation as the ofcial AFST Impact Evaluation <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Our primary research questions to evaluate are: 1) how worker-AFST decisions afected racial disparities in call screening, and 2) whether changes in disparities afected the decision accuracy. We adopt the following evaluation metrics:</p><p>• Racial disparity The primary disparity measure used in this paper is the difference in the screen-in rate between Black children versus white children. Diferences in the screen-in rates between diferent racial groups correspond to one of the simplest and most popular algorithmic fairness notions-statistical parity, e.g. <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">32]</ref>. Specifcally, a classifer satisfes statistical parity if the subjects in the protected and unprotected groups have equal probability of being assigned to the positive predicted class. While statistical parity serves as a starting place for our analyses, to assess the robustness of our results we also evaluate the disparities in other metrics, including accuracy, precision, true positive rate and false positive rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Accuracy</head><p>We measured accuracy by the percentage of decisions that aligned with a proxy ground truth: for screen-in decisions, we measured the percentage of children that were either removed from their home within 2 years or re-referred again within 2 months of the referral; for screen-out decisions, we measured the percentage of children who were neither removed from their home within 2 years nor re-referred again within 2 months of a referral. 10 Our defnition of accuracy difers slightly from prior work on the AFST, in which a screen-in decision was accurate only if the child was later placed in foster care within 2 years and a screen-out decision was accurate only if the child was not re-referred within 2 years <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b96">92]</ref>. We adopt the former defnition of accuracy so that our hypothetical AFST-only accuracy is a decent estimate of what the accuracy of screening decisions would have been had they actually been automated by the AFST-only policy.</p><p>Our measurement of AFST-only accuracy is an imperfect estimate, due to its counterfactual predictions <ref type="bibr" target="#b18">[18]</ref>. For any referrals where the AFST-only decision difers from the actual screening decision, ideally we would want want to measure the AFST-only accuracy in terms of counterfactual outcomes -e.g. whether a child would have been placed in foster care or re-referred had they been screened in (when in reality they were screened out). Since we do not know these counterfactual outcomes, we evaluate AFST-only accuracy based on the actual predictive outcomes instead. However, because the screening decisions afect the predictive outcomes, the actual outcomes may have diferent probabilities than the counterfactual ones. For example, if the AFST-only decision is screen in, its accuracy will be judged based on whether the child is re-referred or placed; but, if they are actually screened out, we assume (but do not know) that the child is less likely to be placed and more likely to be re-referred than if they were actually screened in. This assumption may distort our measurement of accuracy for the hypothetical AFST-only decisions. As Coston et al. <ref type="bibr" target="#b18">[18]</ref> note, this is a limitation endemic to risk assessments where the predictions afect the predictive targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Other metrics</head><p>We also evaluated disparities and prediction performance for a few additional metrics. We defer these results and the necessary terminology to the Appendix. Table <ref type="table">1</ref>: Total number of children that the AFST-only would have hypothetically screened in and out, compared to the actual decisions made by workers aided by the AFST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">HOW AFST-ASSISTED WORKER DECISIONS AFFECTED SCREEN-IN RATE DISPARITIES 5.1 Black-white disparities between AFST-only decisions and workers' fnal screening decisions</head><p>In this section, we look at the disparity in screen-in rates among Black and white children who were referred to CYF for AFST-only and worker-AFST screening policies. In Figure <ref type="figure" target="#fig_2">3</ref>, we see the difference in screen-in rates between white and Black children under these two screening policies for all children reported to CYF from August 2016 to July 2018 who were not immediately screened in or out. 11 The screen-in rates in Figure <ref type="figure" target="#fig_2">3</ref> are calculated over the number of discretionary referrals within the race listed, e.g. the 71% AFST-only screen-in rate for reports with Black children means that 71% of reports with Black children would have been screened in from 2016 to 2018 following the AFST-only screening policy. The Black-white screen-in rate disparity refers to the diference between the screen-in rate for Black children and the screen-in rate for white children under a given policy for this time period. For example, the AFST-only Black-white screen-in rate disparity from 2016 to 2018 was 20%, because 71% of all referrals with Black children would have been screened in and 51% of all white children would have been screened under the AFST-only screening policy. 12 We calculate that the worker-AFST screen-in rate disparity for the same time period was 9%, since 61.8% of Black children and 52.8% of white children were screened-in. These results suggest that the Black-white screen-in rate disparity under the worker-AFST screening policy was less than half than that of the AFST-only policy from August 2016 to May 2018, 11% lower to be exact. We also examined Black-white screen-in rate disparities using other decision thresholds for the hypothetical AFST-only policy, as well as additional disparity metrics. We found that worker-AFST decisions were less disparate than AFST-only decisions, regardless of which threshold was chosen. It should be noted that the threshold of 15 that we default to throughout the paper has the second-to-highest disparity of any threshold from score 10 to 20 (including "mandatory screen-in"). We default to threshold 15 not to overstate our results, but because we argue that it would be the most reasonable threshold to choose from, given the design of the AFST which splits referrals into High and Medium risk referrals at 15, and given that a score of 15 produces an AFST-only overall screen-in rate comparable to the actual screen-in rate, whereas other thresholds do not. With some exceptions, we observe similar patterns in precision rates, true positive rates, and false positive rates-worker-AFST decisions exhibit less racial disparity in these metrics, as compared to AFST-only decisions defned by all thresholds from score 10 and above. See Appendix C for our complete analysis.</p><p>How workers disagreed with the AFST to reduce the Blackwhite screen-in rate disparity. Figure <ref type="figure" target="#fig_3">4</ref> shows the percentage of children which would have been screened in and out under both the AFST-only and worker-AFST policies, broken down by race. Recall that the Black-white screen-in rate disparity under the AFST-only screening policy was 20%. The simplest way that workers could have reduced this disparity would have been to screen in more white families and screen out more Black families than the AFSTonly policy would have. Overall, this is what we observed: however, workers did not disagree with the AFST-only policy exclusively in ways that would have lessened this disparity. If workers were heavily guided by the AFST, but intentionally tried to reduce its screen-in rate disparity, we might expect that workers would not have screened in Black families that the AFST-only would have screened out, and that workers would not have screened out white families that the AFST-only would have screened in. However, we see in Figure <ref type="figure" target="#fig_3">4</ref> that this is not the case: workers screened out 15.9% of Black children who would have been screened out under the AFST-only policy and screened out 17.6% of white children who would have been screened in under the AFST-only policy. Because we see disagreement across the board, it is likely that workers are using their best judgment across the board, and they are not simply following the AFST's recommendations. It is also likely that workers are not making screening decisions explicitly in order to reduce racial disparities. These interpretations align with our qualitative fndings in Section 5.2.</p><p>What ultimately led to the Black-white screen-in rate disparity being lower under the worker-AFST policy than under the AFST-only policy is shown in Figure <ref type="figure" target="#fig_3">4</ref>. Among children that the AFST-only policy would have screened in, call screen workers screened out more Black children than white children (22.3% vs 17.6%). Among children that the AFST-only policy would have screened out, workers screened in less Black children than white children (13.1% vs 29.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative: How did workers achieve lower</head><p>Black-white screen-in rate disparity?</p><p>When reviewing a referral, workers can see the races of everyone involved. In theory, one (naive) way workers could have reduced the screen-in rate racial disparity is by simply looking at the race of the children in the referral, screening in more Black children and screening out more white children, regardless of the other factors involved in the referral. Based on both our quantitative results (Section 5.1) and our qualitative fndings, it is clear that this was not the case: workers were making decisions in a more sophisticated way. Furthermore, four call screen caseworkers and three supervisors explicitly said they did not make screening decisions based on the race of the family. 13 Two caseworkers and three supervisors said</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mechanisms Details Holistic decisions</head><p>Workers considered AFST scores in the context of all the other information in a referral and made holistic, contextual assessments of risk and safety to make screening decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjusting for limitations of the AFST</head><p>Workers adjusted for what they perceived to be limitations of the AFST and disregarded the AFST's recommendations when they thought it over-or under-scored referrals.</p><p>Workers thought the AFST was over-or under-scoring certain referrals because it did not take the allegation or other current referral information into account properly. Workers thought that the AFST over-and under-scored referrals specifcally based on system involvement, i.e. welfare, public medical services, criminal history, or CYF history.</p><p>Workers compensated for what they thought were the AFST's racial disparities caused by systemic racial biases in CYF reporting and county data collection. Collaborative decisions Workers regularly made decisions collaboratively, both under standard procedures between caseworkers and supervisor, and impromptu between caseworkers. (1) Workers made screening decisions based on holistic, contextual assessments of risk and safety. (2) Workers adjusted for limitations that they perceived in the AFST when making decisions on a case-by-case basis. For some workers, this adjustment was a conscious adjustment to try to reduce racial disparities. For others, it was unintentional. However, the efect of reducing disparity in aggregate was the same. (3) Workers made collaborative screening decisions about some reports they were uncertain about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Holistic decision-making.</head><p>Workers made holistic, contextual assessments of risk and safety in order to determine whether to screen referrals in or out. Interview fndings: Four caseworkers and one supervisor said they did not consider the AFST's recommendation as a baseline to guide their fnal screening decision. Rather, workers said the AFST provided additional information to consider (two supervisors and three caseworkers). For example, one supervisor said, "[the AFST is] a good tool to have for some extra information, in terms of risk." Another supervisor said workers "take in consideration what the computer [i.e. the AFST] is saying, but,... we're not making a decision based on what the computer says. If the computer says, 'This score is a 16, ' per the computer we have to assign that family, and we're not doing that, we're using real... information to make the decision and not numbers to make the decision." This supervisor also said they screen reports based on "what risk factors are going to impact the children, because it's all about child safety. "</p><p>For call screen workers, safety refers to present danger or wellbeing of the children in a report; risk refers to the chance that the children will be harmed or neglected in the future. One supervisor said, "safety is more immediate. And risk is even more long-term." For example, one caseworker said living in a dirty home is a risk, but not an impending danger. Workers conduct holistic, contextual assessments when they consider any piece of information in a report to be relevant only when it's relevant to risk and safety in the context of all other information in the report (cf. <ref type="bibr" target="#b3">[4]</ref>).</p><p>Contextual Inquiry Observations: For example, we observed a caseworker review a referral where a single mother was reported by a friend for allegedly using drugs and generally neglecting her four children (ages fve to seventeen). The report also included pressing concerns about some of the children's dental health. The family was reported a month prior because one of the children was truant. The AFST score was 11 (medium, on the side of screening out), which this caseworker considered to be a low score. However, the caseworker recommended screening in the referral, because of the combination of the drug allegation, the presence of some young children, the past truancy case, and the dental health concerns. The caseworker said they would have screened out the referral if it had included only one or two of these risk factors without the others.</p><p>Overall: Because workers considered the AFST risk score in the context of all the other information in a referral, we often observed that they considered other information to be more relevant to risk and safety than the AFST score, and made screening decisions despite the score. We suspect that this holistic decisionmaking contributed to the baseline level of disagreement across both race and AFST-only screening decisions in Figure <ref type="figure" target="#fig_3">4</ref>. One supervisor expressed that they thought this kind of holistic, contextual decision-making led to less racially disparate screening decisions. However, this supervisor said they did so unintentionally and that the 11% reduction in screen-in rate Black-white disparity from 2016 to 2018 was "not the intention, just the outcome. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2</head><p>Adjusting for limitations of the AFST. Next, workers said they adjusted for what they perceived to be limitations of the AFST: they disregarded the AFST's recommendations when they thought it over-or under-scored referrals because it was unable to properly take all referral-relevant information into account. Instead, workers relied on factors that they believed were more relevant in a given referral to make a decision, as discussed below.</p><p>(1) Workers thought the AFST was over-or under-scoring certain referrals because it did not take the allegation or other current referral information into account properly. Interview fndings: For example, one caseworker said that the AFST does not consider the allegation in the report when determining the risk score, "even if it says, 'Dad killed Mom in front of the kids.' You know what I'm saying? Like, something crazy." One caseworker gave an example where the AFST over-scored a referral because it did not properly take into account the results of a recent investigation: "We just investigated and we found that the parents are providing fne... and we just closed it. Then some anonymous person reports the same thing. Then... [the AFST says] it's a high risk again, and we already just previously addressed it. "</p><p>Contextual Inquiry Observations: For example, we observed a caseworker review a referral where a mother was reported for allegedly not giving prescribed mental health medicine to her daughter. The caseworker immediately told us that this was a serious allegation. One prior report had also been for withholding medicine. The AFST score was a Low risk protocol (i.e. a mandated screen out). 14 The caseworker thought the AFST score was too low. The caseworker said they would override the AFST Low risk protocol and screen the current referral in, because of the seriousness of the current allegation and the one prior referral with a similar allegation.</p><p>(2) Workers thought that the AFST over-and under-scored referrals specifcally based on system involvement, i.e. welfare, public medical services, criminal history, or CYF history.</p><p>Interview fndings: Six caseworkers and three supervisors said that the AFST over-scored families with more system involvement and under-scored families with less system involvement. For example, one caseworker said that families who do not use public welfare or medical services get scored lower than families who do: "if you were poor and you're on welfare, you're gonna score higher than a comparable family who has private insurance. Because those people go to private therapists." 15 A caseworker said people who do not have a history of county involvement "could totally be away from Big Brother forever. " Workers' beliefs align with prior work which suggest that the AFST is biased towards poor people and people with system involvement <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b45">45]</ref>. We observed workers disregard the AFST score because they thought it was relying too heavily on system involvement and not taking relevant information into account.</p><p>Contextual Inquiry Observations: For example, we observed one caseworker review a report where they thought the AFST score was high because of system involvement, but they wanted to screen it out. A judge mandated that CYF look into the family after their child came to a juvenile probationary hearing. The child was currently incarcerated. The family had a number of prior referrals and used a lot of public behavioral health services (50+ times). Based on this history, this caseworker said "I know [the AFST score] is gonna be high. " When the caseworker ran it, the AFST score was a 20. However, the caseworker wanted to screen out the referral, because the child was already incarcerated, so it would be no use to investigate the family. The AFST over-scored this referral based on the family's history and neglected highly-relevant context: that the child was in custody of the state and thus would not need to be investigated.</p><p>Overall: Prior work suggests that Black people have higher rates of poverty and system involvement, and that this accounts for racial disparities in the child welfare system <ref type="bibr">[27,</ref><ref type="bibr" target="#b45">45]</ref>. Workers understood that the AFST was over-and under-scoring referrals based on system involvement and were correcting for that by disagreeing with the AFST's recommendations for these referrals. 16  Because system involvement is also correlated with race (Black families having more involvement than white families), this would account for the higher percentage of Black children than white children that the AFST-only decisions would have screened in but workers screened out (22.3% vs. 17.6%) and for the lower percentage of Black children than white children that the AFST-only decisions would have screened out but workers screened in (15.9% vs. 29.6%) as seen in Figure <ref type="figure" target="#fig_3">4</ref>. As stated in Section 5, this pattern of disagreement contributed to the 11% reduction in Black-white screen-in rate disparity from the AFST-only to worker-AFST decisions. In sum, because workers disregarded the AFST score more often when they perceived it to be over-or under-scoring based on system involvement (and not considering other relevant information), they were able to reduce racial disparities in call screening.</p><p>(3) Workers compensated for what they thought were the AFST's racial disparities caused by systemic racial biases in CYF reporting and county data collection.</p><p>Interview fndings: Five caseworkers thought AFST-only decisions were racially disparate not only because the AFST overand under-scored based on poverty and system involvement, but because of systemic racial biases in CYF reporting and in county information collected elsewhere-such as the medical system or the criminal system. These workers did not make screening decisions based on the race of the family, but they did consider the race of the family in order to account for systemic racial biases. These workers also thought that the AFST was biased because of over-reporting on Black families and systemic racism. One caseworker said that "white people are not reported as much as Black kids" and that they "get a lot of reports on African-Americans and a lot of them are bogus. Another caseworker also agreed with this: "I also think [the AFST is] very biased, but so is the world. " This caseworker continued, "the whole system is racially biased. ... It's the people entering the information [i.e. reporting families] that's afecting the [AFST] score. "</p><p>Contextual Inquiry Observations: For example, we observed a caseworker review a report which included a ffteen-year-old boy who had not been to the dentist in fve years and whose teeth were severely damaged: The caseworker said, "he needs nine root canals, seven fllings. " This was not reported by the dentist, however. The caseworker said that "the dentist had all the information of the last fve years of them trying to get [the boy] to come to the dentist... And [the dentist] didn't report him, because he's white. "</p><p>Interview fndings: Workers said they do not take the race of the family into account when making screening decisions. However, they do consider racial biases in CYF reporting and county data collection. For example, when asked if they take race into account when making screening decisions, one caseworker said "we don't treat any of the cases diferently." However, this caseworker later gave a hypothetical example of a referral where a Black family might get reported by someone who doesn't "deal on a daily basis with people of diferent cultures" and who might "automatically assume, like, 'oh my God, holy shit, you can't swear at your kids like that."' But, this report would be unjustifed: "you and I may think [swearing at your kids is] tacky, but is that child hurt? Kid's not hurt. " Another caseworker also said that they consider the race of the family in order to account for biases which could afect reporting:</p><p>"Colorblind assessment also feels like it's ignoring the point. So, I feel like [race is] defnitely something that I take into consideration. ... Bias could be afecting the way that the information is being reported. ... I feel like I'm defnitely more conscious of it now. " Overall: Some workers understood that there were systemic racial biases that existed outside of their agency-in reporters and in professionals (e.g., in the medical and criminal systems) who create the data that they then make decisions based on. These workers expressed that they make screening decisions in order to compensate for these systemic racial biases. When it came to the race of the family, most of these workers said they did not consider race qua race, but rather race qua racial biases, which could color the information that they see in a report. 17   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Collaborative decision-making. Screening decisions were not made by siloed, individual workers. Workers regularly made decisions collaboratively. They did so in two ways: First each report went through multiple (at least two) layers of workers in order to make the fnal screening decision. Second, workers often collaborated in ad hoc, impromptu ways, especially when workers were uncertain about a decision. We suspect that this collaboration contributed to mitigating workers' individual biases, and thus a reduction in the screen-in rate disparity in aggregate. However, this contribution is likely lessened, because fnal screening decisions are still made primarily by a few supervisors, and because the second kind of collaboration listed above is still informal and not applied consistently across all referrals.</p><p>Interview fndings: First, as described in Section 3.1.1, the workfow on a standard referral necessitates that one caseworker and one supervisor review a referral. For most referrals, the caseworker receives a report, feshes it out by searching the KIDS county database, makes a risk and safety assessment, makes a screening recommendation, and runs the AFST to generate a risk score. The report is then sent to a supervisor, who reads it in its entirety. One caseworker said the reports are not written by the caseworker alone-it is often a collaborative efort where the supervisor communicates with the caseworker about any inconsistencies, ambiguities, or mistakes in the report: "it isn't like, 'I'm doing it. Clickety-clickclick-click. '... [The supervisor] knows away that I screwed something up." If there are any holes in the report or any children in report are younger than four, the supervisor may also ask for a feld screen to send a feld caseworker out to conduct a preliminary check on the family or gather more information about the report. Once the report is complete, the supervisor makes the fnal decision to screen the report in or out.</p><p>Supervisors primarily make the screening decisions. That being said, the process of both caseworker and supervisor reviewing a referral, communicating about the referral, making assessments of risk and safety, and making screening recommendations, is an important form of multi-layered, collaborative decision-making. Having the multi-layered process not only adds additional pairs of eyes to review each referral, it also ensures that the referrals are more likely to be reviewed by workers with diferent (demographic) backgrounds. One caseworker believed that the diversity among caseworkers was helping to reduce biases in the screening decisions: "It's good to have diferent backgrounds with supervisors and others who make those decisions after it passes from our hands and it goes on to the next level of folks. And it's good to know there's diversity within those groups. "</p><p>Contextual Inquiry Observations: We also observed workers collaborating in ad hoc, impromptu ways that were not built into the standard decision-making process. Workers often discussed reports with other workers beyond the one caseworker and one supervisor assigned to the report. This occurred especially if they were uncertain about it. For example, while observing one caseworker (call them caseworker A), another caseworker (caseworker B) asked caseworker A to double-check that all the family members on a referral were correct, since caseworker B had taken a call about this family previously. Supervisors often talked with each other (or sometimes caseworkers) if they felt uncertain about a screening decision. For example, if they felt uncertain about a report, one supervisor said, "we'll sit down and discuss it amongst the supervisors. 'Well, what do you think we should do with this?' It's not like we all work in a little bubble." At one point, we observed a caseworker walk to a supervisor's desk to discuss what to do about a referral that the caseworker had reviewed and the supervisor was making the fnal decision about.</p><p>This process was often informal and ad hoc: Workers worked in tightly-packed cubicles in a single room. So, they would talk to one another over the cubicles, often overhear one another talk about a report and add their two cents, or walk to another worker's cubicle to ask them what they think about a certain referral. One supervisor described the call screening decision-making process as "very collaborative" and "a group efort": "Sometimes I'll be reviewing and I'll be like, 'Hey, <ref type="bibr">[supervisor]</ref>, what do you think about this?' You know, or we'll just be talking about it in the room. If the reporting source calls back, someone else will hear it and be like, 'Oh, that's my reporting source'. "</p><p>Overall: We suspect that this collaboration may have an efect of assuaging workers' biases. Prior work suggests that workers' biases play into the CYF screening process <ref type="bibr" target="#b45">[45]</ref>. Some workers acknowledged that they have their own personal biases. For example, a caseworker said, "I try to be conscious of my biases. " With at least a caseworker and a supervisor reviewing each referral in detail, workers have more of a chance to correct for each others' biases. 18  For example, one caseworker said the CYF call screening process has "a lot of checks and balances." Some workers themselves also expressed this view that multi-layered decision-making curbed biases. For example, when asked how workers make fair decisions, one caseworker said that "it's good to have... others who make those decisions after it passes from [caseworkers'] hands and it goes on to the next level of folks." By curbing workers' individual biases, we hypothesize that such collaborative decision-making may have contributed to workers' fnal screening decisions being less racially disparate in aggregate from 2016 to 2018. That being said, however, any bias-curbing efects of multi-layered, collaborative decisionmaking may be lessened due to caseworkers having less agency to infuence the fnal screening decisions. For example, one caseworker said, "I have very limited power... I don't have the power of saying something's not right. " Furthermore, because the second form of collaborative decision-making described above was primarily informal, ad hoc, and not built into the standard decision-making for every referral, it is unclear how many referrals would have been by this form of collaboration.</p><p>6 HOW WORKER-AFST COLLABORATION AFFECTED ACCURACY AND DISPARITY 6.1 AFST-only screening decisions were more accurate than workers when measured on outcomes that the AFST was trained to predict</p><p>So far, we have seen how workers made screening decisions which were less racially disparate than the AFST-only decisions. However, one of the primary arguments for the use of the AFST has been that it is more accurate than call screen workers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b85">82]</ref>. 19   18 Here, we mean individual biases, such as implicit biases or overt prejudices. For systemic biases, such as those built into worker training or the bureaucratic process itself, having more eyes on a report is unlikely to curb them. 19 On the other side, one of the primary arguments against its use has been that it is not accurate enough <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b65">65]</ref>.</p><p>For example, in the ofcial Ethical Analysis of the AFST, Dare and Gambrill <ref type="bibr" target="#b20">[20]</ref> claim that the AFST is "more accurate than any alternative" and argue that it "is hard to conceive of an ethical argument against use of the most accurate predictive instrument. " After all, an inaccurate screening decision could mean that a family is unjustly investigated-a possibly traumatic experience and a step too far towards child separation. Or it could mean that CYF does not intervene when a child will be harmed. In our setting, one concern is that a screening policy which makes less racially disparate decisions might make less accurate decisions <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b94">90]</ref>. For example, it may be the case that because workers make less racially disparate decisions than the AFST, they would also make less accurate decisions. In Figure <ref type="figure" target="#fig_4">5</ref>, we see that this is ostensibly the case: the AFST-only screening policy would have been more accurate than the actual screening decisions made by workers from August 2016 to July 2018 (51.0% vs 46.5%). 20 In Figure <ref type="figure" target="#fig_4">5</ref>, we also see that the worker-AFST decisions were less racially disparate in terms of accuracy than the AFST-only decisions (5.4% vs 13.5%). In sum, although the worker-AFST decisions achieved lower prediction accuracy than the AFST-only decisions, worker-AFST decisions were also less racially disparate than the AFST-only decisions in terms of accuracy. However, the defnition of prediction accuracy we adopt is particularly important when interpreting these results. Accuracy is measured against outcomes that the AFST is trained to predict, i.e. re-referral and placement. Thus, a more precise interpretation of the results in Figure <ref type="figure" target="#fig_4">5</ref> is that the AFST-only was 4.5% more accurate than workers at predicting the outcomes that the AFST was trained to predict. However, prior work suggests that these outcomes are biased and contested <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b50">50]</ref>. Furthermore, as we will discuss in Section 6.2, our qualitative fndings indicate that workers disagree with the use of these outcomes, and that they are predicting diferent outcomes than the AFST day-to-day. Therefore, although the AFST-only screening policy was better at predicting the outcomes that it was trained to predict, it is important to keep in mind that the decisions made by workers aided by the AFST may actually be better at predicting the outcomes that workers fnd useful or important for preventing child abuse or neglect. 21   6.2 Qualitative: Workers and the AFST do not agree on prediction outcomes and accuracy measures 6.2.1 Workers believe re-referral and placement are bad proxies for risk of child abuse and neglect. In an ofcial CYF survey of Human Services <ref type="bibr" target="#b68">[68]</ref>, workers were asked about how confdent they are in "the AFST's ability to accurately assess the risk of a future referral or out-of-home placement?" Workers responded with lukewarm confdence because they thought the AFST was unable "to take expected improvement or individual circumstances into account" <ref type="bibr" target="#b68">[68]</ref>. Yet, this may be the wrong question to ask entirely: workers may or may not be confdent that the AFST can accurately predict the proxy outcomes that it was trained to predict. A more fundamental question is: are caseworkers confdent that any assessments of risk of future referral or out-of-home placement will help them make better decisions to prevent child abuse and neglect? Interview fndings: Based on our observations and interviews with workers, we believe workers lean towards 'No. ' Although the AFST was intentionally designed to complement workers judgment by nudging them to consider longer-term risk, in addition to shortterm safety concerns <ref type="bibr" target="#b97">[93]</ref>, workers did not view the AFST's target outcomes as relevant to their decision-making.</p><p>Re-referral. Five caseworkers and two supervisors thought that re-referral does not necessarily indicate risk of child abuse or neglect. One supervisor said that re-referral may not mean that the frst decision to screen out was incorrect, since the second referral could be for an entirely diferent reason than the frst: "it could have been referred because the mom was outside on the porch, smoking a cigarette with a newborn baby, and then it comes back mom beat the 10-year-old. I mean, it's just a whole diferent reason." Three caseworkers and one supervisor said that some reporters misuse the system and report families for unnecessary reasons. For example, one caseworker gave "retaliation reports" as an example: "I call on you. You call on me. I call on you." In our contextual we observed a caseworker review a referral where a divorced couple called on each other three diferent times and each report was unsubstantiated: the caseworker said these were likely false retaliation reports. The designers of the AFST themselves noted problems with using re-referral as a target outcome and they removed it from the AFST model in 2018 <ref type="bibr" target="#b97">[93]</ref>.</p><p>Placement. Since August 2018, the AFST Version 2 only predicts placement. However, four caseworkers and one supervisor also found issue with using placement in foster care as a proxy for child abuse or neglect. A caseworker said, "knowing the risk of removal within two years is not feeling like it's super relevant to the decision that is needed. And it is very little to do with immediate safety or anything like that. " Two caseworkers said that children were often placed in foster care without any concerns of child abuse or neglect: there are often other reasons for placing a child. For example, one caseworker said: 22   "The majority of these cases [when children are placed in foster care] are not child abuse in nature, it's parentchild confict, the kid doesn't want to live with mom or dad or the grandma, the child is saying, 'I don't feel safe, I don't want to go home.' And if they go to the police and then saying that information, some police will take custody of that child and the court has to place that kid. So it's just a lot of other variables going on that decide whether or not this child is going to be placed. " On the other side, one caseworker and one supervisor said that placement is not the right option for many families who do have concerns of child abuse or neglect. One caseworker said families are often reported for safety concerns, without any possibility of placing a child in foster care: 23 "just because the report is being made doesn't mean that a kid is going to go into placement. " In prior work, De-Arteaga et al. <ref type="bibr" target="#b22">[22]</ref> similarly note that "[n]ot all cases where there is a risk to the child result in out-of-home placement. " Lastly, another caseworker said that placement was not a good outcome to measure because it was a kind of self-fulflling prophecy: "risk of removal in two years is inherently going to be increased by our [CYF] involvement, because we're the only ones that can remove the children. " De-Arteaga et al. <ref type="bibr" target="#b22">[22]</ref> and Coston et al. <ref type="bibr" target="#b18">[18]</ref> also suggest this point. 6.2.2 Workers did not make decisions based on risk of re-referral or placement. Not only did workers say that re-referral and placement were not helpful outcomes to predict, we also observed workers making screening decisions based on assessments of entirely diferent outcomes.</p><p>Contextual Inquiry Observations: We observed that call screen workers did not try to predict whether or not children in a referral would be re-referred or placed in foster care within two years. Workers' screening decisions were based on diferent, shorter-term outcomes related to child safety.</p><p>For example, while reviewing referrals, one caseworker said that call screen workers make decisions by looking for "safety concerns. " This caseworker looked for the following, for example: signs that the child felt unsafe, would be hurt, was left home alone a lot, was in contact with a child molester, that the caregiver was under the infuence of drugs, etc. The caseworker also said that caseworkers asked reporting sources about the following, for example: Is there food in the fridge? Do the kids have sheets on their beds? Is there furniture in the home? Do the parents have drug or alcohol problems? Mental health concerns? Domestic violence concerns?</p><p>Clearly, re-referral or placement were rarely among the factors that workers assessed for. Furthermore, these factors were not indicative of risk of child abuse or neglect over the span of two years: it will not take a child two years to starve if there is currently no food in the fridge. The factors that workers look for when making their screening decisions are either shorter-term safety concerns, or specifc details about the referral that could be longer-term sources of risk, regardless of whether they would lead to removal from home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION 7.1 Interpretation of the fndings</head><p>In this section, we discuss two implications of our work as it pertains to the AFST specifcally. First, our work suggests that Allegheny County CYF's choice to use the AFST to aid call screen workers, rather than to replace them yielded less racially disparate screening decisions from August 2016 to May 2018. Second, our work complicates two of the primary positive arguments that prior work have identifed for introducing the AFST in the place: 1) that it decreases racial disparities; and 2) that it increases accuracy. Although our results do not run entirely contrary to these claims, we suggest that future work is necessary in order to better evaluate how the AFST positively contributes to the decision-making process, if at all. 7.1.1 Automated AFST screening decisions would have yielded larger racial disparities. In this paper, we compared two policies for using the AFST in CYF call screening from August 2016 to May 2018: frst, the hypothetical policy where the AFST entirely automates screening decisions; and second, the actual policy where workers use the AFST to inform their decisions. We evaluated these policies in terms of racial disparities in both screening rates and predictive accuracy. Our results in Section 5 suggest that the automated AFST-only policy would have resulted in larger disparities in call screening: the AFST-only Black-white screen-in rate disparity would have been 20%. This is larger than both the 11.3% pre-AFST disparity from January 2015 through July 2016 and the 9% disparity for AFST-assisted worker decisions from August 2016 through May 2018. Had the AFST automated screening decisions at CYF, our results suggest that this would have increased the disparity in the rates at which Black versus white children were screened in for investigation.</p><p>Instead, when workers used the AFST, this disparity of 20% decreased to 9%. In order to understand why this occurred, we frst looked at how workers disagreed with the AFST. Rather than disagreeing with the AFST in ways that directly reduced disparities, we found that workers disagreed with the AFST across the board. Some of workers' disagreements contributed to greater disparity and some served to reduce disparity. Yet, overall, worker-AFST disagreement led to a reduction in screen-in rate disparity. We then conducted a contextual inquiry and interviews with call screen workers at Allegheny County CYF to further understand this pattern. As shown in Section 5.2, we found that workers were not surprised that the AFST gave disparate screening recommendations. Workers made holistic screening decisions based on their knowledge of relevant context beyond just the AFST score, and they believed that this ultimately led them to make less racially disparate screening decisions than the AFST alone would have. Workers made decisions on a case-by-case basis, focused primarily on the risk and safety of the children involved in each individual report. Within these bounds, some workers made a conscious efort to reduce unwarranted racial disparities. Others believed that any reduction in disparities that occurred due to their disagreements with the AFST was incidental and unintentional, viewing this as a side efect of making decisions holistically based on various sources of information available to them. However, the efect of reducing disparity in aggregate was the same.</p><p>Our results run counter to much prior work on racial disparities in human-AI systems. As Green <ref type="bibr" target="#b36">[36]</ref> writes, the "vast majority of research suggests that people are unable to provide reliable oversight of algorithms" and that human discretion worsens racial disparities. For example, judges have disregarded criminal risk assessment recommendations in ways that worsen racial disparities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b86">[83]</ref><ref type="bibr">[84]</ref><ref type="bibr" target="#b88">[85]</ref><ref type="bibr" target="#b101">96]</ref>. In Mechanical Turk experiments, people have been shown to use their discretion to make racially biased decisions <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>. We present a case study in the opposite direction: worker discretion served to decrease racial disparities in the AFST from August 2016 to May 2018, echoing De-Arteaga et al. <ref type="bibr" target="#b23">[23]</ref>, who suggest that CYF workers used their discretion to counteract errors in the AFST. As discussed below, further research is needed to understand the impacts of worker-AFST decision-making on predictive accuracy, and to untangle the extent to which worker discretion served to decrease unwarranted versus warranted screening disparities.</p><p>In the context of this prior literature, our fndings suggest that caution is needed when drawing broad generalizations about the impacts and dynamics of human-AI decision-making (cf. <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b89">86]</ref>).</p><p>In practice, the interplay of human and AI judgment may yield very diferent results depending on the specifc domain under study, the abilities and expertise of the people involved, and the organizational contexts in which people and AI are making predictions and decisions.</p><p>7.1.2 Was the AFST responsible for the reduction in disparity from pre-to post-AFST? A key concern for any policy change in child welfare is whether it will reduce or amplify existing racial disparities. In their description of the ofcial AFST Impact Evaluation <ref type="bibr" target="#b35">[35]</ref>, Allegheny County Department of Human Services suggests that the "AFST led to reductions in disparities of case opening rates between black and white children" <ref type="bibr" target="#b70">[69]</ref>. We also observed a reduction in screen-in rate disparity from before the introduction of the AFST to after (11.3% disparity from January 2015 through July 2016 to 9% disparity from August 2016 through May 2018). However, it is not clear that this reduction in disparity was caused by the introduction of AFST in August 2016. It is difcult to determine whether and to what extent changes in disparities can be attributed to the AFST, since the AFST was implemented across all Allegheny County CYF screening decisions (without A/B testing or randomization) and there were a number of other factors which infuenced call screening-including a number of changes in practices and policy internal or external to CYF around the time of or since the deployment of the AFST. 24 It is also possible that the kinds of referrals being referred from August 2016 through May 2018 were diferent from those in January 2015 through July 2016, regardless of the AFST's recommendations.</p><p>Our results show that the AFST gave recommendations which were more racially disparate than workers pre-AFST. So, several interpretations are possible. For instance, it may be that workers reduced the overall disparity on their own, by making decisions largely as they had prior to the introduction of the AFST. It may be that AFST's disparate recommendations could have pressured workers to make more disparate decisions, but workers disregarded it and did the opposite. On the other hand, if the introduction of the AFST did contribute to a reduction in screen-in rate disparities from pre-to post-deployment, our results suggest that workers' discretion was integral to this reduction. One possible explanation is that the introduction of the AFST may have led workers to refect on their own biases, resulting in decisions that were less racially disparate than their decisions in the year and a half prior to the introduction of the AFST. A second possibility is that workers and the AFST have complementary strengths and biases, and that the interplay of these led to less biased screening decisions overall. These possibilities are neither mutually exclusive nor exhaustive: some combination of these mechanisms may be at play, and additional mechanisms may be possible beyond those mentioned here. We leave it to future work to further explore whether and how the introduction of the AFST may have impacted disparities from pre-to post-deployment. In any case, our results suggest that the observed reduction in screen-in rate disparity can be attributed to human workers, whether or not they used information from the AFST to do so. 7.1.3 Was the AFST more accurate than workers? Prior work suggests that the AFST makes more accurate predictions than workers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b85">82]</ref>. While our results in Section 6.1 show that the accuracy disparity for AFST-only decisions would have been higher than that of worker-AFST decisions, they also show that AFST-only decisions would have been more accurate than worker-only decisions from January 2015 to July 2016 (55.2% vs 49.7%) and worker-AFST decisions from August 2016 to May 2018 (51.0% vs 46.5%). 25 This increased accuracy is one of the primary reasons for the use of the AFST <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b85">82</ref>]. An ofcial Allegheny County Department of Human Services <ref type="bibr" target="#b4">[5]</ref> statement echoed that "not using [the AFST] might be unethical because of its accuracy." Yet, the AFST's accuracy is measured in terms of specifc proxies for abuse or neglect (re-referral and placement, for Version 1) over a long time range (two years). In Section 6.2, we found that workers did not agree these choices of proxies, and that they viewed themselves as predicting immediate safety concerns-in other words, a diferent outcome than the AFST, over a shorter time span. This is consistent with suggestions from prior work <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>. Thus, while our results suggest that the AFST is better at predicting the proxy variables it was trained to predict, it remains an open question how the AFST compares at the prediction and decision tasks that workers are actually trying to perform. Vaithianathan et al. <ref type="bibr" target="#b99">[94]</ref> suggest that higher AFST Version 2 risk scores identify children with higher rates of injury-related hospitalizations, which may be closer to the kinds of target outcomes that workers actually consider when making decisions. 26 However, our results suggest that workers predict diferent outcomes depending on the specifc referral, many of which may not involve child hospitalization. Overall, our results in Section 6.2 complicate the argument that the AFST alone is more accurate than call screen workers, and point to critical directions for future research. Without frst understanding workers' objectives when making predictions and decisions, such accuracy comparisons may essentially be evaluating workers' performance on a game that they are not playing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Design Implications for Human-AI Collaboration</head><p>Our results suggest that worker-AFST collaboration yielded lower screen-in rate racial disparity than AFST-only decisions, and lend insight into how this reduction occurred. Here, we provide positive design implications for potential ways to improve human-AI collaborative decision-making in child welfare and related contexts.</p><p>• Be cautious about automating decisions in contexts with existing racial disparities and biases. Our results suggest that workers reduced racial disparities in the AFST through their patterns of disagreement with the AFST score. Based on our fndings, we strongly caution against fully automating decisions in high-stakes, real-world social contexts. This recommendation aligns with prior work in the child welfare context <ref type="bibr" target="#b23">[23]</ref>. For the same reasons, even partial or 'soft' automation (like the mandatory policy in the AFST context) should be approached cautiously to ensure that they do not incentivize against potentially productive forms of human disagreement with algorithmic recommendations. • Explainable AI and interfaces to empower workers in mitigating algorithmic limitations. Our fndings suggest that workers' ability to identify instances where the AFST was over-or under-scoring a referral may have contributed to a reduction in racial disparity (5.2). However, it remains unclear how accurately and reliably workers are able to do so. Thus, one possible design implication is to develop interfaces that assist workers in identifying specifc instances where an algorithm may be more or less reliable. For example, in the AFST context, providing local explanations for AFST recommendations may help workers in calibrating their reliance on the tool, case-by-case. However, given recent empirical fndings demonstrating ways such local explanations can backfre in ways that harm human-AI decision-making (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b76">74]</ref>), further research is needed to understand how these can be designed and presented efectively. • Value sensitive AFST models. Our results suggest that workers often adjust for what they perceive as limitations of the algorithm. For example, workers believed that the AFST was unable to sufciently account for certain features (e.g., details of the actual allegation), and the outcomes predicted by the AFST did not align with the outcomes that workers were predicting. Zhu et al. <ref type="bibr" target="#b105">[100]</ref> suggest involving stakeholders into the design process to make sure their values are incorporated into the design of an algorithmic system from the beginning. One design implication is for AFST designers to engage with child welfare workers, among other key stakeholders, and to incorporate their insights and feedback in future iterations of AFST. Some value misalignments between workers and the AFST were intentionally designed into the system, with the goal of nudging workers towards diferent practices. However, in practice we observed that these misalignments meant that workers often needed to work around the algorithm instead of working with it. • Promoting more collaborative decision-making. Child welfare workers often made decisions collaboratively, both formally and informally. We suspect that this may have had the efect of curbing workers' individual biases. However, in formal collaboration, caseworkers felt they had little agencyeven though they made screening recommendations. The frst design implication is to encourage more regular conversations between caseworkers and supervisors about caseworkers' recommendations, so they do not get overlooked. There may also be opportunities for the AFST interface to promote more informal collaboration. For example, future versions of the AFST or similar tools could include a feature to suggest caseworkers in the ofce that have dealt with similar referrals in the past. This could enable workers to collaborate with the right person on each referral, which may be particularly helpful when dealing with highly uncertain referrals. • Diversity and lived experience among workers. Child welfare workers were generally aware of their own individual biases. Another way to curb these biases may be to increase diversity among the child welfare workers, especially supervisors, who make the fnal decisions. As one caseworker stated, it is important to ensure that there are screeners and supervisors who have "lived the experience" of the families in a report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Limitations &amp; Future Work</head><p>One limitation of our methods is that the data used in our quantitative analysis is from August 2016 through May 2018, whereas the contextual inquiry and interviews were conducted in July 2021. Thus, if workers changed how they used the AFST signifcantly or they are unable to remember how they used it previously, then our results from the contextual inquiry and interviews may not refect the reasons why workers reduced disparities from 2016 to 2018. This is likely not entirely the case, since caseworkers' workfow has remained largely consistent over this time frame (even though there have been some changes in CYF policy around the use of the AFST, including the roll-out of a new V2 model).</p><p>Another limitation of the current work is that we focus on the impacts of worker-AFST decision-making on overall disparities. We do not investigate the extent to which worker discretion served to decrease unwarranted versus warranted disparities in the AFST's screening recommendations. Our results demonstrate that worker-AFST decision-making served to reduce the disparity in child maltreatment screening rate between Black and white children compared to algorithm-only decisions. As discussed, this is an interesting fnding in itself, in light of prior empirical research suggesting that human discretion often increases rather than decreases disparities. This fnding also has practical implications, as racial disparities in child maltreatment screening have real consequences for children, families, and communities. For example, higher screen-in rates indicate higher levels of state intervention into families' lives, starting with an investigation. Such disparities may indicate uneven application of interventions or investigations. Importantly however, higher disparities in screening, on their own, do not necessarily imply unfairness. Disparities may be warranted if they refect genuine diferences in underlying distributions: a higher screen-in rate for one group may be justifed if there is higher need or higher risk of maltreatment among children in that group <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b53">53]</ref>. For this reason, naively attempting to equalize screening rates across demographic groups without regard for actual children's needs and safety could be harmful and counterproductive. However, it is clear from our interviews and contextual inquiries that this is not how workers reduced disparities in algorithmic decisions. Rather, workers appeared to reduce disparities by making holistic assessments of child risk and safety based on all of the information available to them, and by working to mitigate limitations that they perceive in the algorithm. An important direction for future research is to untangle the extent to which worker discretion serves to decrease unwarranted versus warranted screening disparities. To support such investigations, it will be critical to overcome limitations of current approaches for measuring accuracy, discussed in Sections 4.3 and 6.1.</p><p>We present two additional avenues for future work, informed by limitations of our current work. First, we leave it to future work to conduct a more comprehensive evaluation of racial disparities between worker-only and worker-AFST screening decisions. We did not focus on these results in the current paper, due to several confounding factors that complicate the analysis. These included changes in external factors-such as a 2015 state-wide policy change in mandated reporting laws and the COVID-19 pandemic since at least March 2020-for afecting the County reporting rates and the CYF screening process. 27 Second, as discussed, measuring prediction accuracy is limited by the use of proxy outcomes (re-referral or placement) that do not align with the outcomes that humans are predicting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">23]</ref>. It is possible that people are better than algorithms at the prediction task they are actually performing. While beyond the scope of the current paper, this remains a critical direction for future work. For example, future research could involve better understanding what constructs human workers are predicting, operationalizing these as target measures, and then re-running some of the accuracy comparisons presented in this paper using those measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A NUMBERS AND PERCENTAGES OF CHILDREN BY RACE AND RISK LEVEL</head><p>See Table <ref type="table" target="#tab_3">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL TERMINOLOGY</head><p>In addition to screen-in rate disparities and decision accuracy, we also evaluate the following additional metrics.</p><p>• Precision: the percentage of children that are screened-in and are either removed from home within 2 years, or re-referred again within months of the referral. • True Positive Rate the likelihood that a child who has the positive proxy ground truth label (i.e. either removed from home within 2 years or re-referred again within 2 months of a referral) will be screened-in. • False Positive Rate (FPR): the likelihood that a child with a negative proxy ground truth label (i.e. neither removed from home within 2 years nor re-referred again within 2 months of a referral) will be screened-in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPARISONS OF OTHER DISPARITY METRICS AND THRESHOLDS</head><p>Figure <ref type="figure" target="#fig_6">6</ref> shows racial disparities in screen-in, accuracy, true positive, false positive, and precision rates across possible AFST-only olds from pre-to post-AFST deployment. Figure <ref type="figure" target="#fig_8">7</ref> shows screen-in, accuracy, true positive, false positive, and precision rates by race, possible AFST-only thresholds, and AFST-only versus worker-AFST decisions from August 2016 to May 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D WORKERS SEE RISK LABELS AS SCREENING RECOMMENDATIONS</head><p>Neither ofcial AFST documentation nor public comments from CYF leadership claim that a Low risk label (scores 1 through 9) means that a referral should should be screened out nor that a High risk label (scores 15 and up) means screen in. However, this is the message that is implied through the interface design of the AFST and is what we heard from workers. For one, the AFST interface shows scores binned into Low, Medium, and High risk levels with clear divisions between them. Low risk is green (calm, low stress) and High risk is red (urgent, severe). Although not explicitly stated in the AFST documentation, the message that Low risk referrals should be screened out and High risk referrals screened in is relayed to workers via these design choices. When we interviewed and observed workers, many implied or echoed this explicitly. One caseworker (erroneously) said that "with a High risk [referral], you know, we absolutely have to screen them in. " 28 Another said, "when it is High risk, I've just been going with open investigation, " i.e. screen in. 28 Workers are not mandated to screen in High risk referrals. However, this quote exemplifes workers' perceived pressure to screen in High risk referrals.</p><p>While observing one caseworker, they got a referral and explained: "it was a Low risk [referral]... And instead of screening out, I just recommended accept for investigation. " Here, the caseworker implied that Low risk referrals should be screened out. Furthermore, fve caseworkers and one supervisor said that they did not know what Medium risk scores meant or did not fnd them when making screening decisions. In all, workers see an AFST Low risk label as a recommendation or pressure to screen out a referral, High risk label to screen in, and Medium risk confers no recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ACCURACY BREAKDOWN BY SCORE</head><p>In this section, we examine the accuracy of AFST-only decisions (with a threshold of 15) versus worker-AFST decisions for referrals binned by AFST score or mandatory screen-in label from August 2016 to May 2018. Accuracy is calculated just as in Section 6.1, except the denominators include referrals with only a single score. If all 15,182 children labeled as mandatory screen-ins were screened in (as the AFST-only policy would have), then 32.1% of these decisions would have been accurate. The actual screening decisions made by workers were accurate for 37.5% of these 15,182 children. Table <ref type="table" target="#tab_4">4</ref> shows that worker-AFST decisions were more accurate than AFST-only decisions would have been for every score from 15 and above. Table <ref type="table" target="#tab_5">5</ref> compares the accuracy of AFST-only decisions versus worker-AFST decisions for Low risk referrals (with scores 1 to 9) from August 2016 to May 2018. Worker-AFST decisions were less accurate than AFST-only decisions for every score from 1 to 9. Overall, these results suggest that worker discretion increased the accuracy for children that the AFST-only would have screened in and decreased it for those the AFST-only would have screened out. In particular, worker discretion increased the accuracy of decisions made in mandatory screen-in referrals, which are recommended to be screened in by default. However, these results are again limited by problems with evaluating counterfactual predictions, as discussed in Section 4.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The current AFST-assisted call screening process.Call screen caseworkers make screening recommendations and supervisors make the fnal screening decisions, both with the AFST's risk score and recommendation.</figDesc><graphic url="image-1.png" coords="4,59.81,83.68,228.23,73.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The interface of the AFST which is used by the child welfare workers to make screening decisions. The left fgure shows the score the for a normal high risk referral. The right fgure shows a referral with an exceptional high risk that triggers the mandatory screen-in policy.</figDesc><graphic url="image-2.png" coords="5,324.59,83.69,226.97,119.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Black-white screen-in rate disparities for AFSTonly and worker-AFST decision</figDesc><graphic url="image-3.png" coords="7,337.20,83.69,201.75,163.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Proportions of Black and white children for which the AFST-only decisions and worker-AFST decisions agreed and difered. Note that the percentages here are over the total number of referrals per race, e.g. the 48.6% in the leftmost bar indicates that 48.6% of all Black children referred to CYF would have been screened in by AFST-only and was screened in by workers aided by the AFST (worker-AFST). Notice that the sum of the two leftmost bars (for a single race) equals the AFST-only screen-in rate -e.g. 48.6%+22.3%=71% for Black children,-whereas the sum of the leftmost and the right middle bars equals the actual worker-AFST screen-in ratee.g. 48.6%+15.9%=61.7%.</figDesc><graphic url="image-4.png" coords="8,324.59,83.69,226.98,112.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The in prediction accuracy between AFST-only and worker-AFST decisions.</figDesc><graphic url="image-5.png" coords="12,73.04,83.69,201.76,164.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Screen-in rate disparity (b) Accuracy disparity (c) Precision rate disparity (d) True positive rate disparity (e) False positive rate disparity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Common group fairness metrics showing disparities between Black and white children. The x-axis is the various decision thresholds for the hypothetical AFST-only decisions (for a given threshold, AFST-only would screen in all referrals with that score or above, and screen out all referrals under). The solid red line represents the worker-AFST fnal decisions after AFST deployment (2016-2018), the solid blue line with dots represents AFST-only decisions in the same time period (2016-2018). The dashed green line represents the worker-only decisions without before the AFST was deployed (2015-2016), and the dashed orange line with crosses represents the AFST-only decisions generated retrospectively for all referrals in the same time period (2015-2016).</figDesc><graphic url="image-10.png" coords="20,184.94,442.39,242.11,147.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparisons of decision outcomes between worker-AFST and AFST-only decisions for referrals between 2016 to 2018. The x-axis is the various decision thresholds for the hypothetical AFST-only decisions (for a given threshold, AFST-only would screen in all referrals with that score or above, and screen out all referrals under).</figDesc><graphic url="image-15.png" coords="22,184.94,444.13,242.11,149.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table1shows the total number of referrals with Black and white children which would have been screened in and out under the AFST-only policy. An AFST-only policy would have screened in over 7500 more Black children than white children. In actuality, workers only screened in only 4713 more Black children than white. For reference, we calculate that from January 2015 to July 2016, before the AFST was implemented, workers screened in 52.5% of Black children and 41.2% of white children in discretionary referrals. This was a Black-white screen-in rate disparity of 11.3%.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of qualitative fndings presented in Section 5.2. they did not look at race at all. For example, one supervisor said, "I have no idea what races people are. " Based on contextual inquiries and interviews with CYF call screen workers, we hypothesize that the decrease in Black-white screen-in rate disparity from 2016 to 2018 occurred because of the following reasons (also summarized in Table2):</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>for a breakdown of the numbers and percentages of children referred to CYF for discretionary referrals from August 2016 to May 2018 by risk level (High, Medium, Low, and mandatory screen-in). Here, we see a full of the AFST's disparate risk scores, as mentioned in Section 5. A higher percentage of Black children than white children were labeled mandatory screen-in and High risk; a lower percentage of Black children than white children were labeled Medium risk or Low risk.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Numbers and proportions of children by race and AFST risk level. Percentages are over total children by race, e.g. 2379 Black children labeled Low risk made up 9.1% of all 26123 Black children referred to CYF (2016-2018).</figDesc><table><row><cell></cell><cell>All children</cell><cell cols="2">Black children white children</cell></row><row><cell>All risk levels</cell><cell></cell><cell>26123 (50.5%)</cell><cell>21623 (41.8%)</cell></row><row><cell cols="3">Mandatory screen-in 15182 (29.3%) 9639 (36.9%)</cell><cell>4863 (22.5%)</cell></row><row><cell>High risk</cell><cell cols="2">31022 (59.9%) 18536 (71.0%)</cell><cell>11013 (50.9%)</cell></row><row><cell>Medium risk</cell><cell cols="2">11778 (22.8%) 5208 (19.9%)</cell><cell>5653 (26.1%)</cell></row><row><cell>Low risk</cell><cell>8950 (17.3%)</cell><cell>2379 (9.1%)</cell><cell>4957 (22.9%)</cell></row><row><cell cols="4">AFST score Num. of children by score AFST-only accuracy Worker-AFST accuracy</cell></row><row><cell>15</cell><cell>3374</cell><cell>16.7%</cell><cell>48.1%</cell></row><row><cell>16</cell><cell>3520</cell><cell>19.5%</cell><cell>40.7%</cell></row><row><cell>17</cell><cell>4003</cell><cell>22.8%</cell><cell>40.3%</cell></row><row><cell>18</cell><cell>4543</cell><cell>25.2%</cell><cell>38.7%</cell></row><row><cell>19</cell><cell>5406</cell><cell>27.3%</cell><cell>39.3%</cell></row><row><cell>20</cell><cell>10176</cell><cell>35.5%</cell><cell>39.2%</cell></row><row><cell>M</cell><cell>15182</cell><cell>32.1%</cell><cell>37.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy rates AFST-only and worker-AFST policies binned by AFST score or mandatory screen-in (M) from August 2016 to May 2018 for High risk referrals. The AFST-only policy (with a threshold of 15) would have screened in all referrals in this table, so the accuracy rates in the third column are equivalent to the percent of children who were either re-referred within 2 months or placed within 2 years. For example, of the 15,182 children labeled mandatory screen-in (placement model score 18 or higher), 32.1% were re-referred or placed in foster care. The rightmost column is the accuracy of workers' actual screening decisions.</figDesc><table><row><cell cols="4">AFST score Num. of children by score AFST-only accuracy Worker-AFST accuracy</cell></row><row><cell>1</cell><cell>81</cell><cell>87.7%</cell><cell>84.0%</cell></row><row><cell>2</cell><cell>329</cell><cell>94.2%</cell><cell>79.3%</cell></row><row><cell>3</cell><cell>621</cell><cell>93.7%</cell><cell>72.0%</cell></row><row><cell>4</cell><cell>805</cell><cell>90.8%</cell><cell>64.2%</cell></row><row><cell>5</cell><cell>1063</cell><cell>90.1%</cell><cell>60.6%</cell></row><row><cell>6</cell><cell>1243</cell><cell>87.5%</cell><cell>61.5%</cell></row><row><cell>7</cell><cell>1366</cell><cell>89.7%</cell><cell>61.9%</cell></row><row><cell>8</cell><cell>1589</cell><cell>88.3%</cell><cell>57.7%</cell></row><row><cell>9</cell><cell>1853</cell><cell>87.1%</cell><cell>60.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Accuracy rates of AFST-only and worker-AFST policies binned by AFST score and mandatory screen-in (M) from August 2016 to May 2018 for Low risk referrals. Any AFST-only with threshold 10 or above would have screened out all referrals in this table, so the accuracy rates in the frst column are equivalent to the percent of children who were neither re-referred within 2 months nor placed within 2 years. For example, of the 81 children with an AFST score of 1, 87.7% were neither rereferred nor placed in foster care. The rightmost column is the accuracy of workers' actual screening decisions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is notable that Los Angeles County, the largest child welfare department in the U.S., dropped a private model earlier, but is now implementing a new model with the same team who developed the AFST.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For example, if there are two children in the referral: the frst child with a re-referral model score of 1 and a placement model score of 15, the second child with a re-referral score of 10 and a placement score of 3; a score of 15 (and a "High risk" message along with Figure2) would be shown to the call screen workers.<ref type="bibr" target="#b4">5</ref> This is the risk of future child maltreatment and immediate safety. A U.S. government source defnes a safety assessment as gathering information to "determine the degree to which a child or youth is likely to sufer maltreatment in the immediate future" and a risk assessment as collecting information "to determine the degree to which key factors are present in a family situation that increase the likelihood of future maltreatment to a child or adolescent"<ref type="bibr" target="#b34">[34]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">According to a Pennsylvania governmental source, "CPS reports are those that allege a child might have been a victim of child abuse"<ref type="bibr" target="#b0">[1]</ref>. Pennsylvania law dictated that these referrals were automatically screened in and investigated.<ref type="bibr" target="#b6">7</ref> All children in the same referral received the same screening decision.<ref type="bibr" target="#b7">8</ref> Within CYF analysis, this is referred to as the individual or child level.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The AFST automatically fags referrals with the highest risk as "mandatory screen-in. " Only supervisors have the authority to override the AFST's decisions for these referrals<ref type="bibr" target="#b68">[68]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We chose to evaluate re-referral within 2 months for consistency with the ofcial AFST Impact Evaluation<ref type="bibr" target="#b35">[35]</ref>. However, that the AFST Version 1 predicted re-referral within 2 years<ref type="bibr" target="#b96">[92]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">By Pennsylvania law, all CPS referrals are automatically screened in. Some other referrals were marked as automatically screened in or out. All of these referrals were excluded from our analysis. All numbers in this section refect only discretionary referrals for which the AFST could have had some infuence on.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Note that this 20% diference is in percentage points, i.e. 71% minus 51%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Note, however, that fve caseworkers stated that they did consider the race of the family in order to account for racial biases in reporting. We explain this further below.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">This Low risk protocol label was introduced in the AFST Version 2<ref type="bibr" target="#b97">[93]</ref>. It is analogous to the mandatory screen-in label, but for screening out: so, the default decision is screen out, but it can be overridden by a supervisor.<ref type="bibr" target="#b15">15</ref> The ofcial AFST FAQ documentation says that "receiving of public benefts" did not necessarily increase a family's AFST score: "[F]or 45% of families, receiving of public benefts (e.g., SNAP, TANF)... was associated with lower scores than for similar families that did not receive those services"<ref type="bibr" target="#b68">[68]</ref>. As one supervisor pointed out, however, workers' perceptions could still be right if there are "other things that are afecting that score" that are "just more associated" with public welfare records, e.g. public mental health records or criminal records.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Prior work also suggests that CYF workers can reliably correct for limitations in the AFST<ref type="bibr" target="#b23">[23]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">As a note, nondiscrimination laws apply to child welfare investigations<ref type="bibr" target="#b67">[67]</ref>, which may motivate workers not to consider race or to say they do not.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">To reiterate, this AFST-only policy is defned as in Section 4 with a screening threshold of 15, i.e. screen in all High risk referrals and screen out everything else.<ref type="bibr" target="#b21">21</ref> Here, we hearken back to a question Dare and Gambrill<ref type="bibr" target="#b20">[20]</ref> bring up in the Ethical Analysis of the AFST: "The question is, how can we make the fewest errors in our eforts to protect children and families?"</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">We leave it to future work to validate this worker's statement, i.e. to see what are the primary causes of removal among discretionary referrals.<ref type="bibr" target="#b23">23</ref> This claim refects this worker's perceptions, yet may not be entirely accurate. We leave it to future work to validate this claim.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">Here, we note amendments to the Pennsylvania Child Protective Services Law (CPSL) that went into efect on December 31, 2014 and could have had residual efects on reporting and screening throughout the Allegheny County and the state.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25">We measure the AFST's accuracy diferently than most prior work, much of which uses AUC (area under the receiver operating characteristic curve).<ref type="bibr" target="#b26">26</ref> Note that AFST Version 2 also predicts foster care placement within 2 years, but not re-referral.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Luke Guerdan, our colleagues from GroupLens Research at the University of Minnesota, and our anonymous reviewers for their valuable feedback. This work was supported by the National Science Foundation (NSF) under Award No. 1939606, 2001851,  2000782 and 1952085.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Department Of Human Services Releases</title>
		<ptr target="https://www.media.pa.gov/Pages/DHS_details.aspx?newsid=253Online" />
	</analytic>
	<monogr>
		<title level="j">Child Protective Services Report</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016-01">2016. January-2022</date>
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Calculating the Souls of Black Folk: Predictive Analytics in the New York City Administration for Children&apos;s Services</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Khadijah</forename><surname>Abdurahman</surname></persName>
		</author>
		<ptr target="https://journals.library.columbia.edu/index.php/cjrl/article/view/8741" />
	</analytic>
	<monogr>
		<title level="j">Columbia Journal of Race and Law Forum</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="75" to="110" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">If You Give a Judge a Risk Score: Evidence from Kentucky Bail Decisions. Law, Economics, and Business Fellows</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Albright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discussion Paper Series</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Street-level algorithms: A theory at the gaps between policy and decisions</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Alkhatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Allegheny County Department of Human Services</title>
		<ptr target="https://www.alleghenycounty.us/WorkArea/linkit.aspx?LinkIdentifer=id&amp;ItemID=6442461672" />
	</analytic>
	<monogr>
		<title level="m">DHS response to Automated Inequality by Virginia Eubanks</title>
				<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Does the whole exceed its parts? the efect of ai explanations on complementary team performance</title>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Fok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using thematic analysis in psychology</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1191/1478088706qp063oa</idno>
		<ptr target="https://www.tandfonline.com/doi/pdf/10.1191/1478088706qp063oa" />
	</analytic>
	<monogr>
		<title level="j">Qualitative Research in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="101" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proxy tasks and subjective measures can be misleading in evaluating explainable ai systems</title>
		<author>
			<persName><forename type="first">Zana</forename><surname>Buçinca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><forename type="middle">L</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName><surname>Glassman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Intelligent User Interfaces</title>
				<meeting>the 25th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gender Shades: Intersectional Phenotypic and Demographic Evaluation of Face Datasets and Gender Classifers</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology (MIT</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classifcation</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research Conference on Fairness, Accountability, and Transparency (Conference on Fairness, Accountability, and Transparency</title>
				<meeting>Machine Learning Research Conference on Fairness, Accountability, and Transparency (Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Carrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Terry</surname></persName>
		</author>
		<title level="m">Hello AI&quot;: Uncovering the Onboarding Needs of Medical Practitioners</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">We visited Allegheny County CYF&apos;s Intake Department in July 2021 when many call screen workers were working from home, due to the COVID-19 pandemic. This has had an impact on the CYF call screening workfow since March 2020. for Human-AI Collaborative Decision-Making</title>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building classifers with independency constraints</title>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshop Domain Driven Data Mining</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constructing Grounded Theory</title>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Charmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAGE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">We will use all resources to keep children safe</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Cherna</surname></persName>
		</author>
		<ptr target="https://www.post-gazette.com/opinion/letters/2018/03/23/We-will-use-all-resources-to-keep-children-safe/stories/201803230094Online" />
		<imprint/>
	</monogr>
	<note>Pittsburgh Post-Gazette</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Benavides-Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Fialko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhema</forename><surname>Vaithianathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability and Transparency. PMLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="134" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Eckerd</forename><surname>Rapid</surname></persName>
		</author>
		<author>
			<persName><surname>Feedback</surname></persName>
		</author>
		<ptr target="https://eckerd.org/family-children-services/ersf/.Online" />
		<imprint>
			<date type="published" when="2021-09">September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithmic decision making and the cost of fairness</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aziz</forename><surname>Huq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining</title>
				<meeting>the 23rd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Counterfactual risk assessments, evaluation, and fairness</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Coston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mishler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="582" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Bo</forename><surname>Cowgill</surname></persName>
		</author>
		<ptr target="http://www.columbia.edu/~bc2656/papers/RecidAlgo.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Impact of Algorithms on Judicial Discretion: Evidence from Regression Discontinuities</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ethical Analysis: Predictive Risk Models at Call Screening for Allegheny County</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Gambrill</surname></persName>
		</author>
		<ptr target="https://www.alleghenycountyanalytics.us/wp-content/uploads/2019/05/Ethical-Analysis-16-ACDHS-26_PredictiveRisk_Package_050119_FINAL-2.pdfOnline" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning under selective labels in the presence of expert consistency</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Dubrawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00905</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Leveraging expert consistency to improve algorithmic decision support</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Dubrawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09648</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Fogliato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Racial disproportionality and disparities in the child welfare system: Why do they exist, and what can be done to address them?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiko</forename><surname>Dettlaf</surname></persName>
		</author>
		<author>
			<persName><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ANNALS of the American Academy of Political and Social Science</title>
		<imprint>
			<biblScope unit="volume">692</biblScope>
			<biblScope unit="page" from="253" to="274" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disentangling substantiation: The infuence of race, income, and risk on the substantiation decision in child welfare</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Dettlaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">L</forename><surname>Rivaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><forename type="middle">R</forename><surname>Fluke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Rycraft</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Children and Youth Services Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1630" to="1637" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Shattered bonds: The color of child welfare</title>
		<author>
			<persName><forename type="first">Dorothy</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Basic Books</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Racial bias in child protection? A comparison of competing explanations using national data</title>
		<author>
			<persName><forename type="first">Brett</forename><surname>Drake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">M</forename><surname>Jolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Lanier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Fluke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonson-Reid</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="471" to="478" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The accuracy, fairness, and limits of predicting recidivism</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Dressel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5580</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd innovations in theoretical computer science conference</title>
				<meeting>the 3rd innovations in theoretical computer science conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dermatologist-level classifcation of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automating Inequality: How High-tech Tools Profle, Police, and Punish the Poor</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Eubanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>St. Martin&apos;s Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What can ai do for me? evaluating machine learning interpretations in cooperative play</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces</title>
				<meeting>the 24th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Child Welfare Information Gateway</title>
		<ptr target="https://www.childwelfare.gov/topics/systemwide/assessment/family-assess/safety/On-line" />
		<imprint>
			<date type="published" when="2021-12">Dec-2021</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
	<note>Safety and Risk Assessment</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Impact evaluation of a predictive risk modeling tool for Allegheny county&apos;s child welfare ofce</title>
		<author>
			<persName><forename type="first">Jeremy D Goldhaber-Fiebert</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Allegheny County</publisher>
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Flaws of Policies Requiring Human Oversight of Government Algorithms</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Green</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3921216</idno>
		<ptr target="http://dx.doi.org/10.2139/ssrn.3921216" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Fairness, Accountability, and Transparency (FAT*)</title>
				<meeting>Conference on Fairness, Accountability, and Transparency (FAT*)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The principles and limits of algorithm-in-theloop decision making</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human decision making with machine assistance: An experiment on bailing and jailing</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Grgić-Hlača</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pupil race and elementary school ability grouping: Are teachers biased against Black children?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emil</surname></persName>
		</author>
		<author>
			<persName><surname>Haller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Educational Research Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="465" to="483" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3315" to="3323" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Designing for human-AI complementarity in K-12 education</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Aleven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01266</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Student learning benefts of a mixed-reality teacher awareness tool in AI-enhanced classrooms</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><surname>Aleven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artifcial Intelligence in Education</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="154" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shifting Concepts of Value: Designing Algorithmic Decision-Support Systems for Public Services</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Naja Holten Møller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">T</forename><surname>Shklovski</surname></persName>
		</author>
		<author>
			<persName><surname>Hildebrandt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3419249.3420149</idno>
		<ptr target="https://doi.org/10.1145/3419249.3420149" />
	</analytic>
	<monogr>
		<title level="j">NordiCHI</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Can an Algorithm Tell When Kids Are in Danger? New York Times Magazine</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hurley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01-02">2018/01/02. September-2021</date>
		</imprint>
	</monogr>
	<note>/magazine/can-an-algorithmtell-when-kids-are-in-danger. html Online; accessed 8-</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sas Institute</surname></persName>
		</author>
		<ptr target="https://www.sas.com/en_us/software/analytics-for-child-well-being.html.Online" />
		<imprint>
			<date type="published" when="2021-12">December-2021</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sherif's Justice Institute</surname></persName>
		</author>
		<ptr target="https" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Central Bond Court Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Data mining program designed to predict child abuse proves unreliable, DCFS says</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marx</surname></persName>
		</author>
		<ptr target="https://www.chicagotribune.com/investigations/ct-dcfs-eckerd-met-20171206-story.htmlOnline" />
		<imprint>
			<date type="published" when="2022-01">January-2022</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Chicago Tribune</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Noise: a faw in human judgment</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Sibony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cass</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Little, Brown</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Residual unfairness in fair machine learning from prejudiced data</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2439" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Algorithmic Justice in Child Protection: Statistical Fairness, Social Justice and the Implications for Practice</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Keddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Sciences</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Combining Human Predictions with Model Probabilities via Confusion Matrices and Calibration</title>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02744</idno>
		<title level="m">Dominik Janzing, and Bernhard Schölkopf. 2017. Avoiding discrimination through causal reasoning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lifetime Prevalence of Investigating Child Maltreatment Among US Children</title>
		<author>
			<persName><forename type="first">Hyunil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wildeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Jonson-Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Drake</surname></persName>
		</author>
		<idno type="DOI">10.2105/AJPH.2016.303545</idno>
		<ptr target="https://doi.org/10.2105/AJPH.2016.303545" />
	</analytic>
	<monogr>
		<title level="j">American Journal of Public Health</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="274" to="280" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Human decisions and machine predictions</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The quarterly journal of economics</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="237" to="293" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards Building Model-Driven Tutorials for Humans</title>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>Why is&apos; Chicago&apos;deceptive?</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On human predictions with explanations and predictions of machine learning models: A case study on deception detection</title>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Algorithms and Decision-Making in the Public Sector</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyla</forename><forename type="middle">E</forename><surname>Chasalow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Law and Social Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Explainable machine-learning predictions for the prevention of hypoxaemia during surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bala</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayumi</forename><surname>Vavilala</surname></persName>
		</author>
		<author>
			<persName><surname>Horibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Eisses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel King-Wai</forename><surname>Liston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Fang</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="760" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Predict responsibly: improving fairness and accuracy by learning to defer</title>
		<author>
			<persName><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06664</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The cost of fairness in binary classifcation</title>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability and Transparency</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Computer-aided prostate cancer diagnosis from digitized histopathology: a review on texture-based systems</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Mosquera-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sos</forename><surname>Agaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Velez-Hoyos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE reviews in biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName><surname>Nash</surname></persName>
		</author>
		<title level="m">EXAMINATION OF USING STRUCTURED DECISION MAKING AND PREDICTIVE ANALYTICS IN ASSESSING SAFETY AND RISK IN CHILD WELFARE (ITEM NO. 49-A, AGENDA OF SEPTEMBER</title>
				<imprint>
			<date type="published" when="2016">2017. 2016</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Los Angeles County quietly drops its frst child welfare predictive analytics experiment</title>
		<ptr target="https://www.nccprblog.org/2017/05/los-angeles-county-quietly-drops-its.htmlOnline" />
		<imprint>
			<date type="published" when="2017-01">2017. January-2022</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>NCCPR: National Coalition for Child Protection Reform</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Predictive analytics in Pittsburgh child welfare: Was the &quot;ethics review&quot; of Allegheny County&apos;s &quot;scarlet number</title>
		<ptr target="https://www.nccprblog.org/2018/03/predictive-analytics-in-pittsburgh.htmlOnline" />
		<imprint>
			<date type="published" when="2018-09">2018. September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
		<respStmt>
			<orgName>NCCPR: National Coalition for Child Protection Reform</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">No, you can&apos;t use predictive analytics to reduce racial bias in child welfare</title>
		<ptr target="https://www.nccprblog.org/2019/06/no-you-cant-use-predictive-analytics-to.htmlOnline" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>NCCPR: National Coalition for Child Protection Reform</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Protection from Discrimination in Child Welfare Activities</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<ptr target="https://www.hhs.gov/civil-rights/for-individuals/special-topics/adoption/index.htmlOnline" />
		<imprint>
			<date type="published" when="2022-01">January-2022</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Department of Health &amp; Human Services</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Allegheny County Department of Human Services</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Allegheny Family Screening Tool, Frequently-Asked Questions | Updated</title>
		<ptr target="https://www.alleghenycountyanalytics.us/wp-content/uploads/2018/10/17-ACDHS-11_AFST_102518.pdf.Online" />
		<imprint>
			<date type="published" when="2018-08">August 2018. September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Impact Evaluation Summary of the Allegheny Family Screening Too</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Allegheny County</publisher>
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
	</monogr>
	<note>Allegheny County Department of Human Services</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">National Council on Crime and Delinquency Children&apos;s Research Center</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">The Structured Decision Making System for Child Protective Services Policy and Procedures Manual</title>
		<ptr target="https://www.cdss.ca.gov/Portals/9/SDM%20Policy%20and%20Procedure%20Manual.pdf.Online" />
		<imprint>
			<date type="published" when="2021-09">September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Human-machine partnership with artifcial intelligence for chest radiograph diagnosis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bhavik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregg</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Willcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mimi</forename><surname>Baltaxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajan</forename><surname>Amrhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safwan</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Halabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">What you see is what you get? the impact of representation criteria on human bias in hiring</title>
		<author>
			<persName><forename type="first">Andi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kıcıman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kori</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</title>
				<meeting>the AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Predictive policing: The role of crime forecasting in law enforcement operations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Walt</surname></persName>
		</author>
		<author>
			<persName><surname>Perry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Rand Corporation</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Manipulating and measuring model interpretability</title>
		<author>
			<persName><forename type="first">Forough</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><surname>Wortman Wortman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Actionable auditing: Investigating impact of publicly naming biased performance results of commercial ai products</title>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><surname>Buolamwini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
				<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="429" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Can Big Data Help Save Abused Kids?</title>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riley</forename></persName>
		</author>
		<ptr target="https://reason.com/2018/01/22/can-big-data-help-save-abused/Online" />
		<imprint>
			<date type="published" when="2018-09">2018. September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Digitizing the Carceral State (Review</title>
		<author>
			<persName><forename type="first">Dorothy</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Law Review</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="1695" to="1728" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Samant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kath</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">and Sophie Beiers. 2021. Family Surveillance by Algorithm: The Rapidly Spreading Tools Few Have Heard Of</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">American Civil Liberties Union (ACLU)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Foiled by FOIL: How One City Agency Has Dragged Out a Request for Public Records for Nearly a Year</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Sapien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ProPublica</title>
		<imprint>
			<date type="published" when="2016-04">2016. April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A Framework of High-Stakes Algorithmic Decision-Making for the Public Sector Developed through a Case Study of</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karla</forename><surname>Badillo-Urquiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shion</forename><surname>Guha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2107.03487v2</idno>
	</analytic>
	<monogr>
		<title level="j">Child-Welfare. arXiv</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A Human-Centered Review of Algorithms used within the US Child Welfare System</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karla</forename><surname>Badillo-Urquiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><forename type="middle">J</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shion</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">CYF: An agency that works, helping kids and their families</title>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">White</forename><surname>Stack</surname></persName>
		</author>
		<ptr target="https://www.post-gazette.com/opinion/Op-Ed/2018/02/11/CYF-An-agency-that-works-helping-kids-and-their-families/stories/201802040037Online" />
		<imprint>
			<date type="published" when="2021-09">September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>Pittsburgh Post-Gazette</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Juvenile Detention Risk Assessment: A Practice Guide to Juvenile Detention Reform</title>
		<author>
			<persName><forename type="first">David</forename><surname>Steinhart</surname></persName>
		</author>
		<ptr target="https://assets.aecf.org/m/resourcedoc/aecf-juveniledetentionriskassessment1-2006.pdf" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Assessing Risk Assessment in Action. Law, Economics, and Business Fellows</title>
		<author>
			<persName><forename type="first">Megan</forename><surname>Stevenson</surname></persName>
		</author>
		<ptr target="https://scholarship.law.umn.edu/mlr/58/" />
	</analytic>
	<monogr>
		<title level="j">Discussion Paper Series</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="303" to="384" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Algorithmic Risk Assessment in the Hands of Humans</title>
		<author>
			<persName><forename type="first">Megan</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Doleac</surname></persName>
		</author>
		<ptr target="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3489440" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Investigating human + machine complementarity for recidivism predictions</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kori</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09123</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">MindShare Technology</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Improving Outcomes using data you already have</title>
		<ptr target="https://mindshare-technology.com/analytics/.Online" />
		<imprint>
			<date type="published" when="2021-09">September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Why machine learning may lead to unfairness: Evidence from risk assessment for juvenile justice in catalonia</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Songül Tolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artifcial Intelligence and Law</title>
				<meeting>the Seventeenth International Conference on Artifcial Intelligence and Law</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classifcation: an open, web-based, international, diagnostic study</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisa</forename><surname>Bengü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Akay</surname></persName>
		</author>
		<author>
			<persName><surname>Argenziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName><surname>Hofmann-Wellenhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Oncology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="938" to="947" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On the relation between accuracy and fairness in binary ˙ classifcation</title>
		<author>
			<persName><surname>Indre ˙ Zliobaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Fairness, Accountability, and Transparency in Machine Learning (FATML)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Implementing a Child Welfare Decision Aide in Douglas County | Methodology Report</title>
		<author>
			<persName><forename type="first">Rhema</forename><surname>Vaithianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haley</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allon</forename><surname>Kalisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chamari</forename><surname>Kithulgoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megh</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Mayur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><forename type="middle">Benavides</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><surname>Prado</surname></persName>
		</author>
		<ptr target="https://csda.aut.ac.nz/__data/assets/pdf_fle/0009/347715/Douglas-County-Methodology_Final_3_02_2020.pdf.Online" />
		<imprint>
			<date type="published" when="2021-09">September-2021</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Developing Predictive Risk Models to Support Child Maltreatment Hotline Screening Decisions</title>
		<author>
			<persName><forename type="first">Rhema</forename><surname>Vaithianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parma</forename><surname>Nand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Putnam-Hornstein</surname></persName>
		</author>
		<ptr target="https://www.alleghenycountyanalytics.us/wp-content/uploads/2017/04/Developing-Predictive-Risk-Models-package-with-cover-1-to-post-1.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rhema</forename><surname>Vaithianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Putnam-Hornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><forename type="middle">Benavides</forename><surname>Prado</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title/>
		<ptr target="https://www.alleghenycountyanalytics.us/wp-content/uploads/2019/05/Methodology-V2-from-16-ACDHS-26_PredictiveRisk_Package_050119_FINAL-7.pdf" />
	</analytic>
	<monogr>
		<title level="j">Allegheny Family Screening Tool: Methodology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Hospital injury encounters of children identifed by a predictive risk model for screening child maltreatment referrals: evidence from the Allegheny Family Screening Tool</title>
		<author>
			<persName><forename type="first">Rhema</forename><surname>Vaithianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Putnam-Hornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Benavides-Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA pediatrics</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Brilliant AI Doctor&quot; in Rural Clinics: Challenges in AI-Powered Clinical Decision Support System Deployment</title>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>Xiangmin Fan, and Feng Tian. 2021</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Not in it for Justice&quot;: How California&apos;s Pretrial Detention and Bail System Unfairly Punishes Poor People</title>
		<ptr target="https://www.hrw.org/report/2017/04/11/not-itjustice/how-californias-pretrial-detention-and-bail-system-unfairly" />
	</analytic>
	<monogr>
		<title level="j">Human Rights Watch</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00582</idno>
		<title level="m">Learning to complement humans</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Unremarkable AI: Fitting intelligent decision support into critical, clinical decision-making processes</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300468</idno>
		<idno type="arXiv">arXiv:1904.09612</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300468" />
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Efect of confdence and explanation on accuracy and trust calibration in ai-assisted decision making</title>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Bellamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Value-sensitive algorithm design: Method, case study, and lessons</title>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Halfaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Terveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
