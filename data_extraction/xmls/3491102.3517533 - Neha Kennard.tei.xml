<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;Because AI is 100% right and safe&quot;: User Attitudes and Sources of AI Authority in India</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shivani</forename><surname>Kapania</surname></persName>
							<email>kapania@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Siy</surname></persName>
							<email>siyj@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Research Cambridge</orgName>
								<address>
									<region>Massachusetts</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Gabe Clapper</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Google Research Seattle</orgName>
								<address>
									<settlement>Washington</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Azhagu Meena SP</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Google Research Bengaluru</orgName>
								<address>
									<country>India Nithya Sambasivan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Google Research Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;Because AI is 100% right and safe&quot;: User Attitudes and Sources of AI Authority in India</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3491102.3517533</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-07-22T04:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial intelligence</term>
					<term>perceptions of AI</term>
					<term>algorithmic decision-making</term>
					<term>India</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most prior work on human-AI interaction is set in communities that indicate skepticism towards AI, but we know less about contexts where AI is viewed as aspirational. We investigated the perceptions around AI systems by drawing upon 32 interviews and 459 survey respondents in India. Not only do Indian users accept AI decisions (79.2% respondents indicate acceptance), we find a case of AI authority-AI has a legitimized power to influence human actions, without requiring adequate evidence about the capabilities of the system. AI authority manifested into four user attitudes of vulnerability: faith, forgiveness, self-blame, and gratitude, pointing to higher tolerance for system misfires, and introducing potential for irreversible individual and societal harm. We urgently call for calibrating AI authority, reconsidering success metrics and responsible AI approaches and present methodological suggestions for research and deployments in India.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Human-centered computing → Empirical studies in HCI; • Social and professional topics → Cultural characteristics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A growing body of research on public attitudes toward algorithmic systems indicates skepticism and moderate acceptance towards these technologies in contexts such as the US, UK and Germany <ref type="bibr">[66]</ref>, where studies report that individuals express concerns about the fairness and usefulness of these systems <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b114">117]</ref>. The research on improving human-AI interactions is thus often set in communities where studies indicate user mistrust towards algorithmic systems 1 <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b78">80]</ref>, with a goal to mitigate harm to users, while many are also explicitly motivated to increase the user acceptance of technologies through various approaches like explainability <ref type="bibr" target="#b24">[26]</ref>, privacy <ref type="bibr" target="#b20">[22]</ref> and transparency <ref type="bibr" target="#b109">[112]</ref>. However, the acceptance among users may be shaped by specific online trajectories and exposure levels, possibly not generalizing to contexts with newer Internet citizens from under-researched socio-cultural settings.</p><p>In particular, AI deployments in India are emerging in several niche, high-stakes areas (e.g., healthcare <ref type="bibr" target="#b98">[100,</ref><ref type="bibr" target="#b126">128]</ref>, finance <ref type="bibr" target="#b113">[116]</ref>, agriculture <ref type="bibr" target="#b25">[27]</ref>). Marda et al. <ref type="bibr" target="#b80">[82]</ref> describe how AI is also emerging as a focus for policy development in India. Prior research, however, presents a case for techno-optimism among technology users that envision technology with a socio-economic promise <ref type="bibr">[95]</ref> for India. AI is viewed aspirationally, as a means to development and modernity <ref type="bibr">[14]</ref>, potentially leading to adoption of high-stakes solutions, often before sufficient ethics testing takes place <ref type="bibr">[107]</ref>. As the world's second largest Internet user population <ref type="bibr" target="#b57">[59]</ref>, it is important to understand how Indian users perceive AI systems.</p><p>In this research, we draw upon, and extend the concept of algorithmic authority proposed by Lustig and Nardi <ref type="bibr" target="#b79">[81]</ref> to AI applications. We define AI authority as the legitimized power of AI to influence human actions, without requiring adequate evidence about the capabilities of the given system. We center the term 'Artificial Intelligence' to investigate public perceptions and contextual factors, which for instance, Kozyreva and Herzog et al. <ref type="bibr" target="#b64">[65]</ref> have also reported to be significantly more familiar (86% vs. 58%) among the German public as opposed to more technical terms such as 'Computer algorithms'.</p><p>In this paper, we describe the characteristics of authority of AI, the sources through which AI systems derive authority, and the user attitudes that AI authority manifests into-from 32 interviews and 459 survey responses across various domains and conditions, set in India. In our study, 79.2% of survey respondents were willing to accept AI decisions. A high acceptance for AI held true even in highstakes 2 scenarios, with 73% respondents indicating acceptance of AI decisions for medical diagnosis, loan approval and hiring. AI authority is legitimized in our study, with participants demonstrating both acceptance and willingness to act upon AI-based decisions.</p><p>The sources through which an AI derived authority were often extraneous to the system, and not indicative of the reliability, usefulness, or effectiveness of the given AI system. AI authority was heavily influenced by institutional, infrastructural and societal factors that lay outside the boundaries of the AI-users' interactions with ineffectual institutions, polarized narratives and misleading terminologies, users' prior experiences with technology, and the availability of human systems around them. For example, many participants were left exasperated by the corrupt or discriminatory practices in their interactions with financial institutions, which is why they perceived AI as a better alternative to avoid those forms of exploitation. AI authority manifests into four user attitudes towards AI: faith in AI capabilities, blaming themselves for adverse and biased outcomes, forgiveness, and gratitude towards AI. Taken together, our results indicate how AI authority led to a greater tolerance for AI harms, and lower recognition of AI biases. An authority of AI in our study meant that participants under-recognized and even tolerated AI harms, posing serious questions on adverse impacts of system errors, bias, abuse, and misfires on nascent Internet users.</p><p>Our work has implications for the design of responsible AI systems, by highlighting that users from under-studied settings could have different attitudes towards and behaviors with AI due to the contextual or non-technological factors. Our results indicate that there is already a high acceptance towards adopting AI systems among Indian technology users, which means we must approach design and research differently (e.g., by supporting people in maintaining a healthy distrust and critical awareness of AI). If AI authority remains over-calibrated, even high-risk and under-tested AI applications may receive user acceptance relatively quickly, and thus might be easily adopted for public use. We call for (i) efforts to calibrate authority, and greater attention towards the trajectory of users that begin from a place of authority for AI and a cultural mistrust for human institutions, (ii) embracing the variability in AI understanding, (iii) redefining success metrics and (iv) developing and disseminating alternative narratives around AI. Our paper makes three main contributions, we:</p><p>-present evidence for AI authority in India, and describe its characteristics -empirically document the sources influencing AI authority beyond system development procedures provide implications for researchers and designers in introducing AI technologies in India or other contexts with authority towards AI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Researchers have studied and documented the ways in which the design of AI-infused systems diverges from traditional user interface designs <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b32">34]</ref>. Most of these differences are attributed to the probabilistic nature of AI, which heavily relies on nuances of various task design and system settings, and often manifests as inconsistent behavior over time or across users <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b70">72]</ref>. Below, we situate our work in a body of related research on trust and algorithmic authority, perceptions about algorithmic systems and decisions, algorithmic folk theories, and techno-optimism in India.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Trust and algorithmic authority</head><p>Understanding the role and measurement of trust continues to be an active topic of research for several decades, across various disciplines such as interpersonal relationships <ref type="bibr" target="#b102">[104]</ref>, organizational studies <ref type="bibr" target="#b87">[89]</ref>, consumer relations [87], and technology <ref type="bibr" target="#b60">[61]</ref>. Several definitions and frameworks have emerged from the widespread and varied interest in trust <ref type="bibr" target="#b56">[58,</ref><ref type="bibr" target="#b81">83,</ref><ref type="bibr" target="#b102">104]</ref>. Lee and See <ref type="bibr" target="#b71">[73]</ref> define trust in automation as "an attitude that an agent will help achieve an individual's goals in a situation characterized by uncertainty and vulnerability. ", and used the framework by Fishbein and Ajzen <ref type="bibr" target="#b40">[42]</ref> to characterize trust as an attitude of the individual, rather than an intention or a behavior. Beyond defining what it means to trust, researchers have devised frameworks for the factors that contribute to trust. Mayer, Davis and Schoorman <ref type="bibr" target="#b81">[83]</ref> present an integrative model of trustworthiness with three components: ability, integrity, and benevolence. Ability refers to the competencies that are required for a trustee to have influence in a specific domain. Integrity relates to following a set of principles that are acceptable to the trustor. Benevolence refers to the extent to which the intentions of the trustee align with the trustor. We utilize these frameworks in the following sections to reflect on our results and present implications.</p><p>Extensive prior research has explored constructs to understand the emergent dynamics between human and algorithmic interactions (e.g., AI utopia <ref type="bibr" target="#b108">[111]</ref>, algorithmic appreciation [79], algorithm aversion <ref type="bibr" target="#b30">[32]</ref>, described below). Santow <ref type="bibr" target="#b108">[111]</ref> described AI utopia as an imagined future with improvements in every aspect of life. In this research, we draw inspiration from Lustig and Nardi's work <ref type="bibr" target="#b79">[81]</ref> that introduced the concept of algorithmic authority grounded in their mixed-methods research of the Bitcoin community. Users perceived Bitcoin as an 'apolitical currency', as more reliable and trustworthy than banks or governments, and the algorithmic was seen as a form of explicit resistance against traditional financial institutions. The concept of trust is of particular importance to algorithmic authority. Bitcoin users place trust in the algorithmic authority: trust is an attitude that mediates relationships between humans and the algorithmic authority. Lustig and Nardi discuss the ways in which Bitcoin was perceived to possess a predictability which led to greater trust in the algorithmic authority. However, the algorithmic authority of Bitcoin was mediated by human judgment, the human oversight even if the algorithmic was perceived as 'self-contained'. In this research, we extend the work of Lustig and Nardi to examine the perceptions and acceptance of AI-based applications among Indian users, and the factors which contribute to these perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Perceptions of algorithmic systems and decisions</head><p>The communities of HCI and CSCW have investigated the perceptions of algorithmic decision-making in comparison with human decisions (e.g., <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b17">19]</ref>) with conflicting results. Most studies find that individuals are more likely to trust human decisions over algorithmic decisions for tasks that require human-like skills (e.g., subjective judgment, emotion) (e.g., <ref type="bibr" target="#b78">[80]</ref>). For instance, Lee et al.</p><p>[74] explore perceptions of fairness and trust for four managerial decisions and find that human decisions are considered fairer and more trustworthy than algorithmic decisions for tasks requiring human skills (hiring and work evaluation). Dietvorst et al. <ref type="bibr" target="#b30">[32]</ref> found that human forecasters are resistant to using algorithms, especially after seeing them perform (algorithm aversion). Logg, Minson, and Moore [79], however, present contradicting results: they find that people rely on algorithmic advice over human advice for forecasting (algorithmic appreciation), but this effect waned when they had to choose between their own judgment vs. an algorithm. Others in the community have studied human perceptions of AI through a wide-ranging set of methods <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b62">63]</ref>. Cave, Couglan and Dihal <ref type="bibr" target="#b18">[20]</ref> conducted a survey to examine public responses to AI in the UK, and find that 25% participants associated AI with robots. Prior research has also investigated worker perceptions of technology in organizations <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b91">93]</ref>. Höddinghaus et al. <ref type="bibr" target="#b46">[48]</ref> compared workers' trustworthiness perceptions of automated vs. human leadership. Their results indicate workers perceived automated leadership agents as being higher on integrity, but lower on benevolence than human leadership agents. Nagtegaal <ref type="bibr" target="#b86">[88]</ref> demonstrate the role of task quantifiability/complexity in perceptions of procedural justice: decision automation was perceived as less fair in tasks with higher complexity (and lower quantifiability <ref type="bibr" target="#b69">[71]</ref>).</p><p>A related area of research examines the factors influencing perceptions of trust, algorithmic fairness and justice (e.g., <ref type="bibr" target="#b73">[75,</ref><ref type="bibr" target="#b133">136]</ref>). People's perception of fairness can be complicated and nuanced, and go beyond formal algorithmic constraints. <ref type="bibr">Araujo et al. [7]</ref> explored the role of individual characteristics in perceptions about automated decision-making, and report that people with more knowledge about AI are more optimistic about algorithmic decision-making. Ashoori and Weisz <ref type="bibr" target="#b8">[10]</ref> report that model interpretability and transparency about training/testing datasets played a critical role in establishing trust. However, recent research by <ref type="bibr">Wang et al. [130]</ref> argues that for non-expert stakeholders, the effect of development procedures on perceived fairness is much smaller as compared to algorithmic outcomes and biases.</p><p>Relatively less work has been done to uncover the perceptions and acceptance of algorithmic systems in India (e.g., <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b82">84]</ref>). Okolo et al. <ref type="bibr" target="#b90">[92]</ref> studied the perceptions of community health workers (CHWs) in rural India towards AI, and find that CHWs considered AI applications trustworthy with expertise greater than their own. Thakkar et al. <ref type="bibr" target="#b119">[122]</ref> examined the perceptions and practices of vocational workers towards automated systems. We extend this body of work by focusing on the ways in which perceptions and intentions are shaped through factors beyond system design. We deliberately focus on 'AI systems' as opposed to 'algorithmic systems', given the specific associations that people make through terminologies and conceptual metaphors <ref type="bibr" target="#b39">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Algorithmic folk theories</head><p>Algorithmic folk theories are conceptions to explain algorithmic behavior and outcomes that are informally shared among similar members of a culture, but are not necessarily accurate <ref type="bibr" target="#b26">[28]</ref>. Prior CSCW and HCI literature has explored people's understanding of social media algorithms <ref type="bibr" target="#b29">[31,</ref><ref type="bibr">45]</ref>, and find that users develop these folk models through their experience with technologies and social interactions. Their results indicate that a majority of people have high-level folk theories about how these algorithms work. Rader and Gray <ref type="bibr" target="#b100">[102]</ref> studied user beliefs about news feed curation on Facebook, the algorithmic behaviors that users notice and the behaviors to which they respond. Taking it further, Eslami et al. <ref type="bibr" target="#b34">[36]</ref> uncovered and codified the folk theories of the Facebook News Feed curation. Most people become aware of algorithmic involvement through unexpected behavior, which causes them to adjust their future actions.</p><p>Prior research has also made headway in examining user awareness of algorithms, and the role of that awareness in their interactions. Bucher <ref type="bibr" target="#b15">[17]</ref>  Algorithmic skills involve the ability to identify that a dynamic system is in place (awareness), and reflecting on how those systems work (understanding) <ref type="bibr">[45]</ref>. Another recent thread of work focuses on the ways in which users adapt behaviors around algorithms <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b53">55]</ref>. Eslami et al. investigate the ways in which users perceive and manage bias in online rating platforms <ref type="bibr" target="#b36">[38]</ref>, and propose the concept of 'bias aware design' to harness the power of users to reveal algorithmic biases.</p><p>Most of this research has been conducted with US or EU-based respondents, largely with social media, and/or MTurk users who are known to be tech-savvier than the average Internet user <ref type="bibr" target="#b44">[46]</ref>. US and EU tend to have critical media discourse around the use of algorithmic systems <ref type="bibr">[64,</ref><ref type="bibr" target="#b116">119]</ref>, scrutiny about algorithmic biases from activists and civil society [3], and existing/emerging laws and regulations around the use of AI <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b103">105]</ref>. This is in contrast with India, which currently lacks substantial research on responsible AI development <ref type="bibr">[107]</ref>. Algorithmic folk theories are shaped through the above elements, which differ greatly between the contexts previously studied and our study site. In addition, our goal is not to provide explicit folk theories, instead, we connect folk understandings with users' intentions to engage with, and act upon AI decisions or recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Techno-optimism and modernity in India</head><p>Prior work in HCI and ICTD has studied the discourses around technology in India, and how it has been frequently tied with notions of development <ref type="bibr">[95]</ref>. The last two decades, in particular, have been crucial in shaping technology as the means for prosperity in India. Digital technologies are viewed as a vehicle for progress, as a solution to societal problems in developing countries <ref type="bibr" target="#b14">[16]</ref>. The following is an excerpt from Prime Minister's speech in 2018 [86]: "Can Artificial Intelligence help us detect serious health conditions before they manifest physically? Can Artificial Intelligence help our farmers make the right decisions regarding weather, crop and sowing cycle? Friends, our Government is of the firm belief, that we can use this power of twenty-first century technology [AI] to eradicate poverty and disease. In doing so, we can bring prosperity to our poor and under-privileged sections. We are committed to achieving this vision. "</p><p>The Indian government's vision for technological development manifested through two major initiatives over the recent years. First, the introduction of Aadhaar-a biometric identification system for 1.3 billion citizens, which was legitimized through a promise for poverty reduction and financial inclusion <ref type="bibr" target="#b117">[120]</ref>. Secondly BHIM, an application for digital payments introduced soon after demonetization as the future for cashless payments <ref type="bibr">[97]</ref>. Technology played a symbolic and functional role in enabling the 'leapfrogging' into the modern era. <ref type="bibr">Bozarth and Pal [14]</ref> examine the social media (Twitter) discourse, and find that politicians have an inclination to discuss technology in connection with development as part of their political messaging. Public discourse around technology and AI has been hyper-optimistic from the general public, the tech industry, and the government <ref type="bibr" target="#b94">[96]</ref>, with several deployments underway <ref type="bibr" target="#b88">[90]</ref>. <ref type="bibr">Sambasivan et al. [107]</ref> describe how due to the aspirational role played by AI, high risk AI solutions are often adopted too quickly in India, including in predictive policing and facial recognition, without sufficient checks and balances. Our research aligns with this perspective on effects of techno-optimism and extends this body of literature by presenting findings of the ways in which this aspirational value is shaping AI authority among Indian technology users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We use a mixed-methods research approach with an exploratory design <ref type="bibr" target="#b48">[50]</ref> in which the qualitative data serves as the foundation of the inquiry. In this design, the qualitative data is collected and analyzed first, followed by the collection and analysis of the quantitative data which typically uses constructs based on the emergent phenomenon from the qualitative study. First we conducted semi-structured interviews to investigate the perceptions about AI systems, users' intentions, and the ways in which those relate to their conceptions of AI. We then implemented a survey as a follow up study to measure the acceptance of AI-based outcomes with our target population. The quantitative research complements the qualitative by presenting results on acceptability with a larger sample size, and establish baseline data for Indian technology users. We describe implementation details for each method in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interviews</head><p>We conducted interviews with 32 adult Internet users based in different regions within India to understand perceptions of AI and their acceptance of AI-based decisions. Participants were located in various tier 1 and tier 2 cities of India, belonging to diverse age groups and occupations. Our sample also consisted of a mix of internet experience, with 16 nascent internet users that first accessed the internet only in the last 2 years, and the remaining 16 more 'experienced' that have been online for more than 2 years. We interviewed 17 male and 15 female participants. Refer to table 1 for details on participant demographics.</p><p>Participant recruitment and moderation. We recruited participants through a combination of a UX Research database internal to our organization, and a market research company Dowell. We conducted 'screener' conversations with potential participants to ask if "[they] had heard of the words AI or Artificial Intelligence?", and if yes, we asked them to describe what it means. We selected people who were aware of these terminologies and did not filter any participant based on their descriptions of AI. The sessions were conducted using video conferencing, due to COVID-19 travel limitations, and each interview lasted 75-90 minutes. We conducted interviews in three languages (Hindi, Tamil and English) by allowing participants to select the language with which they are most comfortable. The interview notes were recorded through field notes and video recording, and transcribed within 24 hours of each interview by the corresponding moderator. Each participant received a thank-you gift in the form of a gift card worth 27 USD or 2000 INR.</p><p>Interview structure. In line with our goal, each interview had structured sub-sections beginning with the participants' general perceptions and understanding of AI. We asked participants to share "in <ref type="bibr">[their]</ref> opinion, what do [they] think Artificial Intelligence means?", and, "overall, do [they] think AI is doing more good or more harm for us? Why? In what ways?" To explore whether participants are able to recognize AI, they were then shown a series of images each containing commonly used AI applications in India across domains <ref type="bibr" target="#b129">[131]</ref>, and requested to identify if and how those products use AI or not. In the main part of the interview, we used a scenario-based approach (details in the following section) to probe experiences, attitudes and intentions to act. For each scenario, we began by presenting a scenario description accompanied with visuals, and then invited participants to share their initial reactions. We explicitly clarified that the applications in the discussed scenarios are built using AI. Following questions aligned with our goal of understanding participants' acceptance of AI technologies, how it relates to their conceptions of AI, and the factors that influence these perceptions. We asked questions around their beliefs and intentions (e.g., if AI made a decision on your loan application, would you believe it to be correct?), their preferences for human vs. AI decision-maker (e.g., What are the differences between a human making a decision on your loan application and an AI making that decision?), the kinds of information they would like to know about the system, and the level of control they believed the AI application should have in the given scenario. Within each scenario, we also elicited reactions for a negative outcome for the individual (e.g., loan rejection, wrong medical diagnosis). The final part of the interview focused on reflecting on qualities of an ideal, responsible AI, the kinds of tasks that an AI system should or should not do.</p><p>Scenario selection. We draw inspiration from prior research on examining various perceptions of trust <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b72">74]</ref>, fairness <ref type="bibr">[7,</ref><ref type="bibr" target="#b133">136]</ref>, justice <ref type="bibr" target="#b11">[13]</ref> and explanations <ref type="bibr" target="#b31">[33]</ref> in using scenarios 3 to investigate perceptions. We used four scenarios in the study: (1) loan assessment, (2) financial advisor, (3) medical diagnosis, and (4) hospital finder (details in appendix A). The scenarios were a part of a vignette experiment with 2 (decision-support vs. decision-making) x 2 (health vs. finance) within-subjects design. We randomized the </p><p>Internet experience 0-2 years (16), 2+ years <ref type="bibr" target="#b14">(16)</ref> Table <ref type="table">1</ref>: Summary of participant demographics for the interviews, n = 32. We aim for regional diversity in our sample, but do not have enough participants from each city to cross-tabulate the differences across regions.</p><p>four scenarios across these participants using Latin Square Design [15] to control for ordering effects, if any. Healthcare and finance are a visible, explicit policy focus (see NITI Aayog Strategy of 2018 <ref type="bibr" target="#b0">[1]</ref>, and the latest responsible AI strategy of India [2]), and key areas for private sector deployments <ref type="bibr">[44,</ref><ref type="bibr" target="#b98">100]</ref>. Participants were the subject of the decision-making in these scenarios, and thus, we selected relatively common interaction use cases <ref type="bibr" target="#b11">[13]</ref> within which most participants could situate themselves (at present or in the future), even if they had not previously encountered those specific algorithmic systems. We developed the scenarios to represent decision-making with different kinds of agency to affect decision-making (e.g., decision support where the AI makes a recommendation and the user is the deciding agent, and decision-making where the AI makes a decision about or on the behalf of the user). Each scenario was based on real-world examples of AI systems. We used hypothetical scenarios (instead of commonly-used consumer applications) to control for differences in actual prior experiences across participants, so that these effects do not manipulate the results of our study.</p><p>Analysis and coding. After transcribing, we translated all the Hindi and Tamil interviews into English, our primary language of analysis. We followed the qualitative data analysis approach by Thomas [123] to conduct multiple rounds of coding at the sentence and paragraph level. We began by open coding instantiations of perceptions, assumptions and expectations of/around AI. Two members of the research team independently read all units multiple times, and identified categories (unit of analysis) together with a description and examples of each category, until a saturation point was reached. Our upper level categories were guided by the evaluation aims, comprising (1) general perceptions about AI, (2) perceived harms and benefits, (3) sources of AI perceptions, (4) acceptability of AI interventions, (5) willingness to give authority, and (6) human vs. AI vs. human and AI. These codes were discussed and iteratively refined through meeting, diverging and synthesizing. We consolidated our codes into two top-level categories on defining AI authority, and the sources of AI authority.</p><p>Research ethics and anonymization. During recruitment, participants were informed of the purpose of the study, the question categories, and researcher affiliations. Participants signed informed consent documents acknowledging their awareness of the study purpose and researcher affiliation before the interview. At the beginning of each interview, the moderator additionally obtained verbal informed consent. We stored all data in a private Google Drive folder, with access limited to the research team. To protect participant identities, we deleted all personally identifiable information in research files. We redact all identifiable details when quoting participants. Members with research backgrounds in HCI, AI, psychology, and interaction design constitute our research team. Authors located in India moderated the interviews. All researchers were involved in the research framing, data analysis, and synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Survey</head><p>The interviews revealed that AI authority manifested, in part, as acceptance of AI-based decisions. We further confirm this observation by examining the acceptability of AI outcomes through a survey with a larger audience. Our qualitative research informed the operationalization process, and we use acceptability as the construct for measurement. AI authority is a multi-dimensional concept of which a likelihood to accept AI decisions is only one element. The survey was completed online and received a total of 459 respondents. We used a scenario-based approach for the survey, similar to prior research in this space <ref type="bibr">[7,</ref><ref type="bibr" target="#b8">10]</ref>. The scenarios were a mix of high-and low-stakes situations, which we describe below.</p><p>Sample. Members of an online market research panel, Cint were invited to participate in our survey. All participants were based in India, and above 18 years of age (minimum age of consent). We began the survey by eliciting informed consent from respondents. The inclusion criteria for our survey was similar to the interviews. The screener question asked, "how much do you know about Artificial Intelligence?" We included respondents that had at least heard of the words Artificial Intelligence. Those who answered, "Never heard of AI" were not asked any further questions. After the screening  Questionnaire. The survey questionnaire was implemented in Qualtrics. Our questionnaire consisted of 14 questions in total, with a mix of multiple-choice <ref type="bibr" target="#b9">(11)</ref> and open-text questions (3). It was divided in four sections: understanding of AI (2), AI acceptability in general (1), scenario-specific AI acceptability (7) and demographics (4). On average, it took respondents 6.5 minutes to complete the online survey. After completing the screener, the first question focused on their understanding of AI. The open-ended question asked, "how would you explain Artificial Intelligence to a friend?" We then requested respondents to provide up to three examples of how AI is used today. The seven questions about measuring AI acceptability ranged on a Likert scale of 1 being "Not at all accepting" to 5 being "Extremely accepting". The questions were worded with the respondents as the subject of decision-making. Each question on AI acceptability was phrased with a variation of, "How accepting would you be of a decision made by an AI for you?", with the scenarios embedded at the end of the question and highlighted for readability.</p><p>After understanding their general willingness to rely on AI for decision-making, we used six scenarios to understand the level of AI acceptability in varying situations/domains: three high-stakes, and three low-stakes. The six scenarios are (1) medical diagnosis, (2) loan approval, (3) hiring decision, (4) song recommendation, (5) route recommendation, and (6) auto rickshaw pricing. We use a within-subjects design, where all respondents were shown all 6 scenarios, presented independent of each other in a randomized fashion to account for any ordering effects. In addition to the Likert scale items, participants were asked to describe why they were willing to accept decisions made by an AI, if at all. Finally, we also included demographic questions (optional to answer) about respondent's age, gender, education, and internet exposure.</p><p>Analysis. We performed several kinds of analysis to determine the prevalence of AI acceptability across various scenarios. We present two key measures from our analysis in table 2(a), (1) the mean acceptability in our sample, (2) 'significant acceptability' which represents the percentage of respondents that selected midpoint (&gt;=3 i.e., "Moderately accepting") or above on the Likert scale for AI acceptability. The questions on levels of AI acceptability were analyzed by calculating percentages and cross-tabulated to view demographic-specific percentages for certain questions. We performed an independent samples t-test to compare AI acceptability between males and females. We conducted a one-way ANOVA to study the influence of age and internet exposure on acceptability.</p><p>Finally, we performed a qualitative analysis to the open-ended questions using an approach similar to the interviews (see section 3.1). We conducted multiple rounds of coding at the response level in conjunction with participants' survey ratings to surface high-level themes. We include direct quotes from our survey respondents in the Findings with the prefix 'S#', to differentiate from our interview participants prefixed 'P#'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FINDINGS</head><p>We describe the concept of AI authority and the various high-risk actions that participants indicated they would be willing to take in section 4.1. Our study identified four user attitudes to AI authority (section 4.2). We then describe the sources through which an AI acquires authority, which lie outside the boundaries of the system (section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">What is AI authority?</head><p>We draw upon Weber's definition of authority, as a form of power whose use is legitimized by those on whom the power is exercised <ref type="bibr" target="#b130">[132]</ref>, and extend the work of Lustig and Nardi <ref type="bibr" target="#b79">[81]</ref> to define AI authority as the legitimized power of AI to influence human actions, without requiring adequate evidence about the capabilities of a given system. AI was seen as reliable and infallible, and thus considered worthy of authority. Participants were willing to trust and accept AI-based outcomes, and perceived AI as a better alternative to the human institutions with which they currently interacted. Thus, AI authority emerged as a form of 'voluntary compliance' [127] that was seen as justified by users.</p><p>The conceptual metaphors and terminologies can play an important role in our interactions with technology <ref type="bibr" target="#b39">[41]</ref>. We broaden the conceptualization of authority from 'algorithmic' <ref type="bibr" target="#b79">[81]</ref> to 'Artificial Intelligence' for the unique disposition of 'AI' to gain authority due to the hype, optimism and narratives surrounding its use. The techno-optimistic narratives, among other sources, played a key role in generating legitimacy for AI authority among the participants in our study. The acceptance of AI decisions was often due to reasons beyond system design characteristics and performance. Participants perceived AI as a panacea-a tool to prevent discrimination and injustice that they had previously encountered in interactions with traditional institutions, as opposed to the ways in which Bitcoin users in Lustig and Nardi's research <ref type="bibr" target="#b79">[81]</ref> viewed the algorithm as a technology of resistance.</p><p>We find that over 79% survey respondents reported significantly high acceptance (i.e., selected midpoint (&gt;=3 "Moderately accepting") or above) of decisions/recommendations made by an AI system, averaged across the six scenarios. A willingness to accept AI-based outcomes for respondents in both studies was often guided by the  consequences associated with a decision. On average, 84.1% survey respondents were willing to accept decisions in 'low-stakes' AI scenarios, whereas only 73.8% respondents were willing to accept decisions in high-stakes AI scenarios (see table <ref type="table" target="#tab_4">3</ref>). We combined the three high-stakes scenarios (Cronbach's α = 0.83), and three lowstakes scenarios (Cronbach's α = 0.77) into two distinct scales to examine the effects of personal characteristics on acceptability. Gender was relevant for AI acceptability across the three measurements, with females seeing AI systems as significantly more acceptable than males. Finally, we found no significant association between internet exposure and AI acceptability. We present the responses across the acceptability scale for each scenario in Appendix A.</p><p>Interview participants also indicated a willingness to take highrisk actions based on outcomes. Several participants noted an acceptance of the medical diagnosis by AI, with an openness to undergo medical treatment, for instance, "the AI has given me a diagnosis. I will go to the hospital and get the necessary treatment. [...] I have more confidence on an AI instead of a doctor." (P21) Participants were willing to accept loan assessments, and in fact, preferred that AI evaluated their application over a human loan officer. For the decision-support scenarios, most participants reported an inclination to follow AI recommendations (e.g., visit a hospital, budget income, invest in financial schemes), often mediated by their own judgment. As P11 expressed, "I would definitely invest in businesses suggested by an AI, because I am 100% sure that it would be the right recommendation." During each interview, we explored situations in which users might receive opposite recommendations from a human expert vs. an AI. Several participants reported not only an intention to act on AI outcomes, but also an inclination to follow AI-based recommendations over those by a doctor or financial advisor. As P7 described, "I would definitely go with AI if there is a contradiction. There is a clear logic behind an AI." Overall, participants demonstrated a tendency to accept AI decisions as correct, and believed AI was worthy of authority unless they had evidence to the contrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User attitudes to AI Authority</head><p>Participants' trust in the AI applications of the scenarios was underpinned by their beliefs about AI's high competence and benevolence, where different levels of trust could result in diverging intentions or behaviors. AI was considered reliable, that it is high-performing and capable of making correct decisions or recommendations. Participants also perceived AI as infallible and emotionless-that systems are based on facts, logic, rules and conditions or parameters, for example, a loan assessment AI looks at whether an individual's salary is above a certain threshold to determine whether they are eligible for a loan.</p><p>Participants in our study ascribed a positive or neutral intent to AI in their responses (more in 4.2.1), and displayed a motivebased trust in the systems presented in the scenarios, which was also observed by Kramer and Tyler <ref type="bibr" target="#b66">[67]</ref>. The social conception of trust suggests that people are influenced by their attributions of the motives of authorities <ref type="bibr" target="#b66">[67]</ref>-"attributions of positive intent lead members to trust the authority, and accept the decision onto themselves. " Next, we present the four user attitudes of faith, forgiveness, self-blame, and gratitude, that accompanied the authority of AI, and the ways in which these attitudes affect people's interactions with AI applications. The following attitudes towards AI made participants more likely to accept AI-based outcomes, even in situations where they receive a wrong or unfavorable outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Faith.</head><p>A commonly held attitude towards AI was a faith in its capabilities and intent, which persisted even when interviewees received an unfavorable outcome. Participants maintained that AI is between 90% to 99% accurate 4 , with a recurrent estimate of 95% accurate. For example, P28 suggested that they "might face an issue 1 out of 100 times. It is very rare." for a medical diagnosis application. A normative perception across our interviews and survey was that "computers do not make mistakes like humans do" (S265). Many participants had the faith that AI would only provide an outcome if it is confident in its abilities. As P9 suggested, "I will believe an AI because it will not tell me such a big thing that I have a [blood] clot, unless it is confident in itself. "</p><p>Participants ascribed neutral or good intentions to AI applications in the scenarios, in contrast to their perceptions of humans with malicious intentions to gain out of their circumstances. Participants suggested that an AI system would have no incentive to cheat or fraud-which P19 described as, "AI has nothing to gain, not a fixed salary or a commission or incentive. AI is neutral, it is a machine, it is a robot. So the suggestions from an AI will also be good. " Prior work has documented the ways in which 'information' and its use is considered inherently objective, free of personal opinion <ref type="bibr">[6,</ref><ref type="bibr" target="#b97">99]</ref>. Our research confirmed this perspective-AI is seen as fair because it is 'driven by data'. In participants' rationale for accepting AI decisions, the data driven nature of AI was limited to their own data, as opposed to curated training datasets that can be fraught with biases <ref type="bibr" target="#b99">[101]</ref>. P2 reported, "if the system/AI is doing that, it will not take into account that factor of human judgment. In a way, it is good that everyone will be treated impartially or fairly. The one who deserves the loan and meets the full requirements, gets the loan. The system will take into account every aspect of your ability to pay the loan. "</p><p>In addition, the AI systems in the scenarios were seen as more capable of fair decision-making than human institutions. Where human processes were riddled with inconsistencies and manipulation, AI was seen as simply a rule-following, clearly specifiable system that took into consideration every parameter that should be a part of the decision making process, and thus fair. P12 expressed their intention to accept a loan assessment AI over a human officer because, "officers make you go through so many procedures for a single approval. It will be easy if you have connections at the office. Otherwise, they will ask you to visit one counter after another, and make you wait in long queues. It is simply exhausting. " Colquitt presented four types of organizational justice <ref type="bibr" target="#b22">[24]</ref>: distributive, procedural, interactional, informational. Participants consider the procedures, workflows, practices of institutions as a frequent source of unfairness (procedural and interactional injustice). These findings point to a need to reorient our research from an exclusive focus on outcomes towards a approach which takes into consideration the entire procedure of receiving an outcome and the various interactions it involves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Forgiveness.</head><p>Several participants demonstrated an inclination to forgive or justify 'wrong' decisions or incorrect recommendations, and give the system multiple opportunities before reducing or terminating use. P19 expressed that it was not the hospital finder AI's fault if they did not have a great experience at the hospital: "if the AI gave a recommendation, I visited the hospital and did not like the doctor's treatment, then that is not the AI's fault. That is my personal judgment. AI cannot tell me that the doctor will behave well or give the right treatment. " The 'forgiveness' towards AI was greater in the decision-support scenarios because participants had a 'choice' among the recommendations. For example, if the user selected between the five schemes recommended by the AI, and the recommendation did not work well for them, then they did not hold the AI accountable, because they believed they made a choice among those options.</p><p>Several participants did not view AI itself as a frequent source of errors or biases. The forgiveness towards AI partly arose out of participants' under-recognition of the range of system errors and biases embedded in AI systems. Instead, participants humanized the AI development process by recognizing the high-level system design decisions (e.g., "what conditions are used" (P24)), the collection and input of data-for which institutions or other individuals are responsible. If an AI made the wrong decision, participants hypothesized that it could be the fault of the system developer (e.g., "low-level mistakes by the programmer" (P25)), or the institution engaging the system (e.g., "hospital did not update the information"). A few participants acknowledged the ways in which the data could be manipulated (e.g., fake reviews or institutions intentionally sharing the wrong information). "I would be disappointed, but I would not blame it completely. It is a machine after all. It is trained to do things in a certain way. [whom would you blame then?] Maybe the programmer.</p><p>[laughs] They have not trained the computers to do the hospital search in a better way. " 4.2.3 Self-blame. Participants perceived that AI development requires specifying conditions (such as a rule-based system, if-else), and thus, would seldom generate wrong outcomes. The notion of clearly specifiable conditions manifested into self-blame, as participants did not see a lot of scope for questioning AI outcomes for errors. Participants consistently blamed themselves for receiving an adverse outcome, especially in the loan assessment scenario. As P32 described, "If we give proper documents, it [AI] will give loan, else there must be some problem with our documents. " Users conjectured that an unfavorable outcome was their error, because an AI rarely ever makes mistakes. For example, participants had a tendency to believe a wrong outcome by AI meant they did not correctly input their medical history into the health application, or entered the wrong location for finding a hospital, or did not upload the necessary documents for financial advising.</p><p>Participants viewed AI systems as emotionless, logical entities. As P18 mentioned, "the decision is right if we keep our emotions aside and think logically. If they [institutions/developers] have certain rules, then they will decide based on those rules. If we did not fulfill those requirements, then we did not receive the loan." Moreover, in some cases, participants believed that they deserved to receive an unfavorable outcome. For instance, users speculated that a loan rejection was "based on [their] transactions" (P4), and meant they had ineligible finances or collateral to receive a loan. Overall, participants had a low recognition of the potential for AI errors and biases, and instead held themselves or institutions accountable for unfavorable outcomes. Self-blame was a recurring attitude which we observed among our participants, even thought it was not grounded the scenario. There is an urgent need to combat self-blame (potential approaches in section 5), or else users might not recognize when an outcome is biased, unfair. Users might not seek recourse or alternative opportunities if they believe that they received an unfavorable outcome, but one that is deserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Gratitude and benevolence.</head><p>Participants felt a sense of loyalty and gratitude towards AI-that they were indebted to AI for the convenience it afforded and reducing dependence on others. 41 survey respondents mentioned 'ease' and convenience within their rationale for accepting AI-based decisions. AI, and technology in general, helped several participants feel independent, for they did not need to rely on others for 'trivial' tasks such as navigating to a location or finding some information. This indebtedness and gratitude was more noticeable with newer internet users, who shared instances of feeling liberated through their use of AI. Participants' prior interactions with general-purpose AI systems (e.g., maps, search, voice to text) generated this attitude of indebtedness. In this way, the authority of transferred from a general conception of AI as a technology to more specific AI use cases that we discussed in our interviews.</p><p>Participants often equated Artificial intelligence with informationfinding tools or voice technologies, for instance, P9 expressed, this smartphone recently. The age at which I am, if I to ask something, I used to go to 10-12th class students. Now AI has it easier for me, as I do not feel any awkwardness in asking anything-it is artificial afterall. " Applications built using AI had them promise, and transformed their lives in many ways. As P11 described, fully believe AI. Till now I have used it for many tasks and it has never given me the wrong result. Whatever it says will be the right thing. " were willing to act upon decisions because their prior interactions with AI were reliable, used those instances as justification for conferring authority to AI, even if these interactions were with completely different systems. P11 recounted their experience with a Maps application while discussing the medical diagnosis scenario and how that solidified trust in AI applications: "I like all things created with AI. While doing field work, on some nights I had to stay outside till 12 am. It helped me reach home safely. I can trust it with my eyes closed. " Beyond these sentiments of indebtedness, several participants reported a tendency for benevolence in sharing their data. While wanted more control over the use of their data, including knowing what data is collected/used, most of them felt comfortable sharing the data in exchange for reliable decisions or recommendations. In participants' perspectives, good outcomes necessitated sharing various kinds of information. P22 discussed their current use of a health application for managing PCOD, and that they were comfortable sharing their health information be-"the information that the app gives is valuable to me." AI systems as a 'safe space' free of judgment, and several interviewees were more comfortable sharing their personal data with an AI, 'a machine' over humans. As P24 discussed, "Most people not comfortable to discuss their financial needs for loan. There are certain personal questions that a bank might ask me. I am answering questions at my home for an AI. A machine is asking me a question, I am not being encroached or judged for my choices even though my information is going out. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sources of AI authority</head><p>this section, we present four sources through which AI acquired authority. A propensity to confer authority to AI was influenced by: (1) interactions with ineffectual institutions, (2) techno-optimistic narratives and misleading terminologies around AI, (3) users' prior interactions with applications, and (4) unavailability of alternative systems. AI authority was often derived through one or all of the above sources, often a combination (e.g., a good experience with a different healthcare mobile application, and the unavailability of a medical doctor in their village). We emphasize that AI authority could be misplaced: if AI derives legitimacy and influence over people's actions through sources which lie outside the AI system and are unindicative of the efficacy of the given system, then an intention to act upon AI decisions introduces a potential for individual harm. These sources of AI authority are extraneous to the they provide no evidence that the system will be effective, moreover, unbiased to the user. In reality, an AI system could be dysfunctional (poor performance and/or unfair), and still gain among users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Interactions with ineffectual institutions. Many participants</head><p>were exasperated by the discriminatory practices and unjust interactions with human systems. While users appreciated the perceived of AI, they often projected AI as a better alternative to human systems. This contrasts with prior work by Lee and Rich <ref type="bibr" target="#b75">[77]</ref> which reported that users in the US with cultural mistrust in doctors consider healthcare AI equally untrustworthy. We find that that authority represented a balancing act: AI was authority-worthy, better alternative because human institutions were perceived as ineffectual. For example, participants found it unfair when business directed patients to specific hospitals to receive a small fee, or financial planners recommended certain products to meet their quarterly targets. Several interviewees narrated their experiences with bureaucratic decision-making institutions (not just banks), with corruptible officers demanding a bribe as the only way to approve their application. AI was consistently seen as a mechanism to avoid encountering prejudiced practices in their interactions with institutions. As P26 expressed, "if a lower middle class individual visits a bank, they ask for so much documentation which you cannot fulfill. The biggest reason for this is that bribes work in many banks for sanctioning loans in India. I have faced this myself. If you go to a bank, the mediators will get a hold of you and ask for 15-20% of the loan amount. Then they will clear all your documents. If there is an AI in this place, then there would not be any issue. " Participants' mental models of how institutions functioned also influenced their willingness to give authority to an AI. Human were perceived as flexible and open to negotiation-"they can bend a few rules and make it work" Human institutions often used appearance and identities as proxies for decision-making, was perceived as unfair, and an aspect that could be detached from AI systems. P6 described how, "AI works on logical thinking, not sentimental thinking. Unlike an officer, AI does not have a predetermined image of anyone-it would not judge them by their appearance standard of living. " Participants conferred authority to AI as they believed that an AI would not consider their appearance/identity but only their documents to arrive at a decision.</p><p>Building and maintaining interpersonal relationships within institutions helped individuals navigate decision-making situations with ease. AI was a better alternative than humans with a perceived 'partiality' towards known acquaintances, friends or family. Overall, participants considered human institutions as prone to biases, and a human-AI collaborative decision would be equivalent to a human-only decision susceptible to manipulation. Several interviewees described the concept of human-in-the-loop as 'interference' humans supervising AI decisions). Users preferred that only made the decision to avoid any form of exploitation, and a human be involved before or after the decision is finalized (e.g., guiding/supporting through the procedure or an unfavorable outcome, receiving feedback on the recommendation or decision).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Techno-optimistic narratives and misleading terminologies.</head><p>The narratives about AI that surfaced in our research often originated media (e.g., Science Fiction movies like the Terminator), news articles, or government and initiatives. Most descriptions of AI were and extreme: mostly optimistic, and rarely very pessimistic. The pessimistic accounts (e.g., killer robots) were often too far-fetched for participants to imagine or consider a possible reality. The realistic, shades-of-gray narratives were often missing in participants' descriptions of AI, its benefits and harms. People carried an optimism about AI owing to breadth of coverage about AI's potential, applicability, and planned use by the government. As described, "the government has made everything an online system nowadays. That cannot go wrong at all. " Several participants brought up the promise of various technological deployments (e.g., "Modi is launching a driver-less train" (P28)) as a way to demonstrate their support and interest in adopting, accepting and acting upon AI decisions.</p><p>We observed that many interview participants and survey respondents saw AI as futuristic (similar to <ref type="bibr">Kelley et [60]</ref>). AI was marketed as a for-good initiative perceived as "very progressive" (S11). Participants noted the ways in which they believed society could benefit from AI, especially in social good domains agriculture, healthcare). Ultimately, AI was considered a course towards modernity, for instance, S58 described AI as the "the modern technology that will change the world." AI was portrayed a tool of convenience: an AI application would make the user's life easier completing tasks and decision-making faster. The words cial Intelligence invoked among our participants an of a machine that imitated human intelligence with superior performance and without any biases. Overall, the portrayals and AI narratives lend authority to an AI system.</p><p>Research participants frequently associated AI with 'machines' (e.g., ATM, washing machine, ECG machine), which engendered a misleading credibility in AI's capabilities. Several interview participants compared an AI system with an ECG machine in the medical diagnosis scenario. Participants suggested that if doctors use ECG machines to conduct medical tests then it is acceptable to use 'ma-(AI) to make a diagnosis as well. This belief often stemmed from the notion that AI is a computer/machine derivative, and thus, similarly reliable. AI was as an unbiased tool that would get the job done without the complications and emotions that come with human interactions. Participants that believed AI lacked considered that AI would take good, correct, and accudecisions, even if it was unfavorable to them. As S394 noted, an AI system is more logical and calculative compared to a human trying to make a decision. It is heartless yet accurate in making the decision with no emotions meddling the decision making. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Prior experiences with technologies. Participants supposed AI</head><p>highly reliable often through their prior experiences with AI/nontechnological systems. Respondents frequently referenced other systems (e.g., maps tech, voice assistants) as they discussed the of the given AI system. As a result, interviewees often over the performance and safety aspects of relatively reliable non-AI systems on to AI systems, for example, as P2 suggested, "nowadays, we are using net-banking, and the transactions are pretty So I think this [financial advisor] AI will also work in the same way." The capabilities of the given AI system were conflated by analogizing technologies in the same domain that are built with different underlying mechanisms (e.g., money transfer vs. financial They believed that the given AI system deserved authorif other perceived technologies (which they considered often AI) had given them good, reliable decisions or recommendations. As P28 described, do not think an AI will make a Till now whatever we have encountered, it has never done wrong thing." Non-AI technologies (e.g., GPS, Internet, ATM) were often misconstrued as AI systems. The boundaries for what as AI are often blurry. We find that of importance is tools are perceived as 'AI-based' and the confidence, if any, such associations instills among users.</p><p>observed that the acceptance of AI transferred through social interactions with friends and family, and through systems released by the institution that created this AI application. Several participants mentioned the ways in which their initial acceptance of applications often relied on experiences of their friends or family.</p><p>people they knew had positive encounter with an AI application, then it would help the participant calibrate authority as well. As P3 suggested, "it helps me if I hear from colleagues, friends that by using this application they are able to manage finances well. " factor that built initial acceptance was if the system was created by an organization that participants' had trusted over the years. Both institutional or social transfer of authority do not reliably determine indicate how a system will respond to an individual due to the of AI systems <ref type="bibr" target="#b2">[5]</ref>, or if the current system is built with the same standards/care as earlier systems by the same organization. 4.3.4 Unavailability of alternative systems. AI acquired authority through the unavailability of human institutions to meet users' basic needs. Participants relied on an AI system if their other alter-(i.e., human systems) are infeasible and/or unavailable due to economic constraints, their location, or the timing. For example, receiving medical treatment or finding financial advice was often cost-prohibitive for several participants. On the other hand, an AIbased medical diagnosis application presented an opportunity to on the consultation fees of a doctor. As P26 noted, "doctors a minimum fee of 300-400 INR, which we can save if we use Individuals were, thus, inclined to accept AI-based decisions or recommendations, especially if it was not an emergency or highstakes situation. A few participants shared a hesitation for AI in emergency, high-stakes situations (e.g., those requiring first aid).</p><p>Traditional public services (e.g., medical, financial) have been historically scarce and challenging to access in remote areas in India As P22 noted, "it also depends on whether you are in a rural urban area, and if this is a small or big money lender. " P21 menthat there are no MBBS doctors in their village, and they only have alternative medicine practitioners (e.g., Ayurvedic). Usually, respondents' (and their family members') chronic life-threatening remained undiagnosed from a lack of multi-specialty in their rural area of residence. P25 described how their Lupus was finally detected after they relocated to a major city in Odisha. For P25, "AI has tons of information to aggregate make the result available to people, so they can diagnose any diseases. " AI systems were conferred authority as a result of inadequate availability and a glaring divide in access to services across geographies. AI systems were believed to be available round-the-clock as per participant's convenience, whereas human systems were available only during hours. Even more importantly, AI was associated with faster decision-making. In addition, with the lower turn-around time, participants could explore other options if they not receive a favorable outcome. For instance, if an AI decides to reject the participant's loan within a few minutes, they could visit another bank. As P4 described, "there is no delay in using the machine. If it is rejected, it gives a direct If we go to loan officers, they say come tomorrow, and this keeps on. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Our results indicate the high acceptance AI where 79% respondents reported intentions to accept AI-based outcomes. AI decisions were considered reliable, with a performance close to 95%, when most real-world deployments currently struggle reach such high performance, especially in high-stakes <ref type="bibr" target="#b106">[109]</ref>. AI systems were also seen as infallible and fairer than human decision-making. These hold true in high-stakes scenarios (e.g., healthcare, finance) as well. Owing to their abundant faith in AI capabilities, participants engaged in or blaming other individuals for an unfavorable outcome. AI acquired an authority to influence actions through sources that lay outside the AI system. Acute and over-calibrated authority has negative implications. Participants in our research demonstrated faith and gratitude towards AI for 'improving their lives'. They indicated a willingness to adopt and new systems, as they believed AI would provide the right outcome. These attitudes could potentially make users vulnerable to algorithmic abuse.</p><p>is extensively documented that if AI systems are not built with care, they perpetuate various forms of bias and inequalities (e.g., through datafication, feedback loops <ref type="bibr">[12, 23,</ref> The impacts of algorithmic biases can be exacerbated in contexts where people are willing to accept systems and algorithmic decisions deployed without adequate accountability. The fact that many users confer authority to AI could be easily exploited to introduce a system into a large but optimistic user base, to utilize harmful approaches data maximization, collecting non-consensual data) without oversight. Even without engaging in overtly malicious practices, creators of these systems might ignore the nuances of the context which they are deploying, use approaches that worked in the West but are inappropriate in India <ref type="bibr" target="#b6">[8,</ref><ref type="bibr">108]</ref>. In the following subsections, we present implications of AI authority for researchers designers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Calibrate AI authority towards an appropriate level</head><p>Participants in our study indicated a propensity to actively rely on AI for decision-making or recommendations. This might seem desirable or align with business goals, indeed, many organizations utilize consumer trust as a metric for success <ref type="bibr" target="#b123">[125,</ref><ref type="bibr" target="#b127">129]</ref>. Greater trust might seem to indicate a well-performing product for users.</p><p>our findings indicate that users' experiences with alternative, human systems could easily confound measurements of trust. Is it possible that a high trust is simply an indication that users place authority in AI instead of existing human institutions? For instance, an application might be dysfunctional, unsafe, or unfair to users, but they would still rely on it because AI is perceived a better alternative. Overall, high perceived trust might not be an indicator about the system performance. Efforts to reduce acceptance of AI outcomes might seem antithetical to business goals: if authority is well-calibrated over time, then it might mitigate harm, retain users, and increase overall satisfaction with the application, leading to success. Overall, unwarranted authority initially seem desirable, but it can negatively harm product experience in the long-term, as people continually receive outcomes that do not align with their expectations.</p><p>We emphasize the need to calibrate authority towards an appropriate level aligned with the actual trustworthiness of AI <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b122">124]</ref>, instead of a trust built through the proxy, confounding sources that we document in our findings. The gratitude shared by our participants was not isolated to the applications that they were using (e.g., media, navigation, voice assistants), but often extended to all of AI systems, including those in our scenarios. Designers can consider introducing features to communicate the actual capabilities of the system while optimizing for understanding One can set the right expectations from early-use about the capabilities. In particular, designers can consider making limitations of the system explicit before and during early use, describing the ways in which a system operates and arrives at a decision, offering examples of situations in which the system is to provide unreliable results (see the PAIR Guidebook <ref type="bibr" target="#b92">[94]</ref>, and the Microsoft Human-AI Toolkit <ref type="bibr">[4]</ref>). More research is needed to discover the nuances of the ways in which users might adjust these components of trustworthiness with continued interactions with a system. work can consider designing alternative metfor measuring success, beyond user trust in a product or feature, that explicitly takes into account the contextual factors and their prior experiences which contribute to acceptance and AI authority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embrace users' qualitatively different meanings</head><p>Individuals' beliefs and understanding about AI was a major factor that contributed to AI authority. Participants conceptualized AI systems as rule-based, as embedded with clear, specifiable conditions. Users' understanding of the ways in which AI works heavily deviates from the non-deterministic, deep learning models which are used in current product experiences. Indeed, the communities of HCI and CSCW have long been interested in understanding the ways in which people perceive algorithmic systems (particularly social media applications) <ref type="bibr" target="#b29">[31,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b61">62]</ref>. find that AI is not understood in a standardized manner across cultures, contexts, internet and age groups. In fact, respondents do not come with a 'common denominator', a shared understanding about AI systems when researchers are measuring various aspects of AI perceptions. AI carries a qualitatively different meaning, especially owing to the contextual narratives surrounding its use.</p><p>We argue for embracing the variability in participants' perceptions AI heavily influence their intentions and behaviors around systems. Perhaps, instead of trying to create a shared meaning of AI or algorithmic systems, could we elicit user conceptions as a basis to guide our analysis of trust, fairness usefulness? How might we foreground these conceptions or use them to contextualize our research findings? Comparisons human vs. decision-making must be foregrounded in participants' beliefs about AI's functioning, which in turn influences their intentions and behaviors. Creating a standardized understanding across users (e.g., by defining AI for the participant) not reflect real-world situations, or effectively erode/replace own beliefs. In our study, several participants suspected the presence of AI in certain applications, but their reason for believthat those applications are created using AI were frequently distorted. For instance, many people alluded that an application requires Internet or GPS is AI-infused. Overall, people might be able to accurately indicate which systems contain AI by conflating non-AI technologies with AI. AI literacy tests may be unable to reveal users' actual competencies or understandings about AI. Combining quantitative tests for AI literacy or awareness with a approach could probe these beliefs and expectations in a holistic way.</p><p>Lateral comparisons of the various constructs of AI perceptions acceptability, trust, fairness) across countries must be conducted with extreme caution. Prior research documents the various kinds of survey biases (e.g., acquiescence bias [133], social desirability <ref type="bibr" target="#b118">[121]</ref>). People in different cultures have different response styles. Subjective Likert scales are often compromised due to reartifacts <ref type="bibr">[91]</ref> or the reference group effect; "the more cultures in their norms (i.e., the more cultures are really different on the dimension), the more the cultural comparisons are confounded." <ref type="bibr" target="#b45">[47]</ref>. A straightforward comparison of beliefs and attitudes across cultures with widely differing norms might lead to measurement and thus requires a careful wording of questionnaire design that proactively accounts for variations in response styles and regional differences. For instance, in the EU, perceptions might be very different considering the attention to GDPR <ref type="bibr">[126]</ref>, and the resulting awareness about algorithmic systems and its biases <ref type="bibr" target="#b58">[60]</ref>.</p><p>applies to interpretations as well: there is a need to cautiously pre-reported data from different countries, especially if there is a chance that the contextual factors might be lending auto an AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Build competencies on AI use</head><p>Though the research on trust and fairness perceptions offers mixed results (often domain-and task-dependent), several prior studies find that respondents in the EU, US indicate a tendency to trust humans more than algorithmic systems <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b68">70]</ref>, especially for tasks requiring human skills <ref type="bibr" target="#b72">[74]</ref> (with some exceptions such as Logg et al. and Lee and Rich <ref type="bibr" target="#b75">[77]</ref>). In our technological context, we find a case of acceptance of AI outcomes with a confidence greater a system might deserve. Misplaced authority is consequential: AI systems have potential to cause harm through incorrect or biased outcomes, especially because users might not actively seek out information about the capabilities of the given system. The effects of misplaced authority are exacerbated and far-reaching in scenarios (e.g., finance, social benefits). Several participants considered a lack of human involvement in making the decision as desirable. Even the use of general-purpose products in critical situations (e.g., virtual assistants for job interview reminders) could lead to adverse outcomes for users.</p><p>Mayer, Davis, and Schoorman <ref type="bibr" target="#b81">[83]</ref> present an integrative model trustworthiness with three components: ability, integrity, and We use this framework to reflect on our findings and a path forward. Prior work (based in US or EU) reports that users rate technology companies (and products) with low trust (benevolence) <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b83">85]</ref>, but high ability. Therefore, when a techsystem goes wrong, it is seen as a benevolence issue. Our results suggest that both-ability and benevolence-components of trustworthiness are viewed as high in India. Users are willing give authority to AI because they have in the competence and benevolence of AI systems. When a system makes a mistake, neither its capabilities (ability) nor its intent (benevolence) were readily questioned. This resulted in self-blame or placing blame other actors, without a recognition that AI could have made that error. Especially for the loan assessment scenario, many participants our study believed that either they deserved an unfavorable outcome or could be their own fault. The self-blame was exacerbated participants indicated a faith in the capabilities of AI. Low or non-contestation of AI outcomes has potential to cause individual harm if people accept decisions which might be wrong. In addition, users reporting decisions or behaviors to system represents important opportunities for model feedback. This scope for mitigating and improving models is lost if users believe the outcomes be correct, with a possibility of causing harm to other users. is a well-established body of work on explainable AI <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b47">49]</ref>, and how explanations impact user trust and reliance <ref type="bibr" target="#b96">[98,</ref><ref type="bibr" target="#b115">118]</ref>. The of this research is to explore when to explain (high low stakes, e.g., <ref type="bibr" target="#b16">[18]</ref>), what to explain (global vs. local <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b67">69]</ref>), and how to explain <ref type="bibr" target="#b135">[138]</ref>. To make the system more transparent, there's increasing interest within the HCI, XAI communities to find humancentered approaches for explainability <ref type="bibr">[78]</ref>. There are broadly three levels of building competencies on AI use: (1) outcome/in-themoment explanation of a certain outcome, (2) an understanding of how a given AI application works, (3) general, accessible education about what AI is, how it works, and what are its strengths and limitations. Most explainability approaches have been context-agnostic, there needs to be particular attention to internet users that often have less familiarity with technical jargon, often coupled with lower literacy <ref type="bibr" target="#b90">[92]</ref>. Additionally, in-the-moment explanations are not always feasible due to legal or usability constraints. Consider a credit card approval application. Designers be unable to 'explain' the reason behind a particular decision due to anticipated legal issues. Even without legal constraints, explanations are most useful when actionable <ref type="bibr" target="#b54">[56]</ref>. A credit card rejection for an applicant with AI authority might mean that they that the system made the correct decision. As a result, they might not seek alternative platforms when it could easily be the that the AI made an error or gave a biased outcome. The onus is on the designers/developers to add safeguards, ensure that users do not share a Utopian view of the systems with which they in- <ref type="bibr" target="#b108">[111]</ref>, and acknowledge the range of system errors that are possible. Another approach to consider is to leverage existing capacities <ref type="bibr" target="#b131">[134]</ref> by educating the user about how the system works in general (e.g., <ref type="bibr">[43]</ref>) that could be a valuable starting point for users to calibrate their responses based off general understanding of how an application provided an outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Imagine and disseminate narratives of contemporary risks</head><p>The narratives shared by our respondents were too fantastical polarized. They described science fiction narratives movies Terminator), and several respondents exclusively associated AI with robot applications. Participants dismissed the more pessimistic (e.g., of killer robots, see also <ref type="bibr" target="#b18">[20]</ref>) which were hard to or relate with. On the other hand, the optimistic narratives were entertained and adopted. AI was perceived as an innocuous, neutral tool to 'make [their] life easier'. Our work reinforces the sentiment shared by Cave et al. <ref type="bibr" target="#b19">[21]</ref> that "narratives might influence the development and adoption of AI technologies. " Then how might we develop alternative narratives about the ways in which AI technoloare currently used? An important emerging research area is to create and disseminate AI narratives that align with contemporary issues of algorithmic bias. Realistic accounts could gradually help people calibrate their authority by painting a less skewed, lopsided of the capabilities of AI. People generally have a tendency to engage with mainstream media (articles, movies), and our results indicate that the polarnarratives to which people were exposed greatly shaped their acceptance and the authority of AI technologies. Overall, an optimistic portrayal of AI, especially through state-sponsored initiatives, could result in less-than-rigorous technologies making their way into usage by the general public, causing irreversible individual and societal harm. Several participants actively acknowledged the human involvement in development, however, we note a faith the of AI applications-that these systems are built to users and improve current workflows, with neutral/good intentions, partly owing to the narratives about the benefits of emerging technologies.Several state-sponsored bodies pushing for AI curriculum in schools and dedicated programs in universities <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b104">106]</ref>, strategy are reporting credible information [2, 114], however, it is equally important to disseminate these to a audience. For instance, "The Social Dilemma" [68] is a film that gained notoriety in the US, which documents the dangers of AI, specifically, social networking. Mainstream media modalities that educate people about the complexities and contextual concerns about AI, but more importantly initiate dialogue about these topics, might play a crucial role in calibrating AI authority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Responsible AI approaches do not across contexts</head><p>Traditional auditing methods (e.g., involving researchers, activists) 110] can be inadequate surfacing harmful behaviors when the auditors lack the cultural backgrounds or lived experiences to recognize if something is problematic <ref type="bibr" target="#b110">[113]</ref>. Shen, Devos et <ref type="bibr" target="#b112">[115]</ref> propose the concept of everyday auditing through which detect and interrogate problematic biases/behaviors through their everyday interactions with algorithmic systems. However, auditing can be premised on informed users with the capacities, and even more, the recognize and report bias. What if users are not likely to seek out instances of biased behaviors? if, beyond that, users are defensive about AI systems, or unquestioning of its abilities to provide good results? Eslami al. <ref type="bibr" target="#b36">[38]</ref> that users were able to notice bias in the rating platforms. User auditing approaches predicated on skeptical users might not generalize well to contexts that demonstrate technooptimism and faith towards AI and its capabilities. People with acceptability of AI might not see the shortcomings of using these systems. <ref type="bibr">Sambasivan et al. [107]</ref> extensively report the ways in which a straightforward porting of responsible AI tenets can be inadequate and often harmful. might we involve users with optimistic views about AI into algorithmic audits? Certainly, their perspectives would be valuable contributions in surfacing biased outcomes. Empowering users to interrogate these systems might be an approach calibrate AI authority towards an appropriate Future work might how we could leverage existing capacities in users that have high confidence in AI to acknowledge bias. This could benefit platforms in two ways: visibilising bias and mitigating harm, but also through building alternative, realistic narratives about AI that are better aligned with the capabilities of system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND FUTURE WORK</head><p>our interviews included a diverse sample, it may be subject to common limitations of qualitative studies, including recall and observer bias, and participant self-censorship. Our findings represent the perceptions of our sample, and may not generalize countries, to groups with different levels of exposures to AI systems or the discourses around AI. Within India itself, there is a wide range of cultural and social associations with AI that could yield varying results about the attitudes towards these systems. In our screening criteria included people that had previously of AI, however, this might exclude perspectives from people who had developed conceptions about algorithmic behaviors through their interactions with technological systems, but had not heard of those exact words. Researchers could consider expanding the inclusion to elicit perspectives from users aware of AI decisions but not with the terminologies. We acknowledge that participants may have had different responses if these had been more commonplace, for example, through the mobile products with which already interact. Finally, we used hypothetical scenario-based approach (similar to prior research [7, <ref type="bibr" target="#b73">75,</ref><ref type="bibr" target="#b132">135]</ref>), so the might not generalize to newer domains or use of AI. Future work could consider investigating acceptance of users of existing high-stakes AI applications (e.g., instant loan applications) or extend the research to other domains of study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Our everyday life experiences are increasingly mediated through AI systems. It is, thus, of crucial importance to investigate the ways in which technology users across diverse contexts would to or adopt AI-based decisions. We presented a mixedmethods study of perceptions about AI systems (both decisionmaking and decision-support) in various domains and settings by drawing upon 32 interviews and 459 survey respondents in India. We that acceptance of AI is high, and find that attitudes towards these systems indicate a form of authority of AI. AI was afforded a legitimized power by our participants, and it manifested as four user attitudes that could lead the participants vulnerable to abuse. AI is conferred authority through extraneous factors which do not provide evidence for the capabilities the system. notably, users give AI an authority because they perceive it as a better alternative to the human institutions with which they currently interact. We urgently call calibrating AI authority by reconsidering methodological norms success metrics which might be confounding, and through appropriate, alternative narratives for deploying AI systems in India.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A INTERVIEW SCENARIOS</head><p>Loan assessment (Scenario 1). Suppose you (or your family members) are for a loan a of Rs. 10 lakhs. The bank is using AI system called MyLoans for evaluating loan applications. will be required to fill out the loan application form, specifying the loan amount, applicant financial history, and submit them along with relevant documents like address ID proof to MyLoans. Once assessed, the status of your loan application will be updated on the MyLoans application. The difference from a regular loan assessment process is that MyLoans (an AI system) will determine whether your loan application is approved/successful or not, instead of a loan officer.</p><p>Financial Advisor (Scenario 2). Consider the where you are planning to start saving for goal such education, marriage, for yourself or a family member. There is an application, MoneyAdvisor, which using AI, helps its users to understand financial behavior. It takes into account the user's age, marital status, employment status, annual income, their assets and expenses. application creates customized advice for their users, recommending ways to budget income and expenditure, and offers suggestions investments. The difference from a regular financial advising process is that MoneyAdvisor (an AI system) will determine generate on how to budget and where to invest, instead of a human financial advisor. Medical Diagnosis (Scenario 3). the situation where you're experiencing symptoms like fatigue, shortness of breath, and pain. The hospital uses a system-GetWell-which using Artificial Intelligence is capable of reading your ECG reports, detect cardiac provides a diagnosis, and responds with the next steps which best match your diagnosis. You would have to enter your age, and upload the ECG report on GetWell at the remote centre. The difference from a regular medical diagnosis process is that GetWell AI system) will determine your medical diagnosis, instead of a doctor. Hospital Finder (Scenario 4). the situation where you are experiencing symptoms of nausea, fever, and stomach ache. But are not sure which hospital to visit. There is an application called FindADoctor which using Artificial Intelligence, helps its users in finding nearby hospitals, nursing homes and clinics. The user needs answer few about their and symptoms, and the AI will options for hospitals to visit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RESULTS</head><p>Not </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>developed the concept of the algorithmic imaginary as the "ways of thinking about what algorithms are, what they should be and how they function [...] plays a generative role in molding the Facebook algorithm itself. " Hargittai et al. [45] study people's algorithmic skills, and report on the methodological challenges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Male (249), Non-binary/ third-gender (0), Prefer not to answer (3) Age 18-24 (110), 25-34 (238), 35-44 (84), 45-54 (22), 55 or older (4), Prefer not to say (1)</figDesc><table><row><cell>Type</cell><cell></cell></row><row><cell cols="2">Gender Female (207), Highest Education Higher secondary (96), General Graduate (176), Professional Graduate (187)</cell></row><row><cell>Internet experience</cell><cell>0-5 years (107), 5+ years (352)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of participant demographics for the survey with 459 respondents and attention check questions, we were left with 459 respondents that completed the entire survey. Each respondent received $1.70 (INR 150) as compensation for their participation. We describe their demographic details in table 2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The level of acceptability of AI-based decisions for the six scenarios from the survey, with a total of 459 respondents. Significant acceptability for a scenario represents the percentage of respondents that selected mid-point (&gt;=3 i.e., 'Moderately accepting') or above on the Likert scale.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Detailed results from the survey on the acceptance of AI-based outcomes in general, and across the six scenarios listed above.</figDesc><table><row><cell></cell><cell></cell><cell>at all</cell><cell>Somewhat</cell><cell>Moderately</cell><cell>Very</cell><cell>Extremely</cell></row><row><cell>General</cell><cell></cell><cell>3.5%</cell><cell>17.2%</cell><cell>22.0%</cell><cell>30.1%</cell><cell>27.2%</cell></row><row><cell>Medical Diagnosis</cell><cell></cell><cell>7.8%</cell><cell>19.4%</cell><cell>22.7%</cell><cell>28.3%</cell><cell>21.8%</cell></row><row><cell>Loan Approval</cell><cell></cell><cell>7.2%</cell><cell>16.6%</cell><cell>22.4%</cell><cell>30.5%</cell><cell>23.3%</cell></row><row><cell>Hiring Decision</cell><cell></cell><cell>9.6%</cell><cell>18.1%</cell><cell>22.4%</cell><cell>26.6%</cell><cell>23.3%</cell></row><row><cell cols="2">Song Recommendation</cell><cell>2.2%</cell><cell>12.9%</cell><cell>18.3%</cell><cell>34.9%</cell><cell>31.8%</cell></row><row><cell cols="2">Recommendation</cell><cell>1.3%</cell><cell></cell><cell>18.3%</cell><cell></cell><cell>32.2%</cell></row><row><cell>Auto Pricing</cell><cell></cell><cell></cell><cell>15.9%</cell><cell>20.9%</cell><cell>31.4%</cell><cell>27.9%</cell></row><row><cell cols="2">4: General</cell><cell></cell><cell>High-stakes</cell><cell></cell><cell>Low-stakes</cell></row><row><cell></cell><cell></cell><cell cols="3">Independent samples t-test</cell><cell></cell></row><row><cell>Gender</cell><cell cols="2">0.37 (0.10), p &lt; 0.001</cell><cell cols="2">0.27 (0.10), p &lt; 0.01</cell><cell cols="2">0.22 (0.08), p &lt; 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ANOVA</cell><cell></cell><cell></cell></row><row><cell>Age</cell><cell cols="2">453) = 7.19, p &lt; 0.001</cell><cell cols="2">F(5, 453) = 11.55, p &lt; 0.001</cell><cell cols="2">F(5, 453) = 6.71, p &lt; 0.001</cell></row><row><cell cols="3">Internet exposure 454) = 0.77, p &gt; 0.05</cell><cell cols="2">F(4, 454) = 1.21 , p &gt; 0.05</cell><cell cols="2">F(4, 454) = 1.43 , p &gt; 0.05</cell></row><row><cell cols="7">5: Results from our survey: Influence of personal characteristics on acceptability</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The results on perceptions of trust and fairness have been domain-dependent and mixed, but predominantly negative in contexts such as the US and EU, which we discuss in the Related Work section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We define high-stakes as a situation with possibly far-reaching consequences for the future of an individual, but acknowledge that stakes involved in a decision are subjective and personal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Drawing from Lee et al.<ref type="bibr" target="#b72">[74]</ref>, "studies have suggested consistency between people's behaviors in scenario-based experiments and their behaviors in real life<ref type="bibr" target="#b134">[137]</ref>. "</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Participants did not specifically use technical jargon like accuracy, instead, they expressed the percentage of times an AI system would give the 'right/correct' outcome, which we loosely translate to accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We are deeply grateful to all the participants for taking the time to participate in our study and sharing their experiences with us. We are also thankful to Ding Wang and Shantanu Prabhat for their valuable feedback on our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">NationalStrategy-for-AI-Discussion-Paper</title>
		<author>
			<persName><surname>Niti Aayog</surname></persName>
		</author>
		<idno>12/03/2021). 2021. Responsible-AI-22022021</idno>
		<ptr target="https://www.niti.gov.in/sites/default/files/2021-02/Responsible-AI-22022021.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Accessed on 12/03/2021</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Community Control Over Police Surveillance</title>
		<ptr target="https://www.microsoft.com/en-(Accessedon09/08/2021" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>ACLU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Aether. 2021. Microsoft HAX Toolkit</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Guidelines for Human-Interaction</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penny</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamsi</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kori</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kikin-Gil</surname></persName>
		</author>
		<author>
			<persName><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300233</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300233" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Systems<address><addrLine>Glasgow, Scotland Uk; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>19)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<ptr target="https://books.google.co.in/books?id=53ZoDwAAQBAJ" />
		<title level="m">Apostles of Certainty: Data Journalism and the Politics of University Press</title>
				<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Natali</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanne</forename><surname>Helberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claes H De</forename><surname>Kruikemeier</surname></persName>
		</author>
		<author>
			<persName><surname>Vreese</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In AI we trust? Perceptions about automated decision-making by artificial</title>
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="611" to="623" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bottom of the data pyramid: Big data and the global south</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Rachel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Vera</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramya</forename><surname>Pedemonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moninder</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Dennis</forename><surname>Varsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03012</idno>
		<ptr target="http://arxiv.org/abs/1909.03012" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">D</forename><surname>Ashoori</surname></persName>
		</author>
		<author>
			<persName><surname>Weisz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02675</idno>
		<title level="m">AI We Trust? Factors That Influence Trustworthiness of AI-infused Decision-Making Processes</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How Americans view U.S. tech companies in 2020 | Pew Research Center</title>
		<author>
			<persName><surname>Auxier</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/fact-tank/2020/10/27/how-cessedon09/06/2021" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">L. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">671</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">It&apos;s Reducing a Human Being to a Percentage&apos;: Perceptions of Justice in Algorithmic Decisions</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Van Kleek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrik</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName><surname>Lyngs</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173951</idno>
		<ptr target="https://doi.org/10.1145/3173574.3173951" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018-06">Jun Zhao, and Nigel 2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Twitter Discourse as a Lens into Politicians&apos; Interest in Technology and Development</title>
		<author>
			<persName><forename type="first">Joyojeet</forename><surname>Bozarth</surname></persName>
		</author>
		<author>
			<persName><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287098.3287129</idno>
		<ptr target="https://doi.org/10.1145/3287098.3287129" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Information and Communication Technologies and Devel</title>
				<meeting>the Tenth International Conference on Information and Communication Technologies and Devel<address><addrLine>Ahmedabad, India; New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>ICTD &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complete counterbalancing of immediate sequential effects in a Latin square design</title>
		<author>
			<persName><forename type="first">V</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="525" to="528" />
			<date type="published" when="1958">1958. 1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Demmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Kam</surname></persName>
		</author>
		<title level="m">Sergiu Joyojeet Pal, Rabin Patra, Sonesh Surana, and Kevin Fall. 2005. The for technology in developing regions</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="25" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The algorithmic imaginary: exploring ordinary affects of Facebook algorithms</title>
		<author>
			<persName><surname>Bucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information, communication &amp; society</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are Explanations Always Important? A Study of Deployed, Low-Cost Intelligent Interactive Sys</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Lauzon</surname></persName>
		</author>
		<idno type="DOI">10.1145/2166966.2166996</idno>
		<ptr target="https://doi.org/10.1145/2166966.2166996" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM International Conference on Intelligent User</title>
				<meeting>the 2012 ACM International Conference on Intelligent User<address><addrLine>Lisbon, Portugal; York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
	<note>IUI &apos;12)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Task-dependent aversion</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald R</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="809" to="825" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scary Robots&quot;: Examining Public Responses to AI</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanta</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><surname>Dihal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3314232</idno>
		<ptr target="https://doi.org/10.1145/3306618" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, (AIES &apos;19)</title>
				<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, (AIES &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Portrayals and perceptions of AI and why they matter</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanta</forename><surname>Dihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsay</forename><surname>Singler</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">User in social networking services: A comparison of Facebook and LinkedIn</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Shuchih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">Yenching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Behavior</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="207" to="217" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the dimensionality of organizational justice: a construct validation of a measure</title>
		<author>
			<persName><forename type="first">Jason A</forename><surname>Colquitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied psychology</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regulation Of The European Parliament And Of The Council -Laying Down Harmonised Rules On Artificial Intelligence</title>
		<ptr target="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&amp;" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Act) And Amending Certain Union Legislative Acts</title>
		<imprint/>
	</monogr>
	<note>Accessed on 09/09/2021</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The effects of transparency on trust in and of a content-based art recommender</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyan</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramlal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Someren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rutledge</surname></persName>
		</author>
		<author>
			<persName><surname>Stash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><surname>Wielinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="455" to="496" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Raghu Dharmaraju, and Rahul Panicker. 2020. Pest Management In Cotton Farms: An AI-System Case Study from Global South</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruvin</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasaheb</forename><surname>Dhame</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event<address><addrLine>CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
	<note>KDD &apos;20). for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The development of cognitive anthropology</title>
		<author>
			<persName><surname>Roy G D'andrade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Why Do They to Use My Robot? Reasons for Non-Use Derived from a Long-Term Study</title>
		<author>
			<persName><forename type="first">Somaya</forename><surname>Maartje De Graaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ben Allouch</surname></persName>
		</author>
		<author>
			<persName><surname>Van Dijk</surname></persName>
		</author>
		<idno type="DOI">10.1145/2909824.3020236</idno>
		<ptr target="https://doi.org/10.1145/2909824" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/IEEE International Conference on Interaction</title>
				<meeting>the 2017 ACM/IEEE International Conference on Interaction<address><addrLine>Vienna, Austria; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="224" to="233" />
		</imprint>
	</monogr>
	<note>HRI &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="https://analyticsindiamag.com/cbse-integrates-ai-(Accessedon" />
		<title level="m">Srishti Deoras. 2020. CBSE Integrates AI Curriculum In 200 Indian Schools In Collaboration With IBM</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Algorithms Ruin Everything&quot;: #RIPTwitter, Folk Theories, and Resistance to Algorithmic in Social Media</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><surname>Gergle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Birnholtz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025659</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human in Computing Systems<address><addrLine>Denver, Colorado, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3163" to="3174" />
		</imprint>
	</monogr>
	<note>CHI &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explaining Models: An Empirical Study of How Explanations Fairness Judgment</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Vera</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">K E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Bellamy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301275.3302310</idno>
		<ptr target="https://doi.org/10.1145/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on User Interfaces</title>
				<meeting>the 24th International Conference on User Interfaces<address><addrLine>Marina del Ray, California; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="275" to="285" />
		</imprint>
	</monogr>
	<note>IUI &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">UX Design Innovation: Challenges for Working with Machine Learning as a Design</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Dove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Halskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><surname>Zimmerman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025739</idno>
		<ptr target="https://doi.org/10.1145/3025453.3025739" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems<address><addrLine>Denver, Colorado, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
	<note>17). Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Runaway Feedback Loops in Predictive Policing</title>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Ensign</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatasubramanian</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v81/ensign18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Friedler</surname></persName>
		</editor>
		<meeting>the 1st Conference on Fairness, Accountability and Transparency ( Machine Learning Research<address><addrLine>New NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="160" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">First I &quot;like&quot; It, Then I Hide It: Folk Theories of Social Feeds</title>
		<author>
			<persName><forename type="first">Karrie</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aimee</forename><forename type="middle">Kevin</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><surname>Kirlik</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858494</idno>
		<ptr target="https://doi.org/10.1145/2858036.2858494" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2016 CHI Conference on Human Factors in Computing Systems<address><addrLine>San Jose, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2371" to="2382" />
		</imprint>
	</monogr>
	<note>CHI &apos;16). for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Always Assumed That I Wasn&apos;t Really That Close to [Her]&quot;: Reasoning about Invisible Algorithms in News Feeds</title>
		<author>
			<persName><forename type="first">Motahhare</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aimee</forename><surname>Rickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Aleyasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karrie</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sandvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems<address><addrLine>Seoul, Republic of Korea; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
	<note>CHI &apos;15). for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Be Careful; Things Can Be Worse than They Appear&quot;: Understanding Biased Algorithms and Users&apos; Behavior Around Them in Rating Platforms</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karrie</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/ICWSM/article/view/14898" />
	</analytic>
	<monogr>
		<title level="m">of the Eleventh International AAAI Conference on Web and Social Media. of the International AAAI Conference on Web and Social</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long-Term Trends in the Public Perception of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI&apos;17)</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI&apos;17)<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="963" to="969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What Can AI Do for Me? Evaluating Machine Learning Interpretations in Cooperative Play</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 24th International Conference on Intelligent User Interfaces del Ray, California) (IUI &apos;19)</title>
				<meeting>the 24th International Conference on Intelligent User Interfaces del Ray, California) (IUI &apos;19)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Metaphors we&apos;re colonised by? The case of data-driven educational technologies in Brazil</title>
		<author>
			<persName><forename type="first">Giselle</forename><surname>Martins Dos Santos Luiz Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename><surname>Rosado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemgruber</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaciara</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sá</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Media and Technology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="46" to="60" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Belief, Attitude, Intention, and Behavior: An Introduction to Theory and Research. and Rhetoric</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Fishbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Icek</forename><surname>Ajzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977. 1977</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="130" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How Google Search Works</title>
		<ptr target="https://www.youtube.com/watch?v=0eKVizvYSUQ" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Elonnai</forename><surname>Goudarzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amber</forename><surname>Hickok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><surname>Mohandas</surname></persName>
		</author>
		<author>
			<persName><surname>Bidare</surname></persName>
		</author>
		<author>
			<persName><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Rathi</surname></persName>
		</author>
		<title level="m">AI in Banking and Finance</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Black box measures? How to study people&apos;s algorithm skills. Information</title>
		<author>
			<persName><forename type="first">Eszter</forename><surname>Hargittai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodora</forename><surname>Djukaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaelle</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication &amp; Society</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="764" to="775" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Comparing internet experiences and in amazon mechanical turk and population-based survey samples</title>
		<author>
			<persName><forename type="first">Eszter</forename><surname>Hargittai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Shaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2378023119889834</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">What&apos;s wrong with cross-cultural comparisons of subjective Likert The reference-group of personality and social psychology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Darrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Greenholtz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">903</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The automation of leadership functions: Would people trust decision algorithms?</title>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Höddinghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Sondern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Hertel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Behavior</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">106635</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Explainable ai (ex-ai)</title>
		<author>
			<persName><surname>Holzinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Nataliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Ivankova</surname></persName>
		</author>
		<author>
			<persName><surname>Creswell</surname></persName>
		</author>
		<title level="m">Mixed methods. Qualitative research in applied linguistics: A practical introduction</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="135" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust AI</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445923</idno>
		<ptr target="https://doi.org/10.1145/3442188" />
	</analytic>
	<monogr>
		<title level="m">of the 2021 ACM Conference on Fairness, Accountability, and (Virtual Event, Canada) (FAccT &apos;21)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="624" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FarmChat: a conversational agent to answer farmer queries</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Bhansali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khai</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shwetak</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">of the ACM on Interactive, Mobile, Wearable and Ubiquitous Tech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Algorithmic Manand Algorithmic Competencies: Understanding and Appropriating Algorithms in Gig Work</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Jarrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information in Contemporary Society</title>
				<editor>
			<persName><forename type="first">Caitlin</forename><surname>Christian-Lamb</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michelle</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Nardi</surname></persName>
		</editor>
		<meeting><address><addrLine>Natalie Greene Taylor; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="578" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Jayapal Joins Colleagues In Introducing Bicameral Legislation to Ban Government Use of Facial Recognition, Other Biometric Technology -Congresswoman Pramila Jayapal</title>
		<author>
			<persName><forename type="first">Pramile</forename><surname>Jayapal</surname></persName>
		</author>
		<ptr target="https://jayapal.house.gov/2020/06/25/jayapal-joins-rep-pressley-and-use-of-facial-recognition-other-biometric-technology/.(Accessedon" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Algorithmic Anxiety and Strategies of Airbnb Hosts</title>
		<author>
			<persName><forename type="first">Shagun</forename><surname>Jhaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Karpfen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judd</forename><surname>Antin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173995</idno>
		<ptr target="https://doi.org/10.1145/3173574.3173995" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Towards Realistic Individual Recourse and Actionable Explain Black-Box Decision Making Systems. CoRR abs</title>
		<author>
			<persName><forename type="first">Oluwasanmi</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warut</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Vijitbenjaronk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Ghosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09615</idno>
		<ptr target="http://arxiv.org/abs/1907.09615" />
		<imprint>
			<date type="published" when="1907">2019. 1907.09615 (2019</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Size, composiand distribution of health workforce in India: why, and where to invest?</title>
		<author>
			<persName><forename type="first">Anup</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Negandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhaib</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Zapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Mairem-Hilde De Graeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Zodpey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human resources for health</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Conceptual and methodological considerations in the study of trust and suspicion</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Knox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of conflict resolution</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1970">1970. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Internet usage in India -statistics &amp; facts | Statista</title>
		<author>
			<persName><surname>Keelery</surname></persName>
		</author>
		<ptr target="https://www.statista.com/topics/2157/internet-usage-in-india/.(Accessedon" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gage</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Heldreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Moessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sedley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Kramm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Woodruff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Futuristic: Public Perception of Artificial Intelligence 8 Countries. for Computing Machinery</title>
		<author>
			<persName><surname>Exciting</surname></persName>
		</author>
		<author>
			<persName><surname>Useful</surname></persName>
		</author>
		<author>
			<persName><surname>Worrying</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461702.3462605</idno>
		<ptr target="https://doi.org/10.1145/3461702.3462605" />
		<imprint>
			<biblScope unit="page" from="627" to="637" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Trust and technology</title>
		<author>
			<persName><surname>Kipnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trust in organizations: Frontiers of theory and research</title>
				<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">It&apos;s like learning a whole other The role of algorithmic skills in the curation of creative goods</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Klawitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eszter</forename><surname>Hargittai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3490" to="3510" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Will You Accept an Imperfect AI? Exploring Designs for Adjusting End-User Expectations of AI Systems</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300641</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300641" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Systems<address><addrLine>Glasgow, Scotland Uk; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>CHI &apos;19). Association for Computing</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">How facial recognition can ruin your life-intercept</title>
		<author>
			<persName><surname>Kofman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Artificial intelligence in online environments: Representative survey of public attitudes in germany</title>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Kozyreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Lorenz-Spreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Lewandowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Public attitudes towards algorithmic and use of personal data online: evidence from Germany, Great Britain, and the United States</title>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Kozyreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Lorenz-Spreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Lewandowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">M</forename><surname>Herzog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Humanities and Social Sciences Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><surname>Tyler</surname></persName>
		</author>
		<ptr target="https://www.thesocialdilemma.com/.(Accessedon09/10/2021" />
		<title level="m">/books?id=ddpyAwAAQBAJ Exposure Labs. 2020. The Social Dilemma</title>
				<meeting><address><addrLine>Thousand Oaks, CA</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>Organizations: Frontiers of Theory Research</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Faithful and Customizable Explanations of Black Box Models</title>
		<author>
			<persName><forename type="first">Ece</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3314229</idno>
		<ptr target="https://doi.org/10.1145/3306618.3314229" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
				<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society<address><addrLine>Honolulu, HI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
	<note>AIES &apos;19). for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Highly automated job interviews: Acceptance under the influence of stakes</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><surname>Papathanasiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Selection and Assessment</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="217" to="234" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The future of artificial intelligence work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">N</forename><surname>Landers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2021.106878</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2021.106878" />
	</analytic>
	<monogr>
		<title level="j">Human Behavior</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">106878</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Characterizing Multi-Click Search Behavior and the Risks and Opportunities of Changing during Use</title>
		<author>
			<persName><forename type="first">Chia-Jung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>De La Chica</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609588</idno>
		<ptr target="https://doi.org/10.1145/2600428.2609588" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval Coast</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval Coast<address><addrLine>Queensland, Australia; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;14)</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Trust in automation: Designing for approreliance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katrina</surname></persName>
		</author>
		<author>
			<persName><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="50" to="80" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management</title>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Kyung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2053951718756684</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A Human-Centered Approach to Algorithmic Services: for Fair and Motivating Smart Community Service Management That Allocates Donations to Non-Profit Organizations</title>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Kyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Lizarondo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025884</idno>
		<ptr target="https://doi.org/10.1145/3025453.3025884" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems<address><addrLine>Denver, Colorado, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>CHI &apos;17). Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Working with Machines: The Impact of Algorithmic and Data-Driven Management on Human Workers</title>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Kyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kusbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Metsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702548</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
				<meeting><address><addrLine>Seoul, Republic of Korea; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1603" to="1612" />
		</imprint>
	</monogr>
	<note>CHI &apos;15)</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Kyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Rich</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445570</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445570" />
		<title level="m">Who Is Included in Human Perceptions AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust. of the 2021 CHI Conference on Human Factors in Computing Systems Japan) (CHI &apos;21)</title>
				<meeting><address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Questioning the AI: In-Design Practices for Explainable AI User Experiences</title>
		<author>
			<persName><forename type="first">Vera</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gruen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376590</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376590" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. for Computing Machinery</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems. for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Algorithm apprecia-People prefer algorithmic to human judgment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Resistance to medical artificial intelligence</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><forename type="middle">K</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName><surname>Morewedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="650" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Algorithmic Authority: The Case of</title>
		<author>
			<persName><forename type="first">Caitlin</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Nardi</surname></persName>
		</author>
		<idno type="DOI">10.1109/HICSS.2015.95</idno>
		<ptr target="https://doi.org/10.1109/HICSS.2015.95" />
	</analytic>
	<monogr>
		<title level="m">48th Hawaii International Conference on System Sciences</title>
				<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="743" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Artificial policy in India: a framework engaging the limits of data-driven decision-making</title>
		<author>
			<persName><forename type="first">Vidushi</forename><surname>Marda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page">2133</biblScope>
			<date type="published" when="2018">2018. 2018. 20180087</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An model of organizational trust</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><surname>Schoorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academy of management review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="709" to="734" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">How Do You Want Your Chatbot? An Exploratory Wizard-of-Oz Study with Young, Urban Indians</title>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Indrani Medhi Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manisha</forename><surname>Magapu</surname></persName>
		</author>
		<author>
			<persName><surname>Subramony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Jacki</surname></persName>
		</author>
		<author>
			<persName><surname>Neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Computer Interaction -INTERACT 2017</title>
				<editor>
			<persName><forename type="first">Regina</forename><surname>Bernhaupt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Girish</forename><surname>Dalvi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anirudha</forename><surname>Joshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Devanuj</forename><forename type="middle">K</forename><surname>Balkrishan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacki</forename><forename type="middle">O</forename><surname>Neill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="441" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapila</forename><surname>Kitcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alao</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><surname>Abiola</surname></persName>
		</author>
		<ptr target="https://doteveryone.org.uk/report/peoplepowertech2020/.(Accessedon09/06/2021" />
		<title level="m">People, and Technology: The 2020 Digital Attitudes Report</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Make Artificial Intelligence in India, Make Artificial Intelligence Work for India: PM Modi</title>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Modi</surname></persName>
		</author>
		<ptr target="https://www.narendramodi.in/prime-minister-narendra-modi-inaugurated-wadhwani-institute-of-artificial-on09/09/2021" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The commitment-trust theory of marketing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelby D</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of marketing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="20" to="38" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The impact of using algorithms for managerial decisions on public employees&apos; procedural justice</title>
		<author>
			<persName><surname>Nagtegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Quarterly</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">101536</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Changing the paradigm: Trust and its role in public organizations</title>
		<author>
			<persName><surname>Ronald C Nyhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Review of Public Administration</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="109" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<ptr target="https://indiaai.gov.in/.(Accessedon08/24/2021" />
		<title level="m">Ministry of Electronics &amp; Information Technology. 2020. INDIAai</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The measurement of values across cultures: A pairwise comparison approach</title>
		<author>
			<persName><forename type="first">Shigehiro</forename><surname>Oishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwon</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Schimmack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phanikiran</forename><surname>Radhakrishan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Dzokoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ahadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of research in Personality</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="299" to="305" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">It Cannot Do All of My Work&quot;: Community Health Worker Perceptions of AI-Enabled Mobile Health Applications in Rural India</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chinasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srujana</forename><surname>Okolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName><surname>Vashistha</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445420</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445420" />
	</analytic>
	<monogr>
		<title level="m">of the 2021 CHI Conference on Human Factors in Computing Systems Japan) (CHI &apos;21). for Computing Machinery</title>
				<meeting><address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The importance of procedural justice in human-machine interactions: Intelligent systems as new decision agents in Computers in</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ötting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Günter</surname></persName>
		</author>
		<author>
			<persName><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Behavior</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Pair</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<ptr target="https://pair.withgoogle.com/guidebook/.on09/08/2021" />
	</analytic>
	<monogr>
		<title level="j">People + AI Research -Guidebok</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">The machine to aspire to: The computer in rural south India</title>
		<author>
			<persName><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The Technological Self in India: From Tech-Savvy Farmers a Selfie-Tweeting Prime Minister</title>
		<idno type="DOI">10.1145/3136560.3136583</idno>
		<ptr target="https://doi.org/10.1145/3136560.3136583" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Information and Communication Technologies and Development Pakistan) (ICTD &apos;17)</title>
				<meeting>the Ninth International Conference on Information and Communication Technologies and Development Pakistan) (ICTD &apos;17)<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Digital Payment and Its Discontents: Street Shops and the Indian Government&apos;s Push for Cashless Transactions</title>
		<author>
			<persName><forename type="first">Joyojeet</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyank</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnav</forename><surname>Kameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Parameshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Johri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing for Computing Machinery</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">How model accuracy and explanation fidelity influence user trust</title>
		<author>
			<persName><forename type="first">Gwenn</forename><surname>Papenmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christin</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName><surname>Seifert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12652</idno>
		<ptr target="http://arxiv.org/abs/1907.12652" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Data-driven journalism and the public good</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Parasie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Dagiral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-assisted-reporters&quot; and &quot;programmer-journalists&quot; in Chicago. New media &amp; society</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="853" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">2020. 3.AI in Healthcare in India: Applications, Challenges and Risks | Chatham House -International Affairs Tank</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Parry</surname></persName>
		</author>
		<author>
			<persName><surname>Aneja</surname></persName>
		</author>
		<ptr target="https://www.chathamhouse.org/2020/07/artificial-intelligence-healthcare-insights-india-0/3-ai-healthcare-india-applications.(Accessedon" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Data and its (dis) contents: A survey of dataset development and use in machine learning research</title>
		<author>
			<persName><forename type="first">Amandalynne</forename><surname>Paullada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">Denand</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hanna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100336</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Understanding User Beliefs About Algorithmic Curation in the Facebook News Feed</title>
		<author>
			<persName><forename type="first">Emilee</forename><surname>Rader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702174</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems Republic of Korea) (CHI &apos;15). for Computing Machinery</title>
				<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems Republic of Korea) (CHI &apos;15). for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing</title>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">N</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamila</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smith-Loud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Theron</surname></persName>
		</author>
		<author>
			<persName><surname>Barnes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372873</idno>
		<ptr target="https://doi.org/10.1145/3351095.3372873" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency Spain) (FAT* &apos;20). for Computing Machinery</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency Spain) (FAT* &apos;20). for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Trust in close relaof personality and social psychology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><surname>Zanna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985. 1985</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Landmark AI legislation could tackle algorithmic bias -Verdict</title>
		<ptr target="https://www.verdict.co.uk/ethical-ai-regulation-eu/.on09/09/2021" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Govt pushes for AI curriculum across schools, universities</title>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<ptr target="https://www.techcircle.in/2021/03/16/govt-pushes-for-ai-curriculum" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Accessed on 08/22/2021</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Virtual Event, Canada) (FAccT &apos;21). Association for Computing Machinery</title>
		<author>
			<persName><forename type="first">Nithya</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Arnesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodku</forename></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445896</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445896Sambasivanand" />
	</analytic>
	<monogr>
		<title level="m">2021 ACM Conference on Fairness, Accountability, and Trans</title>
				<meeting><address><addrLine>New York, NY, USA; Jess Holbrook</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">mar Prabhakaran. 2021. 2018. 2018</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="68" to="71" />
		</imprint>
	</monogr>
	<note>Toward responsible AI for the billion users</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Everyone Wants to Do the Model Work, Not the Data Work&quot;: Data Cascades in High-Stakes AI</title>
		<author>
			<persName><forename type="first">Nithya</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Kapania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Highfill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Akrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><forename type="middle">M</forename><surname>Aroyo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445518</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445518" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems Japan) (CHI &apos;21). for Computing Machinery</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems Japan) (CHI &apos;21). for Computing Machinery<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Auditing algorithms: Research methods for detecting discrimination on platforms. Data and discrimination: converting critical into productive inquiry</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karrie</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Langbort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Emerging from AI utopia</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Santow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Transparency and trust in artificial intelligence systems</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Biessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timm</forename><surname>Teubner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Decision Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="260" to="278" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Algorithms as culture: Some tactics for the ethnography of systems</title>
	</analytic>
	<monogr>
		<title level="j">Big data &amp; society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2053951717738104</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">NITI Aayog wants dedicated oversight body for use of artificial intelligence</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Sharma</surname></persName>
		</author>
		<ptr target="https://economictimes.indiatimes.com/news/artificial-intelligence/articleshow/79260810.cms?from=mdr" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Accessed on 08/22/2021</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Everyday Algorithm Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motahhare</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Holstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/3479577</idno>
		<ptr target="https://doi.org/10.1145/3479577" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 433</title>
				<meeting>ACM Hum.-Comput. Interact. 5, CSCW2, Article 433</meeting>
		<imprint>
			<date type="published" when="2021-10">2021. oct 2021</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Anubhutie</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Srikara</surname></persName>
		</author>
		<ptr target="https://www.dvara.com/blog/2020/04/13/artificial-intelligence-in-digital-credit-in-india/.(Accessedon09/06/2021" />
	</analytic>
	<monogr>
		<title level="j">Dvara Research Blog | Artificial Intelligence in Digital Credit in India</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Public Attitudes Toward Computer Algorithms | Pew Research Center</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/internet/2018/11/16/public-attitudes-toward-computer-algorithms/.(Accessedon08/22/2021" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Smith-Renner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Findlater</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376624</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376624" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. for Computing Machinery</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems. for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Is That Traffic Light Tracking You? A Case Study on a Surveillance Technology in Seattle</title>
		<author>
			<persName><forename type="first">Cynthias</forename><surname>Spiess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions Technology and Society</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="19" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Mega e-Infrastructure Project India</title>
		<author>
			<persName><forename type="first">Janaki</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Johri</surname></persName>
		</author>
		<idno type="DOI">10.1145/2516604.2516625</idno>
		<ptr target="https://doi.org/10.1145/2516604.2516625" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Information and Communication Technologies and Development: Full Papers</title>
				<meeting>the Sixth International Conference on Information and Communication Technologies and Development: Full Papers<address><addrLine>Cape Town, South Africa; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
	<note>ICTD &apos;13)</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Extent and impact of response biases in cross-national research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gerard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepa</forename><surname>Tellis</surname></persName>
		</author>
		<author>
			<persName><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Research in Marketing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="329" to="341" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Towards an Future That Works for Vocational Workers</title>
		<author>
			<persName><forename type="first">Divy</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithya</forename><surname>Sambasivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020</title>
				<meeting>the 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<idno type="DOI">10.1145/3313831.3376674</idno>
		<ptr target="https://doi.org/10.1145/3313831" />
		<title level="m">CHI Conference on Human in Computing Systems. Association for Com-Machinery</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A general inductive approach for analyzing qualitative data</title>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of evaluation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">The Relationship between Trust in AI and Trustworthy Machine Learning Technologies</title>
		<author>
			<persName><forename type="first">Mhairi</forename><surname>Toreini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kovila</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Coopamootoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aad</forename><surname>Gon-Zelaya</surname></persName>
		</author>
		<author>
			<persName><surname>Van Moorsel</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372834</idno>
		<ptr target="https://doi.org/10.1145/3351095.3372834" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency<address><addrLine>Barcelona, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
	<note>FAT* &apos;20). for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Optimizing for Trust: The New Metric of Sucfor Today&apos;s Economy | by Joe Toscano RE: Write | Medium</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Toscano</surname></persName>
		</author>
		<ptr target="https://medium.com/re-write/optimizing-for-trust-the-new-metric-of" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Accessed on 09/09/2021</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">General Data Protection Regulation (GDPR) Compliance Guidelines</title>
		<idno>09/10/2021</idno>
		<ptr target="https://gdpr.eu/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>European Union</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Distinguishing power, authority &amp; legitimacy: Taking Weber at his word by using resources-exchange analysis</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Uphoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polity</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="295" to="322" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Rise of Artificial Intelligence in Healthcare Startups in India</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vijai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Worakamol</forename><surname>Wisetsri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Management</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="48" to="52" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Trust is a Critical Customer Experience Metric: Determine How Perceptions of Data Privacy and Security Affect Your Business</title>
		<author>
			<persName><surname>Voice+code</surname></persName>
		</author>
		<ptr target="https://www.voiceandcode.com/our-insights/2019/3/26/trust-is-a-critical-customer-experience-metric-determine-how-perceptions-of-data-privacy" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Accessed on 09/09/2021</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Factors Influencing Perceived Fairness in Algorithmic Decision-Making: Algorithm Outcomes, Development Procedures, and Individual Differences</title>
		<author>
			<persName><forename type="first">Ruotong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of the 2020 Conference on Human Factors in Computing Systems</title>
				<meeting><address><addrLine>Honolulu, HI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>CHI &apos;20). for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<ptr target="https://www.similarweb.com/apps/top/google/app-index/in/all/top-free/.on09/10/2021" />
		<title level="m">Top Apps Ranking -Most Popular Apps in India | Similarweb</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Economy and society: An outline of interpretive sociology. 1. of California Press</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Weber</surname></persName>
		</author>
		<ptr target="https://www.b2binternational.com/publications/understanding-accounting-cultural-bias-global-b2b-research/.(Accessedon09/07/2021" />
	</analytic>
	<monogr>
		<title level="m">Conor Wilcock. 2020. Cultural Biases in Market Research -B2B International</title>
				<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Culture in Action: Unpacking Capacities to Inform Assets-Based Design</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Disalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betsy</forename><surname>Disalvo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376329</idno>
		<ptr target="https://doi" />
	</analytic>
	<monogr>
		<title level="m">of the 2020 CHI Conference on Human Factors in Computing Systems. for Computing Machinery</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">A cold, technical decision-maker&quot;: Can AI provide explainability, negotiability</title>
		<author>
			<persName><forename type="first">Allison</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmin</forename><forename type="middle">Asare</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">Jameson</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Gkiza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Moessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">Lynette</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick Gage</forename><surname>Kelley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00874</idno>
		<ptr target="https://arxiv.org/abs/2012.00874" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A Qualitative Exploration of Perceptions of Algorithmic Fairness</title>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Rousso-Schindler</surname></persName>
		</author>
		<author>
			<persName><surname>Warshaw</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174230</idno>
		<ptr target="https://doi.org/10.1145/3173574.3174230" />
	</analytic>
	<monogr>
		<title level="m">of the 2018 CHI Conference on Human Factors in Computing Systems. for Computing Machinery</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Comparing robot interaction scenarios using live and video based methods: towards a novel methodological approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kheng</forename><forename type="middle">Lee</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koay</surname></persName>
		</author>
		<author>
			<persName><surname>Dautenhahn</surname></persName>
		</author>
		<idno type="DOI">10.1109/AMC.2006.1631754</idno>
		<ptr target="https://doi.org/10.1109/AMC.2006.1631754" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Advanced Motion Control</title>
				<meeting><address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="750" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">How Do Visual Explanations Foster End Users&apos; Appropriate Trust in Machine Learning</title>
		<author>
			<persName><forename type="first">Fumeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuanyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Scholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><forename type="middle">L</forename><surname>Arendt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377325.3377480</idno>
		<ptr target="https://doi.org/10.1145/3377325.3377480" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Intelligent User Interfaces Italy) (IUI &apos;20)</title>
				<meeting>the 25th International Conference on Intelligent User Interfaces Italy) (IUI &apos;20)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
