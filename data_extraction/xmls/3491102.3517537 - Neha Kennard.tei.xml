<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving Separation-of-Concerns Problems in Collaborative Design of Human-AI Systems through Leaky Abstractions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hariharan</forename><surname>Subramonyam</surname></persName>
							<email>harihars@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Im</surname></persName>
							<email>imjane@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colleen</forename><surname>Seifert</surname></persName>
							<email>seifert@umich.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eytan</forename><surname>Adar</surname></persName>
							<email>eadar@umich.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Solving Separation-of-Concerns Problems in Collaborative Design of Human-AI Systems through Leaky Abstractions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3491102.3517537</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-07-22T05:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-AI systems</term>
					<term>UX design</term>
					<term>AI applications</term>
					<term>Industry practices</term>
					<term>design processes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In conventional software development, user experience (UX) designers and engineers collaborate through separation of concerns (SoC): designers create human interface specifications, and engineers build to those specifications. However, we argue that Human-AI systems thwart SoC because human needs must shape the design of the AI interface, the underlying AI sub-components, and training data. How do designers and engineers currently collaborate on AI and UX design? To find out, we interviewed 21 industry professionals (UX researchers, AI engineers, data scientists, and managers) across 14 organizations about their collaborative work practices and associated challenges. We find that hidden information encapsulated by SoC challenges collaboration across design and engineering concerns. Practitioners describe inventing ad-hoc representations exposing low-level design and implementation details (which we characterize as leaky abstractions) to "puncture" SoC and share information across expertise boundaries. We identify how leaky abstractions are employed to collaborate at the AI-UX boundary and formalize a process of creating and using leaky abstractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Human-centered computing → HCI theory, concepts and models; • Software and its engineering → Designing software; Abstraction, modeling and modularity; Collaboration in software development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In conventional software development, work processes for user experience (UX) designers and software engineers are optimized for efficiency through separation of concerns (SoC) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b83">84]</ref>. UX roles focus on human psychology and design by working with endusers to define system requirements. Software engineers skilled in computer programming then implement those requirements in a system <ref type="bibr" target="#b83">[84]</ref>. For example, to create a conventional (non-AI) To-Do List application, the designer first gathers information from different end-users (students, IT professionals, educators, etc.) on how they define and manage their tasks. Based on those insights, the designer generates several interface alternatives to find one that meets end-user needs. Using knowledge about graphical user interfaces (GUI), established design guidelines, and design tools, designers generate specifications for all aspects of software behavior, including UI designs, functional requirements, style guides, data requirements such as task description lengths, interaction maps, and evaluation criteria <ref type="bibr" target="#b64">[65]</ref>. Finally, this highly controlled and abstracted knowledge is handed to engineers through serial coordination <ref type="bibr" target="#b83">[84]</ref> to be translated into technical requirements and subsequently implemented as software code <ref type="bibr">[77]</ref>.</p><p>However, the efficiency of SoC that works well in conventional software development may fail for Human-Centered AI (HAI) systems. HAI systems differ in several important ways: (1) they offers greater capacity for human-like intelligent behavior, (2) they dynamically learn and adapt their behavior over time through feedback and learning, and (3) their outputs can be non-deterministic, making it difficult to align output presentation with end-user expectations <ref type="bibr" target="#b101">[102]</ref>. By examining the complex dependencies across different components in the AI lifecycle, prior research has laid out desiderata for AI systems. This includes ensuring contextually relevant datasets and comprehensive and comprehensible verification of AI models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, adapting for AI uncertainties and failures in human interface design <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39]</ref>, and incorporating human and social interpretations for AI model design <ref type="bibr" target="#b14">[15]</ref>. These demands make it challenging to separate current UX work processes from AI software development tasks. Consider designing a "smart" To-Do List application to automatically create task items from email content (e.g., <ref type="bibr" target="#b35">[36]</ref>). In taking a human-centered approach, UX roles must identify representative AI dataset characteristics based on diverse users covering a range of expressive email tasks. They need to support creating "ground truth" data to define how users want to generate tasks from those emails. UX designers need to provide inputs about AI model behavior by considering how the AI experience will integrate with end-user task workflow: what to automate, when to offer assistance, and when to maintain human control of tasks. Finally, designers must consider uncertainties in AI model outputs and design interface adaptations for explainability, failures, feedback, and hand-off. Consequently, combining these rationalistic and design goals for HAI requires multidisciplinary collaboration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b92">93]</ref>.</p><p>While a growing body of HAI design guidelines point towards blending AI and UX work practices <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51]</ref>, we still lack concrete knowledge on how to achieve such collaboration. Recent work has highlighted numerous concerns due to SoC at the AI-UX boundaries, including challenges in understating AI capabilities <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b101">102]</ref>, difficulty in specifying AI system requirements <ref type="bibr" target="#b97">[98]</ref>, and prototyping HAI interfaces <ref type="bibr" target="#b99">[100]</ref>. Further, current practices in which the AI components are developed before envisioning the human user experience (i.e., AI-first design process) have led to AI systems that do not align with human needs (e.g., incorrect labeling <ref type="bibr" target="#b61">[62]</ref>, biased auto-cropping <ref type="bibr" target="#b44">[46]</ref> of social media photos, faulty facial recognition features <ref type="bibr" target="#b17">[18]</ref>, etc.). Understanding how industry practitioners can and should collaborate across technical and non-technical roles is essential for producing HAI systems that can be successful in real-world settings. In this work, our goal is to improve our understanding of how industry practitioners (both UX and AI roles) work and the challenges and solutions they have identified in creating human-centered AI systems. Ultimately, we aim to propose a better approach for team-based HAI development based on the derived insights.</p><p>Research Question 1: What challenges do HAI designers and engineers face in creating HAI systems following the standard SoC process? Research Question 2: How do designers and engineers adapt their work processes to improve HAI outcomes? Research Question 3: How might HAI teams integrate concerns across disciplinary boundaries to align human and AI needs?</p><p>To investigate these questions, we first collected and analyzed a total of 280 HAI design guidelines across different industry sources. From this analysis, we derived a component model for designing HAI applications that span data, ML model, user interface, and end-user mental models (see Figure <ref type="figure">2</ref>). Using our model as a guide, we interviewed 21 industry practitioners (UX designers, AI engineers, data scientists, and product managers) across 14 different organizations to understand their current practices for creating HAI systems. Through the interviews, we identify sources of friction in the HAI development process and uncover how practitioners currently bridge the design-engineering boundary. Our findings show that current HAI workflows rarely begin with end-user needs due to the challenges for designers in defining AI experiences upfront. In practice, HAI designers are now working to shape user experiences around novel AI capabilities. However, successful teams circumvent collaboration challenges by delaying commitment to solutions until later in the design process. Finally, we highlight specific practices around knowledge sharing across expertise boundaries that oppose established SoC practices. As opposed to SoC and information hiding, we find that in successful teams, designers and engineers communicate across boundaries through "leaky" abstractions that facilitate a collaborative design process. Many existing practices have evolved in a haphazard fashion. We attempt to better formalize the use of leaky abstractions.</p><p>We contribute to the current understanding of challenges faced by UX roles <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b101">102]</ref> and AI roles <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b103">104]</ref> in developing AI systems that align with human needs, values and are useful and usable by people <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b96">97]</ref>. Through the lens of the component model of HAI guidelines, we describe the limitations of existing SoC practices that are optimized for efficiency but hinder cross-disciplinary collaboration. Further, we discuss alternatives to standard software development workflows to bridge knowledge boundaries and highlight solutions for collaboration and co-design of HAI systems. Finally, through our discussion, we offer advice for software organizations to realize HAI guidelines and make recommendations for HAI pedagogy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Human-Centered AI frames AI as technology that "augments the abilities of, addresses the societal needs of, and draws inspiration from human beings" <ref type="bibr" target="#b66">[67]</ref>. Based on this vision, research in HCI and AI communities has characterized and detailed domain-specific viewpoints <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b89">90]</ref>, identified challenges <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b101">102]</ref>, and put forth requirements and strategies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref> to operationalize HAI. Here we synthesize what is known about current humancentered software development (HCSD) processes, expertise, design workflows, and boundary representations to identify challenges to designing HAI systems. Through this synthesis, we highlight the gap we aim to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Human-Centered Approaches in Industry</head><p>Software Teams 2.1.1 Modular Software Development: HCSD is a complex problem requiring knowledge and expertise beyond what any single person can possess. When multiple individuals are involved (UX designers, software engineers, and database experts, etc.), the preferred approach is to decompose the system into modules and tasks that can be carried out relatively independently by different people <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b83">84]</ref>. Often, system modules and work-team structures observe a homomorphic relation <ref type="bibr" target="#b23">[24]</ref>. For instance, UX professionals create the user interface, and engineers implement the underlying functionality. Specific to HAI, Amershi et al. propose a nine stage software engineering workflow for machine learning that begins with specifying model requirements and subsequently, data collection, features engineering, and model development <ref type="bibr" target="#b4">[5]</ref>. Prior studies with data scientists <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b103">104]</ref>   <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b102">103]</ref>. Further, while agile methodologies have improved HCSD workflows in conventional software <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b77">78]</ref>, the short development cycles and rapid turnarounds are infeasible for AI development which requires a longer time to design and implement <ref type="bibr" target="#b100">[101]</ref>.</p><p>2.1.2 Information Hiding, Abstractions, and Collaboration: In multidisciplinary teams, to reduce dependencies between tasks, team members first define the module's outward-facing interface while the implementation details are abstracted from one another (i.e., information hiding) <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b75">76]</ref>. In HCSD, designers take a "UX first" approach to design the system's 'user interface' <ref type="bibr" target="#b86">[87]</ref>. Here, the user interface can be considered the highest level module for endusers to invoke. Designers map end-user needs into interface design specifications. Engineers who also understand the language of the user interface can translate interface representation into implementation <ref type="bibr" target="#b83">[84]</ref>. In other words, the user interface acts as a natural 'seam' for designers and engineers to coordinate. However, such interface-level abstractions quickly break down when designing AI-powered applications. For instance, in investigating how designers sketch experiences for natural language processing (NLP), Yang et al. highlight the challenges to design abstractly and propose the need for ML specific abstractions (e.g., language, capabilities, and experiential qualities of NLP) to support designers <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b100">101]</ref>. Yet, other work has shown that it can be challenging to enforce strict abstractions <ref type="bibr" target="#b81">[82]</ref>. In fact, ML is beneficial in cases in which behavior cannot be explicitly specified through software logic <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">77]</ref>. Further, in the case of HAI, the contract nature of abstractions hides implementation details that are necessary for designing AI adaptations, such as explainability and feedback <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. With AI, designers and engineers need to bridge abstraction levels along a part-whole hierarchy to center people in the design of AI sub-components, and within an implementation hierarchy to offer interface adaptations to AI uncertainties <ref type="bibr" target="#b93">[94]</ref>.</p><p>In sum, prior work has uncovered limitations of existing HCSD workflows when it comes to HAI development. However, previous studies tended to focus solely on data scientists <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b103">104]</ref> or designers <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b100">101]</ref>. It remains an open question on how to handle abstraction in multidisciplinary collaboration between technical and non-technical roles. Our work aims to address this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Key Design and Engineering Challenges for HAI</head><p>2.2.1 Challenges for Designers: Design knowledge for human-AI systems is comprised of (1) understanding task characteristics, including type of goals and data representations, (2) machine learning paradigms, (3) human-AI interactions such as machine teaching, and (4) AI-human interactions such as interpretability <ref type="bibr" target="#b27">[28]</ref>. However, current UX designers are not trained in these aspects of HAI systems. First, UX designers lack the expertise to generate design ideas for incorporating AI in human tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b101">102]</ref>. As a result, they often misunderstand the capabilities of ML models and propose designs that can be difficult to implement <ref type="bibr" target="#b51">[52]</ref>. Second, given that AI takes a long time to build <ref type="bibr" target="#b100">[101]</ref>, rapid prototyping with ML through a "fail fast, fail often" approach characteristic of UX design is challenging for HAI <ref type="bibr" target="#b99">[100]</ref>. Moreover, AI requires vertical end-to-end prototyping to identify uncertainties and edge cases and to create UI adaptations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>. However, black-box views of ML make it difficult for designers to understand, design, and evaluate with AI <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>. Third, UX processes favor creativity and imagination of desired futures, which contradicts AI's emphasis on specificity and accuracy <ref type="bibr" target="#b97">[98]</ref>. This introduces friction into the design thinking process for HAI systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.2</head><p>Challenges for Engineers: Similarly, engineers focused on algorithms and techniques fail to consider human perspectives during initial experimentation and AI prototyping processes <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b60">61]</ref>. Several aspects of HAI design need to be incorporated throughout AI workflow, including identifying model requirements, data collection and labeling, features engineering, and model training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b79">80]</ref>. But expertise in HCI and involvement in exploring human needs are lacking in engineering training. Engineers who are ML novices were shown to experience breakdowns in early-stage software development due to lack of specialized design schemas, insufficient understanding of the design process, and sub-optimal design solutions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">41]</ref>. Consequently, even when designers suggest modifications for better human-centered experience design, model and data changes to the AI may be challenging to execute. In AI techniques such as deep learning, it can be challenging to identify specific functional areas to address human user issues <ref type="bibr" target="#b7">[8]</ref>. Further, by focusing on creating the algorithm, engineers often fail to consider the AI experience as a whole and their involvement in UX design tapers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref>. AI and UX practitioners can benefit from a symbiotic relationship <ref type="bibr" target="#b21">[22]</ref>. HCI perspectives about the user interface can improve AI through better quality feedback on performance <ref type="bibr" target="#b80">[81]</ref>. For example, AI output presentation can impact end-users' subjective perception of errors and how they adjust their expectations about AI <ref type="bibr" target="#b53">[54]</ref>.</p><p>To summarize, prior research has separately uncovered design and engineering challenges and respective knowledge barriers for HAI. However, we lack an understanding of the entire design and engineering pipeline for creating HAI systems in a multidisciplinary team-based approach. In this work, we build on existing research by studying how industry practitioners (both UX and AI roles) collaborate across technical and non-technical roles. This includes challenges that arise in work processes, workarounds the practitioners have created to address the challenges, and their needs for solutions that do not yet exist. We propose a concrete approach for successful team-based HAI development based on this understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Boundary Representations for Collaboration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Role of Boundary Representations:</head><p>In complex domains such as HAI, teams would ideally address knowledge differences or "symmetry of ignorance" between HCI and AI professionals through collaboration and social creativity <ref type="bibr" target="#b34">[35]</ref>. Prior work on software collaboration has identified three types of knowledge boundaries, including (1) assembling-how information should be structured, (2) designing-how information artifacts are designed, and (3) intended user interaction-how users interact with designed information <ref type="bibr" target="#b95">[96]</ref>.</p><p>The goal for collaboration is to bridge the knowledge boundaries described in section 2.2 to acquire common ground for interaction <ref type="bibr" target="#b87">[88]</ref>. Common ground in collaborative work includes content common ground and process common ground <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b67">68]</ref>. In HAI, the content common ground is the data which forms the backbone of machine learning (AI) applications <ref type="bibr" target="#b92">[93]</ref>, and the process entails the design <ref type="bibr" target="#b101">[102]</ref> and engineering <ref type="bibr" target="#b4">[5]</ref> in creating both the AI and the UX. Further, these knowledge boundaries can be bridged by either converging content and process knowledge bases through elaboration, discussion, and negotiation of dependencies across boundaries (i.e., traversing knowledge boundaries) or through knowledge transcendence by integrating just the necessary information for collaboration through co-created scaffolds (i.e., parallel representations) and dialog around scaffolds <ref type="bibr" target="#b65">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Boundary Objects:</head><p>Boundary objects <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b88">89]</ref>, such as external representations, play a critical role in bridging knowledge boundaries by supporting information sharing, interpretation, negotiation, and co-design. In collaborative design, these representations also include epistemic objects such as artifacts of design-pursuit characterized by incompleteness and technical objects including design tools that support the process of design inquiry <ref type="bibr" target="#b33">[34]</ref>. Further, when the boundaries are blurry and non-standard, material artifacts support the process of characterizing boundaries and collaboration, which are called boundary negotiation artifacts <ref type="bibr" target="#b55">[56]</ref>. These artifacts consist of (1) self-explanation artifacts for learning, recording, organizing, remembering, and reflecting, (2) inclusion artifacts for proposing new concepts, (3) compilation artifacts to coordinate and align knowledge, (4) structuring artifacts to establish principles at the boundaries, and (5) borrowing artifacts that are repurposed in unanticipated ways across communities to augment understanding <ref type="bibr" target="#b54">[55]</ref>. The eventual representation created by the differing expertise through collaboration is the artifact's specifications encapsulating the what-the artifact product itself, the how-the procedure by which it should be implemented, and the why (design rationale)-the reason why the design should be as it is <ref type="bibr" target="#b93">[94]</ref>.</p><p>In conventional software development, prototypes are commonly used as boundary objects <ref type="bibr" target="#b49">[50]</ref>. They serve to bind user needs and technical information and can include design prototyping, prototypes for verification, prototypes for exhibition, etc. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b94">95]</ref>. The need for boundary objects for AI interface design has been emphasized in recent studies <ref type="bibr" target="#b100">[101]</ref>. But as collaborations for HAI systems still lack standardization, the concept of boundary negotiation artifacts is also likely to be important and relevant. Prototypes for HAI should promote agreement in defining task specifications, communicating states of design, identifying references of central notions, and negotiating weights of criteria and constraints <ref type="bibr" target="#b93">[94]</ref>. Given the collaboration challenges described in section 2.2, we need new prototyping approaches for defining specifications that include process, content, structure, and form <ref type="bibr" target="#b57">[58]</ref>. Further, prototypes should embody a means-ends hierarchy for envisioning HAI in which each level specifies the what, the how of the level below, and the why of the level above <ref type="bibr" target="#b57">[58]</ref>. Prior work has identified characteristics of effective boundary prototypes, including interpretive flexibility, plasticity <ref type="bibr" target="#b52">[53]</ref>, and translucency <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>. These characteristics support (1) establishing a shared syntax, (2) concrete means to learn about differences and dependencies, and (3) joint knowledge transformation without causing information overload <ref type="bibr" target="#b19">[20]</ref>.</p><p>Our work studies the boundary negotiation artifacts to overcome knowledge barriers and achieve standardization across technical and non-technical roles. We further propose alternative software development workflows to accommodate the practice of boundary negotiation and blending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STUDY 1: ANALYSIS OF HAI DESIGN GUIDELINES FOR COLLABORATION RECOMMENDATIONS</head><p>To address our research questions on collaborative HAI practices, we began by determining a consensual view of recommended industry design practices for HAI. We collected HAI design guidelines from major industry sources to characterize current understanding of collaboration requirements in the field. Then, we synthesized the recommendations as a set to create a comprehensive model of HAI guidelines. This summary model serves as structure for our interviews (Study 2) with industry practitioners to organize inquiries about actual design processes used in industry projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>3.1.1 Data Collection. Various companies have offered recommendations for human-AI application design based on their internal design and development practices. Our primary sources include Microsoft's "Guidelines for Human-AI Interactions" [4, 7], Google's "People + AI Guidebook" <ref type="bibr" target="#b38">[39]</ref>, and Apple's "Human Interface Guidelines" <ref type="bibr" target="#b50">[51]</ref>. In addition, we collected recommendations published through formal and informal reports by industry practitioners, including "Human-AI Guidelines Cheat-sheet" <ref type="bibr" target="#b58">[59]</ref> and "Deloitte Insights" <ref type="bibr" target="#b2">[3]</ref>. If a guideline combined multiple recommendations in a single sentence, we split the guideline into individual recommendations. In total, we collected 280 separate design guidelines across these sources. We arrived at a final 249 after removing or combining similar guidelines.</p><p>3.1.2 Analysis. The first author conducted an affinity diagramming exercise <ref type="bibr" target="#b82">[83]</ref> to identify key topic hierarchies in the guidelines (Figure <ref type="figure" target="#fig_0">1</ref>). To create the affinity notes, each guideline was printed on paper and pasted onto a physical sticky note. By mounting blank flipchart sheets onto a wall, the first author grouped individual notes based on perceived affinity. The authors discussed the emergent hierarchies of clusters and determined that the HAI guidelines stress the goal of combining AI and UX design but do not describe (or prescribe) how designers and engineers might collaborate. Based on these clusters, we developed a component model of human-AI design guidelines (Figure <ref type="figure">2</ref>) and a set of questions for structuring the interview. Here, we summarize the guidelines and questions about individual components of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Findings: A Component-Model Representation of HAI Guidelines</head><p>As shown in Figure <ref type="figure">2</ref>, the model consists of four main components, including (1) human mental models, (2) user interface, (3) AI models, and (4) training data. As indicated by the arrows, humans (and their mental model) are tightly linked to all other components to realize human-centered design.  <ref type="formula">3</ref>) identifying the best type of AI interaction experience given the situational context; namely, the interaction model. The guidelines suggest that designers and engineers elicit these human mental models based on their application vision (or context) and develop a shared understanding for downstream AI and UX design choices. For instance, one of the guidelines about the task model recommends identifying opportunities for AI by understanding the existing task workflow: "mapping the existing workflow for accomplishing a task can be a great way to find opportunities for AI to improve the experience <ref type="bibr" target="#b38">[39]</ref>". During this need-finding process, the guidelines also recommend assessing end-user expectations about AI behavior to find the right balance between control and automation when performing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mental-Models Interface AI</head><note type="other">Data</note><p>To operationalize these guidelines, designers need to understand AI capabilities and limitations, and they need to share information about human tasks workflows with engineers. However, the guidelines do not specify how to do this: How do UX practitioners understand AI capabilities, implementation assumptions, and needs? How do they formulate expectation models with end-users? And how do they gather, synthesize, and communicate their understanding of human tasks with engineering teams? Our interview questions target these concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">User Interface:</head><p>This set of 65 user interface guidelines target the software and hardware interface between end-users and AI. The recommendations center on lowering the gulf between execution and evaluation <ref type="bibr" target="#b73">[74]</ref> by designing for (1) end-user inputs and AI outputs, (2) explainability, (3) feedback, and (4) failures and hand-offs. For example, these guidelines recommend that when presenting uncertain AI outputs, we should "prefer diverse options and, when possible, balance the accuracy of a response with the diversity of multiple options <ref type="bibr" target="#b50">[51]</ref>. " These guidelines also suggest demonstrating to end-users how to get the best results based on their inputs, conveying how end-user actions will impact future AI behavior, and providing easy ways for users to edit, refine, or recover from AI failure.</p><p>On their own, UX designers cannot realize these guidelines when making user interface choices. Implementing them requires that UX designers understand low-level implementation details about the AI model. This requires designers and engineers to collaboratively specify (or negotiate) the application programming interface (API) for AI features. Our interview questions addressed API design and the negotiation process between designers and engineers. We asked how designers understand AI failures and collaboratively design that the design should reflect information, goals, and constraints that human decision-makers weigh in decisions, avoid unwanted biases and unfair stereotypes, and evaluate the impact of AI "getting it wrong <ref type="bibr" target="#b3">[4]</ref>. " AI model guidelines mirror the human mental-models guidelines about the task and expectation model subcomponents of HAI design. Working with these guidelines requires designers to be somewhat knowledgeable about AI implementation choices.</p><p>Our interview questions thus focus on how engineers communicate about AI assumptions and implementation choices with designers.</p><p>Other guidelines recommend defining AI model performance in a human-centered way by considering human values when weighing the cost of false positives and negatives, ensuring that model metrics such as accuracy are appropriate to the context and goals of the overall system, and making conscious trade-offs between precision and recall. For instance, guidelines recommend that "while all errors are equal to an ML system, not all errors are equal to all people. You will need to make conscious trade-offs between the precision and recall of the system <ref type="bibr" target="#b38">[39]</ref>. " Similarly, guidelines about evaluating AI features recommend assessing whether model objectives provide a good experience for all users, assessing for safety and whether the AI design performs under the "realities of the environment in which it to be used." We included interview questions to uncover how designers and engineers work together to define model performance metrics and how they evaluate model behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training Data:</head><p>According to these 34 guidelines, data needs for training the AI model should reflect human data needs for their tasks. This includes (1) planning data sources, (2) data collection, (3) labeling data, and (4) privacy, security, and ethics of data. For instance, when planning data needs, guidelines recommend aligning them with the task model by asking what information a human will use to perform the task on their own <ref type="bibr" target="#b38">[39]</ref>. For data collection, the guidelines include (1) responsibly sourcing the data, (2) planning data collection to be representative of expected end-users, use cases, and context of use, (3) formatting data in ways that make sense to human users, and (4) collecting only the most essential information from end-users. For labeling data, these guidelines focus on using proper labels; i.e., data labels must reflect the people's diversity and cultural context.</p><p>Implementing these guidelines again requires that designers understand the AI's data needs and the types of computation that AI engineers will apply to the data. Further, they need to work with engineers to define human-friendly data labels, plan data collection, and mitigate problematic biases in the dataset. Our interview questions thus target how teams collaboratively scope data needs based on AI model needs, human task, and expectation models.</p><p>In summary, existing HAI guidelines focus on 'what' needs to be done, but they make no recommendations about 'how' specific design and engineering processes (user research, data collection, model development, interface evaluation, etc.) serve to align AI development with human-centered design. Nor do they recommend how designers and engineers can bridge their respective knowledge boundaries to acquire a shared understanding to collaborate on HAI design. To answer these questions, we turned to practicing AI and UX professionals in industry to ask about their current processes for HAI design. We structured an initial set of questions using the component model created from affinity clusters and included the concerns highlighted above. To specify the question content, we identified the key nouns (e.g., 'data', 'human-needs') and verbs (e.g., 'collect', 'align') from the guidelines within each cluster. We then translated them into questions about who implemented the guidelines, and how they did so. For instance, we ask teams about who is involved in collecting data for the AI and how they defined representative data collection needs. As a second example, we ask who is involved in envisioning the AI behavior and how they incorporate human needs into their design. The complete set of interview questions in available in the supplemental material. Our interview study with HAI designers and engineers working in industry aims to identify how they implement these design concerns in their collaborative practice on the job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STUDY 2: INTERVIEW WITH HAI PRACTITIONERS 4.1 Method</head><p>4.1.1 Procedure: We conducted interviews with 21 industry professionals from 14 different organizations of differing sizes (see Table <ref type="table" target="#tab_2">1</ref>). Each participant was interviewed separately (i.e., we conducted 21 interviews in total). We recruited individuals involved in building AI components for user-facing products; mainly, UX professionals and AI engineers, data and research scientists, and managers. Starting with university alumni and other industry connections, we used snowball sampling to recruit participants through referrals. They had between one to 12 years of professional HAI experience, with 13 having at least three years and an average of 3.7 years (SD=2.5 years) (Table <ref type="table" target="#tab_2">1</ref>). Participants were not compensated for participation, but could opt-in to receive a small gift of university merchandise. Before the interview, participants completed a consent form, and in many cases, also sought approval from their company's legal team. The first and second authors conducted all interviews through video-conferencing, with each interview lasting about 60-minutes.</p><p>In these semi-structured interviews, we started by asking about the participant's role within their company and their team affiliation and organizational structure. We then asked them to choose and describe an AI-based application they helped create. We asked participants to walk us through how they participated in the process of creating the application (as allowed by disclosure rules). Based on their description, we used follow-up questions based on our component model about whether (and how) they operationalized different guidelines, different roles involved in the process, and their workflows for collaborating with others. For example, we asked designers how they learned about potential AI errors and asked engineers how they obtained requirements for the particular feature they built. We also inquired about conflicts during the collaboration and how they were resolved. Participants were probed about the kinds of tools they used (e.g., in-house tools versus outsourced ones), whether and how they referenced existing Human-AI guidelines, and if there are any tools they wished they had. Later in the interview, we inquired about their use of prototypes. Other questions addressed perceived differences and similarities between AI-driven and non-AI applications. Finally, we asked participants for their feedback about Human-AI design guidelines and ideal workflows for collaboratively building AI-based applications. The interview questions are available in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Analysis:</head><p>We contracted a third-party service to transcribe the interview with the exception of one where the team manually transcribed at the participant's request. We then conducted qualitative coding analyses using a grounded theory approach <ref type="bibr" target="#b90">[91]</ref> starting with an initial review of the interview notes and an in-vivo analysis. Two authors independently open-coded five transcripts and then collaborated to develop an initial codebook, resolving disagreements by consensus. The resulting codebook consists of 40 top level codes including the use of prototypes and artifacts, multiple workflows, friction or tension in collaboration, differences between AI-driven apps and conventional software, and tools used for communication and collaboration. The complete set of codes is available in the supplementary materials. The two authors then individually analyzed the remaining transcripts using this codebook <ref type="bibr" target="#b28">[29]</ref>. Because we used a grounded theory approach, we did not see a strong need to compute coder reliability <ref type="bibr" target="#b68">[69]</ref>. A memoing activity synthesized findings across transcripts <ref type="bibr" target="#b16">[17]</ref> focusing on how collaborative teams develop human-AI applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Findings</head><p>Our interviews reveal how team structures and separation of concerns (boundaries) between differing roles and expertise hinder human-centered AI design. Several of our participants reported a separation between individuals who conceptualize AI capabilities and those who integrate those AI capabilities within end-user products. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, many large organizations have dedicated AI research teams (primarily computer scientists) who explore novel AI capabilities and techniques. In these teams, the focus is on advancing foundational research in AI. Product teams are not typically involved in this process, and the technology itself may be only partially motivated by real end-user needs. However, once the technology vision is achieved, research teams join in with different product teams to identify product use cases for applying AI innovations (i.e., an AI-first workflow).</p><p>To support the research-to-product pipeline, as reported by three participants, large organizations may have intermediary technology transfer teams that envision product and human uses for AI innovations. On the other hand, smaller organizations and start-ups may rely on third-party AI providers (e.g., Microsoft Azure AI <ref type="bibr" target="#b69">[70]</ref>) to add new AI capabilities into product features. Outside of core research and product teams, AI development commonly requires support from domain experts and data annotators. These teams tend to be external to the organization. Further, according to two participants, teams consult with legal representatives about ethical data collection and data privacy issues. Both large and small organizations may have a pool of beta-testers available to evaluate new features during development. Collectively these team boundaries introduce numerous challenges to operationalizing the HAI design guidelines. We summarize our findings in terms of (1) limitations due to separation of concerns at the design-engineering boundaries, (2) design workflow challenges from data centric nature of AI development, and (3) current workarounds to alleviate collaboration difficulties at the boundaries.   Boundaries Introduce Knowledge Blindness about End-Users and AI Capabilities. HAI guidelines recommend that the AI capabilities should be motivated by human needs and should align with human behavior, task workflows, and cognitive processes (see Figure <ref type="figure">2</ref>). However, the boundaries between core AI developers and UX designers limit possibilities for creating human-centered AI from the ground up. Given the novelty of AI, researchers and engineers are motivated (and incentivized) to explore AI capabilities independently and without regard to products and end-user needs. As manager M3 described: ". . . research coming up with new cutting edge state-of-the-art techniques for doing something that the product team wasn't even thinking about, or users aren't asking for, because they hadn't thought that way." This boundary separates core AI developers from end-user product teams and introduces end-user blindness about product users' needs and concerns. The result is often erroneous assumptions about what users would want from AI. In describing their frustration due to end-user blindness, manager M1 commented: "You have these requirements where you need these videos to be analyzed, and tagged, and categorized. . . a lot of times people [AI engineers] would go off in just weird directions, get obsessed with trying to identify guns, or something that wasn't really that important to what we were doing" -[M1]</p><p>On the other hand, product teams-specifically UX designers who advocate for end-users in design specifications-may lack an understanding of AI technology capabilities, i.e., AI technology blindness. As a result, UX designers appear to either distrust or over-rely on AI capabilities, which manifests in their UX design for AI. As research scientist R2 puts it, designers tend not to automate things that they could be automating: "There's under trusting where it's like oh actually you should let the algorithm make a suggestion, maybe offer a choice, maybe you should trust it more than you do." R2 further adds that in other cases, there is over trust on what AI can and cannot do: ". . . then other times, especially when you get into the cases around anthropomorphism and things like that, people really overshoot their trust and think yeah this is going to be great no matter what happens. . . " A consequence of the blackbox approach to design is that designers themselves lack clarity about the AI performance and output. This makes it challenging to design user experiences that align with end-user mental models and expectations. In advocating for overcoming technology blindness in UX design, M2 commented on designers needing to understand the capabilities and limitations of AI:</p><p>"It used to be that UX designers made static mocks and there was screen-to-screen flow. But with AI the probability that the end user makes it through that path is lower because it is driven by a sequence of machine learning models. Just the probability curve on any journey is now much more complicated and is much more branched. Designers need to understand probabilities. They need to understand the failure cases, understand confidence and how to deal with confidence scores, how to interpret thresholds. They need to be able to understand the grain of the technology. . . the possibilities and the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[M2]</head><p>Traditional Software Practices at Boundaries Impose Premature Specification of AI and UX Designs. In conventional software design, UX professionals work with end-users to define the requirements for the application. However, because of knowledge blindness, on their own, designers and engineers may not be able to fully define specifications for either the AI model or the user experience and interface. Yet, our interviews identified the tendency to define AI and UX specifications independently because of the work-role boundaries and lack of coordination practices. This problem takes control away from designers attempting to craft the end-user's experience. Across many interviews, designers expressed frustration in trying to design the UX around an independently created AI specification. For instance, in one of the sessions, the AI team developed a nuanced tagging schema for media content and handed it off to the UX designer to integrate into a voice assistant. The designer (U1) commented on the challenge in integrating UX around predetermined AI specifications, noting the extensive rework and feedback required to retrain the AI tagging model in a way that meets their understanding of end-user needs: "In the first meeting, [the AI team] were just saying 'We need to add this [categorization] on the screen, can you find where is the right place?' Then I need to work a little bit backward to say there are constraints on wordings, and voice UI had another layer of constraints, so I need to understand how this information fits into the screen we already have. They have their own taxonomy on what kind of information they are looking for, but for users, it doesn't evoke a response . . . The [label] semantics is not on the same level to what we already have. "</p><formula xml:id="formula_0">-[U1]</formula><p>Similarly, AI engineers also found it challenging to implement desired AI features when the UX is already specified in great detail without AI involvement. AI models (unlike conventional applications) are challenging to build to specifications because their behavior is dynamic. This makes it difficult for engineers to independently create AI technical specifications from design requirements alone. S7, an AI engineer, commented about their frustration in the coordination and hand-off process between UX design and engineering: ". . . they [UX] would hand that [design document] off to the engineer and say 'Implement this. ' And of course my reaction to this was 'This is garbage. ' This does not reflect the appropriate architecture for implementing this thing. It felt particularly extraneous when it got very granular, and it was not the best medium for describing the desired behavior. Because the designers were not technical really. This is not a good reflection of how the actual AI software engineering is going to happen. And I was like, 'Stop trying to do my job for me. "' -[S7]</p><p>The problem of AI blindness among designers arises from the role boundary created by professional expertise. By advancing UX design independently from AI teams, UX features become "set adrift" from the other source of constraints for end user's needs-the AI model.</p><p>Boundaries Limit Access for AI and UX Collaboration. Because of differences in the design and engineering processes, there is no clear understanding of how human-centered design requires alignment across both tasks. For instance, U6 expressed concerns that the UX team was not involved in the training data annotation process-the core of how end-users experience the AI. According to U6: "it seemed very odd to me that as designers we were not invited to the annotation session. So, we had to invite ourselves to just talk to domain experts. . . " U6 further commented that ". . . for engineers, their approach is more like 'the machine is going to figure it out. ' We could be talking about health or elephants in the circus, and it is all the same to them . . . "</p><p>Across the interviews, other collaboration challenges also emerged. First, the core responsibilities for UX professionals are defined differently from basic AI research. In addition, the time needed to conduct user research was viewed as out of sync with AI research progress. For instance, U4 comments that "we don't necessarily participate as much in that whole AI thing, but the thing is because we're also trying to make sure that we're doing user research and participating in that. " S4, a research engineer in a different organization, offers their perspective on collaboration: ". . . they [UX] might complain after the fact that they weren't early enough, but on the flip side if we try to involve early then they'll say they're busy doing x, y, and z. In my experience, it's not always practical." Acknowledging the added time it takes to conduct user research, M3 comments: "You obviously need a human need, or you're not going to have anything worthwhile, but the reality is in most of these companies there are in-flight research efforts that are happening on basic capabilities, and it's not like you can say, 'Okay, everybody stop what you're doing until I find you a human need, and then you may start working again. ' It's just kind of absurdity. "</p><p>Further, in smaller organizations that work with third-party AI services, boundaries severely challenge the design of AI behavior and presentation for end-users. In working with third-party AI, M1 describes that designers often have to engage in the laborious activity of translating AI output into user-friendly labels and formats: ". . . the label that the database has for the data may not be the same as what your end-user understands it to be. So, understanding there's a difference between how an engineer labeled it in the database, versus how you might want to show it on your UI to the end user. . . we would at the raw JSON files and create our own labels . . . " The disconnect between external AI services and products forced designers to alter the AI model specifications to avoid AI experience issues for end-users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">AI's Data Needs Transform Designers' Role in HAI Development.</head><p>In conventional UX workflows, designers synthesize higherorder system requirements from end-user data. However, in HAI workflows, individual data points (i.e., examples) are essential requirements for AI development. Our interviews revealed constraints to human-centered data requirements and designers' involvement in AI development pipelines.</p><p>From Developer-Centered to User-Centered Data Requirements. In contradiction to guideline recommendations, in AI-first workflows, technology requirements appear to drive data requirements. When exploring new AI capabilities, researchers don't always know what types of data might be needed. Requirements about data and its characteristics, such as variables, data types, labels, and the number of data points, evolve through a "trial and error" approach. As reported by three participants, this workflow aims to optimize the AI development process. That is, researchers start with an initial, small dataset to train the model. For early-stage data collection, organizations may have internal data collection and logging apps (e.g., one that collects gesture data while using the phone) that can be deployed across different teams. There is often an "unwritten agreement [S5]" that development teams will provide data for AI development purposes. This lowers the cost of data access. As S5 comments: ". . . you have to spin up a dedicated user study program, go through a lot of process, a lot of review, it's a whole lot of bureaucracy to get that kind of rich data collection." Therefore, AI researchers work with pre-existing data sets or 'minimum-viable-data' collected from within their UX team and then gradually increase scope of data over time:</p><p>"For collecting data, we will start from people within our team and do a first pilot testing [of the AI]. If it works well, we will increase the size of the data. For example, we will recruit people from a different project team so they are not biased to our data needs. And if it continues to work well but we still need more data, we will start looking for external partnerships to collect from our target users. " -[S1] In this workflow, engineers also reported striving for "clean" data by removing outliers and noise to improve model performance. This may lead to an idealistic version of data for AI model exploration that omits data characteristics and features relevant to real-world use.</p><p>Once the AI capability and data specifications are determined, UX researchers get involved in validating data needs with end-users or collect additional data to test for AI robustness. For instance, UX teams may work with end-users and customers to validate data labels for the application design. As M2 comments: "if you want a certain data structure with a hundred hypothetical labels, you can show that to users and get sentiment on that. . . " Further, UX designers commented that such a partnership requires careful consideration about privacy and content ownership, as well as communication about benefits to customers. AI engineers also emphasized the need for clear communication about how customers (who are assisting with data labels) might benefit from their contributed data. Because of the way user inputs are elicited, S6 commented on end-users being hesitant to provide information for labeling tasks:</p><p>"We asked customers [to label the data], but it wasn't good enough for our use. Anecdotally, I think the people who are being asked to label weren't sure how this information is going to be used. I think there was some hesitation because it wasn't tied to their day to day metrics or goals. I don't know if there was an element of fear of automation. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[S6]</head><p>As a consequence of AI model needs, end-user data collection appears to occur more incrementally and less formally than in conventional applications. Further, this alters how designers conduct user research to now include the AI development pipeline.</p><p>Designing Data Collection Tools with People in Mind. Given the significance (and multiple roles) of data in HAI design, data collection and annotation tools are essential for gathering end-user requirements. Consequently, engineers develop custom tools for collecting needed data. According to S1: "A lot of times, our problem is not generalizable, so we build our own tools in-house." Such tools are often optimized to lower the engineering cost for data cleaning and transformation. For instance, the data collection tool may explicitly ask participants to start a session and perform some task or prompt participants to validate whether or not the correct label was detected (e.g., labeling sensor-based activity detection). Both designers and engineers acknowledged that labeling could be tedious work. They expressed empathy for people charged with labeling the data (e.g., "there are overseas sweatshops where people are just filling in Mechanical Turk surveys day in and day out, figuring out whether the image has a dog. . . with all the empathy in the world you have, you feel really bad for those people" [S2]). In one interview, the designer reported visiting those performing labeling on-site to understand their pain points and run user studies with them to evaluate data annotation tools.</p><p>"We wanted annotators to create object segmentation boundaries on images by drawing polygons. To design the tool, I visited [location] and asked the annotators to generate labels. From these trial runs, we noticed that using the keyboard was essential for a good UX, and they needed ways to undo or edit polygons. Based on this, we did a focus to know how we can improve the labeling tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[S3]</head><p>This example illustrates the change in the nature of user research data, how it is collected to design HAI systems, and how it is used. Designers are learning to provide new types of UX support driven by AI model development. Their objective is to optimize the user experience for end-users but also lower engineering effort in data usage.</p><p>Authentic Data for HAI Evaluation. As with the technical evaluation of AI models using a "holdout" dataset, in many instances, human-centered evaluation requires that designers adapt evaluation practices for end-users to supply their own data based on their personal experience history in a domain. This allows endusers to provide feedback about AI behavior from their viewpoint as experienced within their own situated contexts. As R2 puts it: "The best mock for AI is a lot of times human. We really try to use people's own content. This is the thing; if I look at photos of my friends and family, I'm going to have an emotional reaction, I'm going to have an authentic experience there. " AI model design requires continued evaluation and feedback from diverse end-users with personal experiences in a task domain. However, constant engagement with end-users (ranging from novice to domain expert) within existing design and development workflows is challenging for UX designers to accommodate. In describing this challenge, S5 comments:"User studies, especially things of this nature, like, getting around a lot of our privacy constraints tend to be difficult, which that's a whole another like, can of worms you probably don't need to attack right now." S5 points out that evaluation, especially for recommendation systems, requires access to user data and requires time-consuming review for privacy compliance.</p><p>In addition, teams find it challenging to develop the right metrics to gather feedback on AI experience design. According to D3 "To me, evaluation is still very, very hard. And especially I think maybe more subjective evaluation too in terms of the quality or how enjoyable was the experience?. . . if you were using the measure of how many items you interacted with or how long you engaged, it would feel like the one that was a five-item engagement was more successful than the two-item engagement, where actually they [end-user] didn't really think that at all. " (D3). A lack of well-tested metrics makes it hard to run deployment studies to gauge end-user expectations and trust. These challenges are amplified in evaluating AI behavior over time, especially for learnability through end-user feedback. Addressing these issues will require designers and engineers working together to identify appropriate performance metrics and privacy-preserving evaluation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Bridging Boundaries Through Collaborative Design and Constant Co-Evaluation.</head><p>In responding to the expertise boundary and data role chalparticipants revealed how they reduced friction to facilitate engagement across teams. These workarounds involved a variety of boundary negotiation artifacts <ref type="bibr" target="#b55">[56]</ref> for (1) knowledge sharing, (2) collaborative prototyping and design negotiation, and (3) design evaluation and feedback.</p><p>Bridging Knowledge Boundaries between Designers and Engineers. In conventional software workflows, UX designers rarely share raw end-user data and low-fidelity representations with engineers. However, the interviews revealed that sharing lowfidelity artifacts is effective in centering the end-user within AI model design. For instance, UX designers reported sharing raw user data and co-creating user personas with engineers to help them think about training data needs. While it demands a more extensive data collection program, collaborative synthesis generates requirements for different data collection tools, of end-users to recruit, storing and processing data, and collecting data preserving privacy and ethical concerns. In describing their approach to ensuring the representativeness of different end-user groups in collected data, U5 comments: "Often, I look out into the world to see what information is there about existing groups, and then evaluate for myself, do these groups make sense or do I need to make new groups. I have done all of the user research and come up with groupings on my own and then brought them back to the team. Then I talk it through with the PM and the engineers what the value of different user segments are, why would we want to prioritize the different users, why are they important to the company. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[U5]</head><p>With HAI, the task of anticipating relevant differences in enduser populations impacts not only the UX design but also the behavior of the resulting AI model through training. Another change in UX designers' work for HAI occurs when designing interaction or task workflows. AI engineer S7 reported that designers sharing sketches and storyboards (instead of high-fidelity prototypes) offered flexibility and control in mapping user needs to AI features and implementation logic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. " -[S7]</head><p>Similarly, engineers reported varied strategies for sharing AI capabilities and details about implementation (such as assumptions and logic) with UX designers and domain experts on projects. Again, the intent is to resolve technology blindness and to facilitate collaborative design and feedback. As S6 comments: "If we don't adequately communicate to designers, they fill in the gaps with their own theories and its not clear what input needs to be provided in order to get the desired results." In one scenario, AI researchers reported working with university interns to develop a conceptual prototype of an AI feature. Here the goal was to (1) demonstrate a new capability of AI within an application context and (2) define a design space for UX researchers to think about the experience. As S5 describes: "We got something tangible enough that we could actually go talk to a designer and. . . we started letting them play around with it, and said, 'Try it out for a week and tell us is this better than the old way that we've done things. '. . . it also broke the problem down such that the designers understand, here's the benefits of where the machine learning can be applied. " Once UX designers understand the AI design space, they are able to collaborate with researchers to explore end-user needs, using the prototype as a design probe.</p><p>In other cases, AI researchers may identify a new technical capability but find it hard to define its use context. In such cases, UX researchers need first to understand the technology and then identify its benefits for potential end-user experiences using prototyping approaches (as suggested earlier). As M2 described:</p><p>"A lot of times, people are, just kind of, down in the weeds, really deep and get a little lost in the day-today work. UX teams can actually bring a little hope to those folks and give people a target, and really paint a picture of that through design visualization, whether that's making a movie or just making a series of mocks, or building an experiential prototype, or something like that, really help land the tangibility of something that's pretty deep and complex. Sometimes, it's the light at the end of the tunnel. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[M2]</head><p>In this regard, two participants, both project managers, emphasized the value of UX-friendly machine learning tools for creating experience prototypes. Specifically, these tools allow designers to take "off the shelf" ML models or plug in their own team's model and work with real end-user data to demonstrate an envisioned AI feature. In addition, using functional ML models mitigates the danger of setting or communicating unrealistic expectations with AI mock-ups.</p><p>Collaborative Design of HAI Prototypes. By bridging expertise boundaries, designers and engineers reported working towards collaborative prototyping, including data and labels, AI model behavior, implementation, and end-user experiences. For instance, HAI guidelines recommend defining data labels and annotations by consulting with expert users. In the interviews, UX designers and engineers identified multiple ways to work with domain experts to co-design labels. For example, in one interview, data scientist D3 reported that they generated database queries for exposing different views of training data requiring labeling. Engineers then define ML constraints for labels, and UX designers and domain experts generate and validate labeling schemas (i.e., rules for assigning labels to raw data). A second example occurred when data scientists find pre-existing datasets they need to re-purpose for their AI needs. In this workflow, data scientists work with domain experts to clean data, identify variables for prediction, interpret data analysis results, and perform labeling. As D2 describes: "we would be talking to meteorologists about how to adjust variables, and create flag variables, so if it is above this temperature or dew point, we would categorize it. . . " This collaborative process happens through sharing CSV files, Python scripts, and visualizations. Further, creating experience prototypes combining AI capabilities and UX needs requires close collaboration between designers and engineers. In the case of Wizard-of-Oz prototypes, UX designers gather end-user data and work with engineers to generate outputs and understand the logic behind them. This is essential to understand the unanswered questions from an engineering standpoint, plan the type of user study needed, and design the presented experience of the prototype for end-users. As U3 describes: "Let us say I am doing food recommendations. And I want to tell users why something was recommended. It may be because they are liking a few restaurants, or they added items to their shopping cart, or maybe it is because of past orders. It is a Wizard-of-Oz prototype where I first get users' data. Then I get the model output from the data scientist and work with them to understand the model labels and explanations. The data scientist wrote down all the equations and explained it to me very clearly. They showed me how the weights were set, and we discussed things we need to know from users, whether to do an A/B testing or a walkthrough. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[U3]</head><p>Engineers also support UI designers through annotations on UI wireframes about what is happening behind the scene. According to S6: "I added annotations on the side about what is happening behind the scenes like an API is being called. Then as an example, I would [annotate] for the API what output it comes back with. . . I use Balsamiq [UI prototyping tool <ref type="bibr" target="#b11">[12]</ref>] because I think it lowers the barrier of what can be a design tool, and you don't need specialized knowledge to communicate that idea." These comments by developers indicate efforts to support greater collaboration and extension of expertise across boundaries.</p><p>Design Iteration with Constant Evaluation. Lastly, the interviews revealed that evaluation happens frequently using unfinished prototypes still under development. Participants reported that this form of assessment is necessary when the user experience design is co-evolving with AI development: "I think the process that works best is fairly tight review cycles with the actual evolving behavioral artifact. " (S7).</p><p>Early stage evaluation of model behavior. In the early stages of development, engineers may make certain assumptions about AI behavior. Frequent evaluation allows UX researchers to provide early feedback about these assumptions. As S7 describes: ". . . as I was implementing this feature and I ran into this problem of how to handle this use case? . . . Here is the guess that I made, but let us talk about whether that was the right choice. As things were getting built, we would look at the running prototypes and be like, 'Do we like how this plays?'. . . " Here, S7 describes how this approach is more suitable for AI development compared to having a black box prototype provided by the UX designer. Similarly, for AI perceptual interactions (e.g., computer vision), UX designers may supply an initial set of desired interaction gestures. Then, during development, designers and engineers evaluate the feasibility of those interaction gestures using prototypes and discuss alternatives. S1 explains that:</p><p>"The designer will say 'we want ten different facial expressions for this model'. . . we start from there to build the backbone of the interaction, and then we iterate through it. . . we call like grayboxing. . . there are three facial expressions where it's just really hard to get that right, it is not going to perform very well. The other seven is fine. So, in the process of testing, we find out, there are two other facial expressions that are not in the original ten expressions that can perform pretty well. And so we will tell the designer that these three we will need to cut it. But if you want, there are two more gestures, you can add into your interaction. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[S1]</head><p>In a different scenario, evaluating early prototypes with target users helped engineers determine the optimal algorithm for a problem (user need) they are trying to address. In describing the iterative process of model comparison to find the best approach, S2 explains that: ". . . the process involved 20 different prototypes I had to build for all the different algorithms we've tested on." Further, they describe that the prototypes expose the actual model logic using visualizations for test users to evaluate: ". . . user uploads an image, and I visualized the palette for the image so that we know how the algorithm is working under the hood. Because AI is a black box, we need to have some transparency for the user to understand. You show the palette. Once you have the palette, it will search and return the results. For each result, I also show the pre-indexed pallets we use to compare with other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" -[S2]</head><p>This allowed the UX designer to do a comparative evaluation iteratively: "Every week when we have a new algorithm, we compare it to the existing best and see which one is still the winner and then that will compete with the next algorithm. . . . That is how we reach to find the one which we shipped to productions. "</p><p>Iteration with domain experts to determine AI behavior and interpretability. In other cases, the data scientist may provide domain experts with a spreadsheet containing rules and assumptions made in building the AI model. The expert then annotates changes to the rules for updates of the model. According to D1: "There are rules and codes we have that we use for making recommendations. We would list out the rules so the domain experts could look at them. Then, we started to give them more accessible tools like sharing a spreadsheet where they could give their inputs. . . They could flag, add notes and annotations [about model output]. " This process allows domain experts to participate in specifying AI behavior at a conceptual level. Similarly, designers worked with engineers and domain experts to evaluate interpretability features by creating functional prototypes with different output formats. For example, U2 discussed their process for showing output probability to end-users, working with domain experts to translate percentages into categorical bins, such as "high", "medium", and "low".</p><p>"There are two versions we iterated. The first one is to show the possibility as numbers. If I have ten patients and nine of them have 100 percent, and only one shows 20 percent, it might confuse a user because a number is really hard for [end-users] to understand. . . The second version we actually tried was high, medium, low possibility. So that turns out to be more positive by the user. "</p><formula xml:id="formula_1">-[U2]</formula><p>Evaluating data and model for privacy and ethical concerns. To evaluate privacy and ethics concerns during data collection, AI engineers often collaborate with legal team members. Many interviewees described this as a collaborative process where engineers walk through what data is collected and why. Then, they discuss alternate data sources in case of privacy violations and how to collect data in a privacy-preserving way. As described by S5: "all data collection has to go through a privacy review . . . you sit down with one of them, you walk them through, here is the data we want to collect, here is why we want to collect it. Then, they discuss about is all this data necessary, can we do different ways to interpret it?" This process involves sharing compliance documents and details about model implementation and data, and a legal team may draft a privacy statement for end-users to review.</p><p>Evaluation in the wild. When a fully functional prototype is available, UX researchers may conduct deployment studies with test users to evaluate how the model performs in the real world. M2 describes this process as "Anybody can download [the] app and try it out, That's how we collect data a lot. . . it's very easy for a UX researcher to go back and say, 'We see this fail for this use case,' or 'for this population,' and just go back to the team and it's an open conversation about the limitations of the current model and how to adapt. . . ." Further, UX researchers may conduct a longitudinal evaluation with functional prototypes. According to U5: ". . . doing longitudinal research is really helpful . . . if it is something that takes a bit of ramp-up time, giving the people you are testing with time to spend with it, to see where it lands and how useful it is over that time." In communicating to users about longitudinal testing, U5 comments that: "I think some of it is just product transparency, it would make sense for me to just be like, 'Right now we don't know anything about you, but come back as you use this app over the next couple of weeks. We will start to produce better recommendations for you.' So keep checking back because otherwise, I think you might make assumptions that it will never work or things like that. So I think transparency can be really helpful in those situations. " -[U5] One challenge with this iterative process is communicating with designers and end-users about what is implemented (and what is not) and what type of feedback they need to provide. Identifying primary functions to test, and why, along with which functions are missing and why they don't matter at this moment, requires UX designers to have a high-level understanding of the AI development process. As S7 puts it: "I think that part of being in a nontechnical role is understanding enough about development. So you need to tell them 'Listen, what we are showing you today is two weeks of work. Here are the things that it doesn't have but it will have. We don't need feedback on the fact that it doesn't have sound effects or graphics. What we need feedback on is, is this the basic kind of interaction you want? Does this look like something that is going to solve the problem? Trust us. We will get back to polishing it, that is not what we are looking at at this stage. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. " -[S7]</head><p>In summary, we find that teams overcome collaboration challenges in HAI design by disregarding conventional software separationof-concerns to create and share low-level design and implementation details across knowledge boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Our findings show that SoC introduces numerous challenges to HAI software development. First, we find that delayed specifications in software workflows is integral for operationalizing the HAI guidelines. As Yang et al. <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b101">102]</ref> note, UX professionals lack familiarity with AI capabilities and the means to design AI components. This leaves key AI specifications (such as feature selection and model assumptions) to those with technical expertise (echoing Zhang et al. <ref type="bibr" target="#b103">[104]</ref>). Our study especially builds on prior work by finding that AI specifications are often made prematurely, necessitating difficult and expensive changes when user experience concerns are later identified. Changing workflows to postpone technical commitments may facilitate the continued integration of concerns throughout the design process. Next, we report taskspecific creative workarounds introduced by both designers and engineers to overcome knowledge blindness and support collaborative HAI design. Importantly, we concretely show how HAI teams can achieve multidisciplinary collaboration. These workarounds contradict established software development practices dictating abstraction, information hiding, and modular design in SoC. Here, we further characterize these workarounds as leaky abstractions intended to share key information across concerns. Leaky abstractions appear to help designers and engineers (1) coordinate and implementation details, (2) collaborate on designing both the AI and UX, and (3) integrate concerns across disciplinary boundaries to develop HAI systems. Building on leaky abstractions, we theorize towards a collaborative design workflow through delayed specifications and constant evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Leaky Abstractions Support Collaborative HAI Design</head><p>In collaborative software design, the purpose of abstractions is "not to be vague, " but instead to "create a new semantic level in which one can be absolutely precise" <ref type="bibr" target="#b30">[31]</ref>. However as our findings show, existing abstractions that are effective for conventional software development, hinder collaboration in HAI system design. Moreover, given the recency of HAI guidelines and its novelty for software practitioners <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b101">102]</ref>, our understanding of abstractions for designengineering collaboration is still evolving. Our definition of "leaky abstraction" as -ad hoc representations shared across expertise boundaries to expose low-level design and implementation detailscaptures this evolving nature of understanding over time. Through a related characterization of representations as technical objects and epistemic objects <ref type="bibr" target="#b33">[34]</ref>, leaky abstractions emphasize the importance of incomplete and constantly changing design knowledge during HAI software development. Leaky abstractions also encompass a wide variety of inter-designer representations <ref type="bibr" target="#b93">[94]</ref> for collaborative design. We observed instances where teams created different leaky abstractions to address explainability, error handling, feedback, and learnability. As Yang's description of "designerly abstractions and exemplars <ref type="bibr" target="#b100">[101]</ref>" suggests, leaky abstractions may be needed to address a wide variety of cases where lower level detail is needed to inform and support interface and interaction components. In our interviews, leaky abstractions took many different forms and addressed many different elements of user interface and AI model design.</p><p>As reported in our findings, designers shared low-level design details with AI engineers to shape AI around end-user needs (see Figure <ref type="figure" target="#fig_5">4</ref>). First, contrary to conventional wisdom, designers shared details about personas and user segments that emerged from surveys, qualitative code-books for training-data labeling, and raw end-user data (gathered through UX research processes) to inform representativeness and formatting needs for AI's training data. Second, designers shared 'examples' of desired human-AI interactions through low-fidelity artifacts such as storyboards, prototype interfaces for task workflows, spreadsheets with ground truth data, and even interaction logs from existing non-AI software use. These artifacts are often ad hoc inventions intended to communicate with engineers about needed AI behavior. Third, given the challenges in articulating and reporting feedback about AI from end-users, designers share raw feedback from user testing through videos and direct observational notes and invite engineers to participate in end-user evaluation sessions. These new collaborative practices characterize the nature of leaky abstractions about designing AI components with end-users in mind. Finally, designers also offered technical representations such as qualitative code-books and epistemic design objects (including storyboards and prototypes) as shared representations for AI and UX specifications. Through these leaky abstractions, designers cross design-engineering boundaries to provide input about model behavior and training data. These design artifacts help engineers situate AI decisions within the broader context of human needs in HAI design.</p><p>Similarly, engineers reported numerous leaky abstractions and novel collaboration practices to uncover AI implementation details for designers. As shown in Figure <ref type="figure" target="#fig_6">5</ref>, leaky abstractions allow engineers to (1) communicate about needed training data characteristics for user interface design, (2) communicate model behavior for user experience design, and (3) evaluate the AI with end-users. For instance, engineers assisted designers in exploring training data characteristics by creating and sharing computational notebooks with ready-to-run data queries and data specification documents. Access to these details supports designers in determining appropriate interface controls and presentation features, such as formatting and categorizing AI outputs. When prototyping ML models, engineers create envisioning prototypes to demonstrate capabilities and potential uses to designers. In other cases, they work with design teams to 'align' model logic with interface designs by directly annotating over UI wireframes. Lastly, leaky abstractions showing AI logic, including interpretable visualizations, spreadsheets with model rules, and controls for tuning model parameters, allow designers to validate AI behavior with end-users and provide feedback on detailed AI implementations.</p><p>Each instance of leaky abstractions in the Figures <ref type="figure" target="#fig_6">4 and 5</ref> may be difficult to anticipate, and may not be needed in a different project addressing a similar issue. This raises the question of whether standardized abstraction tools may be helpful, or if support is more helpfully aimed at training practitioners to invent their own leaky  abstractions as needed. While a "ready repository" <ref type="bibr" target="#b100">[101]</ref> of abstractions may solve a number of HAI challenges, useful generalizations and standard practices for abstractions may require more time to mature. This is especially true given the dynamic nature of AI tasks. In this regard, our definition of leaky abstractions is ingrained more specifically in software development and aimed toward shifting practitioners' mindsets towards integration rather than separation of concerns. Leaky abstractions appear useful in engaging designers in thinking about technical requirements, and in drawing engineers into thinking about how and what types of human data may improve system performance. In this sense, leaky abstractions may serve as a "lingua franca" to allow mutual consideration and problem solving to fuse human needs and AI capabilities in a design. From the findings collected, it is unclear whether there is substantial overlap in the problems arising and in the utility of specific leaky Over the longer term, it is possible that a consensual set of useful representations and training may be helpful in supporting practitioners in this process.  Finally, leaky abstractions may appear be similar to "boundary negotiation artifacts" used to form collaborative practices in situations where teams lack well-established standards <ref type="bibr" target="#b55">[56]</ref>. In HAI, leaky artifacts can be seen as functioning in a similar manner by allowing designers and engineers to mutually alter the implementation hierarchy-covering the product's function, specific implementation logic, and aggregation (part-whole) hierarchy-by representing how each component fits within the design. However, leaky abstractions differ from boundary negotiation artifacts in several important ways. First, they appear in our findings within collaborations where a well-established standard of practice (the conventional software SoC) already exists. There is no need to create boundaries because they are already well-known and practiced in software design; instead, leaky abstractions emerge when the established boundaries fail to support the needs of the design task. Second, the leaky artifacts observed emerge in response to a question or problem within a given design task; that is, the ad hoc nature of the representations suggest they are perceived to be useful in a specific collaboration and stage in the process. Finally, in a boundary negotiation, the desired result is a specification of how a separation of concerns is to be effected; in leaky abstractions, the impact is to facilitate sharing across concerns to collaborate on designs. Rather than form a new or different boundary, the outcome of using leaky abstractions is to "break through" a boundary within a circumscribed window of operation. The leaky abstraction provides a point for interchange across expertise situated within the present design task. Consequently, we conclude that leaky abstractions serve to enable specific collaborations about design decisions, and not to renegotiate boundary responsibilities in the design process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Delayed Specifications Reduce Friction in HAI Software Workflows</head><p>In conventional software workflows, best practice advice is to hide unimportant (and potentially complex) implementation detail across software modules and expertise boundaries <ref type="bibr" target="#b74">[75]</ref>. In fact, any "information leak" about implementation is considered a 'red flag' <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b74">75]</ref>. In this regard, our argument that leaky abstractions are necessary for HAI development may seem counter-intuitive. However, our intent is not to argue against the power of abstractions and established software practices. Rather, we suggest that abstraction may interfere when collaboration requires combining expertise. Conventional software design may be best accomplished with a "divide and conquer" process; but in cases where integrative expertise is collaboration may sharing key lower level details (and not all). Because HAI development now requires fusing human needs within technical designs, points of intense collaboration across expertise roles will occur. Our findings document leaky abstractions as means for experts to jointly consider novel issues arising in HAI As shown in Figure <ref type="figure" target="#fig_8">6</ref>, successful teams delayed system fications through iterative prototyping and constant evaluation. In the early design stages, designers and engineers fuzzy design specifications with some aspects more concretely defined. By sharing those initial design artifacts including low level details, teams overcome knowledge blindness to align AI and UX, and then collaboratively assess, negotiate, and revise their design choices.</p><p>For instance, by sharing emerging AI behavior specifications, designers can evaluate assumptions and fit for end-users, update their own design representations for task workflows and interactions, and provide feedback for human-centered design of AI. During this stage, avoiding commitment to technical specifications affords later changes and invites collaboration and inputs. As the design progresses, more and more aspects of AI and UX components become concrete, and consequently, the need for leaky abstractions lessens. In the final design stages, successful teams arrive at realized designs solutions aligned across AI, UX, and human users.</p><p>Such a workflow could address critical concerns around responsible AI design. While not explicitly documented in this study, we suspect that the collaborative approach we identified may be helpful in anticipating problems in fairness, accessibility, and trustworthiness. For instance, SoC abstractions may lose key information about complex socio-technical contexts necessary for fairness <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b84">85]</ref>. Selbst and colleagues argue that social context information may be critical for some design considerations, so standard abstraction methods may require alteration for HAI design. Similarly, our findings show that strict abstraction in representations shared between designers and engineers limit true collaboration. For instance, to design AI systems with fairness in mind, teams need to collaboratively define fair performance by considering diverse stakeholders, contexts of use, and assessment criteria (i.e., disaggregated evaluation <ref type="bibr" target="#b13">[14]</ref>). In developing their fairness checklist for software teams, Madaio et al. have enumerated steps for practitioners that span across all stages of the software lifecycle from product envisioning to deployment and maintenance <ref type="bibr" target="#b63">[64]</ref>. Further, teams need to anticipate how their design practices may limit considering diverse users. We imagine following the practice of delaying specification allows later changes and greater opportunities for responding to emerging information about system bias. Correspondingly, by sharing machine learning performance metrics and results (e.g., word error rate) with designers, engineers can better align model-level performance with diverse user needs and use contexts. In fact, our further research investigates collaborative practices for assessing fairness in AI systems <ref type="bibr" target="#b62">[63]</ref>. Despite increasing awareness, organizational goals and resource constraints continue to pose challenges for building responsible AI guidelines into current design practices <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b63">64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Advice and Open</head><p>Questions for Software Organizations: Our findings on leaky abstractions and delayed specifications pose challenges for organizations creating AI applications. Typically, AI engineers and designers are situated in separate reporting structures (including different physical environments, incentives and performance reviews), and professional exchanges occur less often between professional groups. However, when designers and engineers successfully collaborate, the resulting human-AI system design succeeds. Their individual expertise requires that designers and engineers take very different perspectives on the design of AI applications. Engineers have a more immediate perspective of being "in the weeds" (M2), accountable for building specific functionality and ensuring the computational model is accurate and robust. Designers must take a future perspective by envisioning how an AI system might work, what human users might want, and how these work together seamlessly. HAI collaboration requires mutual respect for the expertise unique to each. As identified in our findings,  leaky abstractions provide glimpses the "other side," windows large to help but not overwhelm or take over their own perspectives. In this across other team members may assist in translation. For example, project managers with a more holistic perspective may help facilitate cross-talk between engineers and designers. What might organizations do to promote the integrative collaboration practices identified in our findings? One suggestion is to increase points of contact for engineers and designers through coworking sessions. Regular informal interaction may help to identify intermediate points during the development process where integration is most needed. For instance, Subramonyam et al. propose a HAI design process model for early-stage co-design <ref type="bibr" target="#b92">[93]</ref>. This process may aid in catching emerging needs for hidden information in a just-in-time format rather than at its end. In addition, regular discussions about data-so central to defining AI capabilities-may be helpful to both groups even without specific review goals. "Data dives" might share current observations, consider what data might better inform choices, and review what is known about what users want. Providing space (in schedules and location) to inhabit the co-design process and build team familiarity will likely increase communication. Further, enacting the expectation that designers and engineers must share specifics about their progress facilitates the team co-design process within organizations.</p><p>Further, to support co-design practices, as indicated by Yang et al. <ref type="bibr" target="#b100">[101]</ref> documentation, development, and regularization of formats for leaky abstractions may be helpful. Organizations might build these formats based on current team experiences where shared leaky abstractions have proven beneficial. When and why might a similar artifact be valuable in other design issues and projects? For example, engineers may create and collect leaky abstractions aimed at illustrating an AI's dynamic behaviors, such as showing how performance will change with more user data and demonstrating how failure cases arise. Designers may create and collect leaky abstractions to help engineers (and users) envision how a final application may feel to use, and how design choices may differently impact different users. As one engineering manager said, designers can ". . . give people a target, and really paint a picture of that through design visualization, whether that's making a movie or just making a series of mocks, or building an experiential prototype, or something like that, really help land the tangibility of something pretty deep and complex" (M2). How might designers create visualizations of user experiences to help engineers appreciate detailed information about the user's AI experience?</p><p>Finally, the methods and tools available to designers require further innovation and development to respond to the need for considering more, and more varied, forms of data within the user experience. From simply testing prototypes with users, designers now need to consider data qualities representing what users want, how users differ, who compiled databases represent, system data collected over users, users' personal data histories; and user feedback to systems. HAI systems can make use of much more information from human users to improve performance; this data is not at all abstract, but contained in individual examples. Off-the-shelf ML systems and data generation tools can support designers as they investigate alternative designs for data-intensive AI. New tools are being created; for example, Proto-AI is a prototyping tool for designers that can directly invoke AI models and services, incorporate model outputs into interface designs, and enable iterative and rapid evaluation of design choices across diverse end-users and data contexts <ref type="bibr" target="#b91">[92]</ref>. However, additional robust methods and tools are needed to support designers in understanding the impact of data and providing targeted leaky abstractions for collaboration with engineers on HAI systems. 5.2.2 Advice for HCI and Software Pedagogy: Ideally, to reduce the knowledge blindness identified in this work, HAI practitioners benefit from π -shaped expertise across HCI and AI (i.e., in-depth understanding of HCI and AI) <ref type="bibr" target="#b14">[15]</ref>. Quickly acquiring such knowledge is impractical given the rapidly advancing state-of-the-art in AI technology. However, as suggested above, it is essential to rethink the existing emphasis on abstractions and separation of concerns in software development pedagogy. For AI application development, intensive data use requires fusing concerns across expertise roles. For instance, new software engineering courses might emphasize the importance of using leaky abstractions to communicate with UX professionals when developing AI-driven systems. Further, HCI pedagogy should equip future UX practitioners with data-driven design tools and methods to facilitate co-design. For instance, designers should receive training in incorporating data into their design and working with representations (e.g., interpretable ML) that occur at the boundaries. New toolkits and instructions can make HAI design accessible for students from differing backgrounds through supportive pedagogy and tools. Similarly, AI engineers should receive training in the parallel processes taking place in technical AI and UX design. They should be trained to understand the importance of UX in AI development, create and share representations of AI behavior before implementation, and co-design AI working across boundaries. Finally, the HAI curriculum should bring together students from varying backgrounds to engage in learning about team co-design throughout the engineering pipeline. Multidisciplinary pedagogical initiatives are essential to shaping the future of HAI as it is practiced in industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Limitations and Future work</head><p>Our interviews with HAI practitioners provide insightful findings on boundary tensions between designers and engineers and their innovation in creating workarounds to share design ideas across boundaries. However, interview data may be limited compared to direct observation of work practices during in-situ interactions. Due to non-disclosure agreements, our participants could not share some specific details about their HAI collaborations. For instance, interview participants were not able to share some representational artifacts they described. Consequently, our findings may fail to capture domain-and data-related nuances in conceptualizing leaky abstractions. Further, our observations do not reveal factors related to team dynamics, power relationships, and company cultures. Further research is needed to investigate the distinct characteristics leaky abstractions used across product domains, and practitioners' needs for techniques and tools to support them. Second, an essential aspect of HAI design is to meet responsible AI criteria. While our findings address the knowledge sharing and co-design pracnecessary for various responsible criteria, future research should examine socio-technical and develop end-to-end strategies for enabling responsible AI. For instance, co-design will require innovative boundary artifacts and 'leaks' to fair performance metrics across AI and Lastly, our work the boundaries between UX designers and AI practitioners. Future work should investigate how new emerging roles in (such as ML-Operations practitioners), guidelines, and improved incentive structures, can provide organizational support for responsible AI design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In conventional software design, a clean separation of concerns between UX design and software implementation provides effective coordination and hand-off between designers and engineers on a team. However, there is no clean way to separate concerns when designing AI applications for human users. Instead, HAI demands points of greater integration in AI and UX design in order to address the burgeoning use of system dynamics and human data. While our analysis of proposed HAI design guidelines in the field (and the HAI component model we construct from them) emphasized the necessity of multidisciplinary collaboration, little is known about how HAI systems are currently designed and developed in industry practice. Based on our interviews with UX researchers, AI engineers, data scientists, and project managers working on HAI applications, we identified current challenges in HAI design. Boundaries between designers and engineers introduce knowledge blindness about end-users and technology. For example, designers may not know the possibilities and limits of AI or be equipped to design for AI uncertainties. Engineers also describe difficulties in aligning data and AI models with end-user needs in the presence of uncertainty. Further, AI technology is based on a data-intensive approach that challenges conventional UX design practices. As a solution, we find that sharing leaky abstractions allows designers and engineers to overcome knowledge blindness and engage in collaborative HAI design. We offer an approach to collaboration that involves deferred specifications through iterative design and constant evaluation. Finally, we make recommendations for practice and pedagogy to support the collaborative creation of human-AI applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Affinity Diagram of HAI Guidelines</figDesc><graphic url="image-1.png" coords="5,53.80,96.00,504.41,295.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4.2.1 Design-Engineering BoundariesHinder the Cross-Cutting Requirements of HAI Guidelines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Generalized Organizational Structure of Teams in Human-AI Application Design and Development. Interview participants are overlaid onto corresponding teams (S = Software Engineer, U = UX Professional, M = Manager, D = Data Scientist, R = Research Scientist). "O" denotes the organization number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>"</head><label></label><figDesc>Storyboards or other documents that get into describing what the purpose of the behavior is, what the desired user experience is without getting into the engineering. I think of it as a sort of comic book illustration of what the user experience should be and what the system's reaction should be in different interactive situations. It was like sort of the key expected traversal through an interaction, and then maybe some of the most likely other paths about what experience you want the user, and the [system] to have together. Here is a situation, and what should happen over the course of this interaction. And I don't mean to seem territorial about this, but it's really useful to have back and forth with the people who are trained to think carefully about user experience. .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Communicating</head><label></label><figDesc>User Needs for Training Data Communicating User Needs for AI Behavior Communicating User Feedback for Iterative AI Design Qualitative Codebooks: Designers create and share codebooks to support consistent and human-centered annotation of training data [U6]. Structured Templates and Data Patterns: Designers research structured data such as user speech patterns to inform training data structure [U1]. Survey Responses &amp; User Segments: Designers work with engineers to identify user segments and personas for representative data collection [U5]. User Log Reports: Designers/ Product teams share usage logs conveying user behavior and constraints to inform model capabiltities [M3]. Labeled User Data: Designers/ Domain Experts share hand-labeled ground-truth data to communicate about correct model behavior [D1]. User Friendly Model Designers create low-fidelity mockups to communicate formatting needs for model outputs [U2]. Storyboards with AI Interaction: Designers share envisioned ideas of user interactions with AI capabilities as examples of desired model behavior [S7]. Videos of User Testing: Designers directly share videos from user testing to communicate faulty model behavior in HAI [M1]. Direct Feedback from Users: Designers share end-user reactions to AI features to communicate pertaining to trust [M2]. Engineering Participation during User Testing: Designers invite engineers to participate in user study to directly receive feedback on HAI [U3].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Designers Share Low-Level UX Knowledge with Engineers to Inform AI Implementation Decisions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Engineers Share AI Implementation Details with Designers to Inform UX Decisions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Delayed Specification through Vertical Prototyping and Constant Evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Each organization is listed with interviewees by role (S = Software Engineer (AI), U = UX Professional, M = Manager, D = Data Scientist, R = Research Scientist) and a brief description. The number in brackets next to each interviewee indicates the participant's years of professional experience in HAI.</figDesc><table><row><cell cols="2">Organization Interviewee (Years of HAI Experience)</cell><cell cols="2">Business Model Size of Organization</cell></row><row><cell>O1</cell><cell>S1 (2 yrs)</cell><cell>B2C</cell><cell>1,000 -5,000</cell></row><row><cell>O2</cell><cell>S2 (3 yrs), S4 (2 yrs), M3 (12 yrs)</cell><cell>B2C</cell><cell>10,000 -50,000</cell></row><row><cell>O3</cell><cell cols="2">M2 (4 yrs), R2 (6.5 yrs), U1 (4 yrs), U5 (3 yrs) B2C, B2B</cell><cell>&gt; 100,000</cell></row><row><cell>O4</cell><cell>M1 (2.5 yrs)</cell><cell>B2B</cell><cell>&lt; 100</cell></row><row><cell>O5</cell><cell>D1 (5 yrs)</cell><cell>B2B</cell><cell>&gt; 100,000</cell></row><row><cell>O6</cell><cell>S5 (3 yrs), R1 (7 yrs)</cell><cell>B2C</cell><cell>&gt; 100,000</cell></row><row><cell>O7</cell><cell>U2 (3 yrs)</cell><cell>B2B</cell><cell>&lt; 100</cell></row><row><cell>O8</cell><cell>U6 (2 yrs), D2 (4 yrs)</cell><cell>B2B</cell><cell>&gt; 100,000</cell></row><row><cell>O9</cell><cell>S3 (1 yr)</cell><cell>B2B</cell><cell>&lt; 100</cell></row><row><cell>O10</cell><cell>D3 (6 yrs)</cell><cell>B2C</cell><cell>1,000 -5,000</cell></row><row><cell>O11</cell><cell>U3 (1 yr)</cell><cell>B2C</cell><cell>10,000 -50,000</cell></row><row><cell>O12</cell><cell>U4 (1 yr)</cell><cell>B2B</cell><cell>100 -500</cell></row><row><cell>O13</cell><cell>S6 (2.5 yrs)</cell><cell>B2B</cell><cell>5,000 -10,000</cell></row><row><cell>O14</cell><cell>S7 (3.5 yrs )</cell><cell>B2C</cell><cell>&lt; 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Engineers share data provenance, and interpretations of feature values for UX presentation [D2]. Function Logic/API Annotations: Engineers annotate AI behavior and logic wireframes to communicate user input and interaction needs for AI [S6]. Dashboard for AI Performance: Engineers share visual dashboards to inform designers about AI performance and setting end-user expectations [D1, D3]. Engineers share spreadsheets with raw model outputs to help designers prototype user interfaces for AI [D2, U3]. Engineers share graybox prototypes (AI feature demos without product UI) to get early stage feedback on AI interaction behavior [S1].</figDesc><table><row><cell>Communicating Data Characteristics for Design</cell></row><row><cell>Dataset Specifications : Raw JSON Data: work with raw JSON data from third-party AI services</cell></row><row><cell>to create end-user-friendly labels for AI output presentation [M1].</cell></row><row><cell>Computational Notebooks: Engineers share notebooks with</cell></row><row><cell>data to allow designers to explore on their own [R1,</cell></row><row><cell>Communicating Model Behavior for UI/UX Design</cell></row><row><cell>Sharing AI Implementation for Human-Centered Evaluation Model Outputs, Features, and Weights: Engineers share spreadsheets with model outputs to get feedback on model behavior from domain experts [D1]. AI Capability Demo Prototypes: Engineers showcase interactive prototypes of AI features to communicate novel capabilities with designers [S5, U4]. Raw Model Outputs: Knobs to Tune Model Parameters: Engineers expose knobs for designers to</cell></row><row><cell>explore optimum parameter values and defaults</cell></row><row><cell>Graybox Prototypes: Model Rules and Assumptions: Engineers share spreadsheets with rules and</cell></row><row><cell>assumptions in model implementation to get feeback on model logic [D1]</cell></row><row><cell>Model Logic Visualization: Engineers create interpretable visualizations of</cell></row><row><cell>ML models to get feedback from end-users on model performance [S1].</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank our reviewers and interview participants for their time and inputs. We also thank Steve Oney, Steven Drucker, Jasmine Jones, Ammari, Yixin Zou for feedback on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reductions approach to fair classification</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Computation and human experience</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Agre</surname></persName>
		</author>
		<author>
			<persName><surname>Philip E Agre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deloitte Insights</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www2.deloitte.com/us/en/insights/deloitte-insights-magazine.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Guidelines for human-AI interaction design</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/blog/guidelines-for-human-ai-interaction-design/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Software engineering for machine learning: a case study</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice</title>
				<meeting>the 41st International Conference on Software Engineering: Software Engineering in Practice</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Power to the people: The role of humans in interactive machine learning</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Bradley</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="105" to="120" />
			<pubPlace>Ai Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guidelines for human-AI interaction</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penny</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamsi</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kori</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Software engineering challenges of deep learning</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Arpteg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Brinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luka</forename><surname>Crnkovic-Friis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Assuring the machine learning lifecycle: Desiderata, methods, and challenges</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Ashmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Calinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Human-centered AI: The role of Human-centered Design Research in the development of AI</title>
		<imprint>
			<date type="published" when="2020-01">Jan Auernhammer. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Merging the Cultures of Design and Engineering: A Case Study</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Baca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Carruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elijah</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waddell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Design, User Experience, and Usability</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="628" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Balsamiq</surname></persName>
		</author>
		<ptr target="https://balsamiq.com/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bootstrapping: Douglas Engelbart, coevolution, and the origins of personal computing</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Bardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Stanford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacquelyn</forename><surname>Krones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Wadsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06076</idno>
		<title level="m">Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward human-centered algorithm design. Data &amp;amp</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><surname>Baumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2053951717718854</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prototyping tools and techniques</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><forename type="middle">E</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Computer Interaction</title>
				<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="137" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memoing in qualitative research: Probing data and processes</title>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Birks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ysanne</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Francis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of research in nursing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on fairness, accountability and transparency. PMLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Software developers learning machine learning: Motivations, hurdles, and desires</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), VL/HCC</title>
				<meeting>the IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), VL/HCC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A pragmatic view of knowledge and boundaries: Boundary objects in new product development</title>
		<author>
			<persName><surname>Paul R Carlile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organization science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Architecting in software ecosystems: interface translucence as an enabler for scalable collaboration</title>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Cataldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Herbsleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth European Conference on Software Architecture: Companion Volume</title>
				<meeting>the Fourth European Conference on Software Architecture: Companion Volume</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Design and AI: prospects dialogue</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ceconello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spallazzo</surname></persName>
		</author>
		<author>
			<persName><surname>Sciannamè</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Grounding in communication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>Brennan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How do committees invent</title>
		<author>
			<persName><forename type="first">E</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName><surname>Conway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Datamation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="28" to="31" />
			<date type="published" when="1968">1968. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meg</forename><surname>Pirrung</surname></persName>
		</author>
		<title level="m">Interactive Machine Learning Heuristics</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The &apos;double diamond&apos;design process model</title>
		<author>
			<persName><forename type="first">Design</forename><surname>Council</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<publisher>Design Council</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sometimes you need to see through walls: a field study of application programming interfaces</title>
		<author>
			<persName><forename type="first">Cleidson Rb De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Redmiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Te</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Millen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM conference on Computer supported cooperative work</title>
				<meeting>the 2004 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The future of human-ai collaboration: a taxonomy of design knowledge for hybrid intelligence systems</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Dellermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Calma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Lipusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Weigel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Ebel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Sage handbook of qualitative research. sage</title>
		<author>
			<persName><forename type="first">K</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonna S</forename><surname>Denzin</surname></persName>
		</author>
		<author>
			<persName><surname>Lincoln</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Responsible artificial intelligence: designing AI for human values</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Dignum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The humble programmer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Edsger</surname></persName>
		</author>
		<author>
			<persName><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="859" to="866" />
			<date type="published" when="1972">1972. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ux design innovation: Challenges for working with machine learning as a design material</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Dove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Halskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Translucent cache management for mobile computing</title>
		<author>
			<persName><forename type="first">Maria R Ebling ; Carnegie-Mellon Univ Pittsburgh Pa School Of Computer</forename><surname>Science</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge practices in design: the role of visual representations asepistemic objects</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ewenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organization studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="30" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Symmetry of ignorance, social creativity, and metadesign. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="527" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Wire: Your AI-Powered &apos;To Do&apos; list</title>
		<author>
			<persName><forename type="first">David</forename><surname>Flink</surname></persName>
		</author>
		<ptr target="https://people.ai/blog/the-wire-your-ai-powered-to-do-list/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Artificial intelligence, values, and alignment. Minds and Machines</title>
		<author>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="411" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From knowledge accumulation to accommodation: Cycles of collective cognition in work groups</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cristina</surname></persName>
		</author>
		<author>
			<persName><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Organizational Behavior: The International Journal of Industrial, Occupational and Organizational Psychology and Behavior</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From tool to partner: The evolution of human-computer interaction</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://pair.withgoogle.com/JonathanGrudin.2017" />
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human-Centered Interaction</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="2017">2019. 2017</date>
		</imprint>
	</monogr>
	<note>People + AI Guidebook</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Breakdowns and processes during the early activities of software design by professionals</title>
		<author>
			<persName><forename type="first">Raymonde</forename><surname>Guindon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herb</forename><surname>Krasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Curtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical studies of programmers: Second Workshop</title>
				<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="65" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ground: A Data Context Service</title>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Joseph M Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Sreekanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akon</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreyashi</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhanshu</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirshanka</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Leaky Objects: Implicit Information, Unintentional Communication</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference Companion Publication on Designing Interactive Systems</title>
				<meeting>the 2017 ACM Conference Companion Publication on Designing Interactive Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="182" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Design Methods to Investigate User Experiences of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Karey</forename><surname>Helms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Airi</forename><surname>Lampinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Michi</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">API design matters. Queue</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Twitter apologises for &apos;racist&apos; image-cropping algorithm</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Trials and tribulations of developers of intelligent systems: A field study</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Design prototyping in&quot; fuzzy front end&quot; of product development-Rapid prototyping at the stage of high uncertainty</title>
		<author>
			<persName><forename type="first">Akimitsu</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Mizuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPIM Innovation Symposium. The International Society for Professional Innovation Management (ISPIM)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving fairness in machine learning systems: What do industry practitioners need</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Miro</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The use of prototypes to bridge knowledge boundaries in agile software development</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Maike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><forename type="middle">V</forename><surname>Dibbern</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information systems journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="270" to="294" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Designing the UI and User Experience of a Machine Learning App</title>
		<author>
			<persName><forename type="first">Apple</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://developer.apple.com/design/human-interface-guidelines/machine-learning/overview/introduction/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Identifying the intersections: User experience+ research scientist collaboration in a generative machine learning interface</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Kayacik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherol</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Signe</forename><surname>Noerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Holbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">S09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Boundary objects and the technical culture divide: successful practices for voluntary innovation teams crossing scientific and professional fields</title>
		<author>
			<persName><forename type="first">Zack</forename><surname>Kertcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Coslor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Management Inquiry</title>
		<imprint>
			<biblScope unit="page">1056492618783875</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul N</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">411</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Between chaos and routine: Boundary negotiating artifacts in collaboration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charlotte</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECSCW 2005</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="387" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Boundary negotiating artifacts: Unbinding the routine of boundary objects and embracing chaos in collaborative work</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charlotte</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<publisher>CSCW</publisher>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="307" to="339" />
		</imprint>
	</monogr>
	<note>Supported Cooperative Work</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">This is not a boundary object: Reflections on the origin a concept</title>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">Leigh</forename><surname>Star</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology, &amp; Human Values</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="601" to="617" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Intent specifications: An approach to building humancentered specifications</title>
		<author>
			<persName><surname>Nancy G Leveson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on software engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="15" to="35" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Human-centered AI Cheat-sheet</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Lovejoy</surname></persName>
		</author>
		<ptr target="https://uxdesign.cc/human-centered-ai-cheat-sheet-1da130ba1bab" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">IBM design thinking software development framework</title>
		<author>
			<persName><forename type="first">Percival</forename><surname>Lucena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Braz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adilson</forename><surname>Chicoria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Tizzei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Workshop on Agile Methods</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A taxonomy of software engineering challenges for machine learning systems: An empirical investigation</title>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Ellen</forename><surname>Lwakatare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiswarya</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Agile Software Development</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
	<note>Helena Holmström Olsson, and Ivica Crnkovic</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Facebook Apologizes After A.I. Puts &apos;Primates&apos; Label on Video of Black Men</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mac</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Assessing the Fairness of AI Systems: AI Practitioners&apos; Processes, Challenges, and Needs for Support</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Madaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Egede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hariharan</forename><surname>Subramonyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">W1</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Co-designing checklists to understand organizational challenges and opportunities around fairness in ai</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Madaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">User requirements analysis</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Bevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP World Computer Congress, TC 13</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Transcending knowledge differences in cross-functional teams</title>
		<author>
			<persName><forename type="first">Ann</forename><surname>Majchrzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samer</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><surname>Faraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organization Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Definitions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">How Data ScientistsWork Together With Domain Experts in Scientific Collaborations: To Find The Right Answer Or To Ask The Right Question?</title>
		<author>
			<persName><forename type="first">Yaoli</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Baldini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><surname>Mojsilović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>GROUP</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Reliability and Inter-Rater Reliability in Qualitative Research: Norms and Guidelines for CSCW and HCI Practice</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarita</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Forte</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359174</idno>
		<ptr target="https://doi.org/10.1145/3359174" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2019-11">2019. Nov. 2019</date>
		</imprint>
	</monogr>
	<note>Interact. 3, CSCW, Article 72</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Azure AI -Make artificial intelligence (AI) for your business today</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://azure.microsoft.com/en-us/overview/ai-platform/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. &amp;amp</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Therese</forename><surname>Png</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Isaac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="659" to="684" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Human-centered study of data science work practices</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">E</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Beth</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Kery</surname></persName>
		</author>
		<author>
			<persName><surname>Passi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">How data science workers work with data: Discovery, capture, curation, design, creation</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Piorkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Tsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI conference on human factors in computing systems</title>
				<meeting>the 2019 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">User centered system design: New perspectives on human-computer interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><surname>Draper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">A Philosophy of Software Design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Yaknyam Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the criteria to be used in decomposing systems into modules</title>
		<author>
			<persName><forename type="first">Parnas</forename><surname>David Lorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1053" to="1058" />
			<date type="published" when="1972">1972. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Lowering the barrier to applying machine learning</title>
		<author>
			<persName><forename type="first">Kayur</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct proceedings of the 23nd annual ACM symposium on User interface software and technology</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="355" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Combining Design Thinking and Agile Development to Master Highly Innovative IT Projects</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Przybilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schreieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Klinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Pflügler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Wiesche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Krcmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Projektmanagement und Vorgehensmodelle 2018-Der Einfluss der Digitalisierung auf Projektmanagementmethoden und Entwicklungsprozesse</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Human-centered artificial intelligence and machine learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Behavior and Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="36" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Research priorities for robust and beneficial artificial intelligence</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ai Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><surname>Joachims</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07578</idno>
		<title level="m">Improving Recommender Systems Beyond the Algorithm</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Hidden technical debt in machine learning systems</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Francois</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Dennison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2503" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The KJ method: A technique for analyzing data derived from Japanese ethnology</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Scupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human organization</title>
		<imprint>
			<biblScope unit="page" from="233" to="237" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Human-centered software engineering-integrating usability in the software development lifecycle</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Seffah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gulliksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel C</forename><surname>Desmarais</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fairness and Abstraction in Sociotechnical Systems</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danah</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Vertesi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287598</idno>
		<ptr target="https://doi.org/10.1145/3287560.3287598" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the Conference on Fairness, Accountability, and Transparency<address><addrLine>Atlanta, GA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
	<note>) (FAT* &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Human-centered artificial intelligence: Reliable, safe &amp; trustworthy</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="495" to="504" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">APIs: The Future Is Now</title>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">M</forename><surname>Spool</surname></persName>
		</author>
		<ptr target="https://articles.uie.com/api_" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Common ground</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Stalnaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and philosophy</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="701" to="721" />
			<date type="published" when="2002-05-06">2002. 5/6 (2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The structure of ill-structured solutions: Boundary objects and heterogeneous distributed problem solving</title>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">Leigh</forename><surname>Star</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed artificial intelligence</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Facial recognition is the plutonium of AI</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">XRDS: Crossroads, The ACM Magazine for Students</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Basics of qualitative research</title>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliet</forename><surname>Corbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Sage publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces</title>
		<author>
			<persName><forename type="first">Hariharan</forename><surname>Subramonyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colleen</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Intelligent User</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Towards A Process Model for Co-Creating AI Experiences</title>
		<author>
			<persName><forename type="first">Hariharan</forename><surname>Subramonyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colleen</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Adar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461778.3462012</idno>
		<ptr target="https://doi.org/10.1145/3461778.3462012" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1529" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">The cognitive artifacts of designing</title>
		<author>
			<persName><forename type="first">Willemien</forename><surname>Visser</surname></persName>
		</author>
		<imprint>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Sticky information&quot; and the locus of problem solving: implications for innovation</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Von</forename><surname>Hippel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Recurrent Knowledge Boundaries in Outsourced Software Projects: A Longitudinal Study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Maike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Toward human-centered AI: a perspective from human-computer Interactions</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="42" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The role of design in creating machine-learning-enhanced user experience</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 AAAI Spring Symposium Series</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Machine Learning as a UX Design Material: How Can We Imagine Beyond Automation, Recommenders, and Reminders</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Sketching NLP: A Case Study of Exploring the Right Things To Design with Language Intelligence</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Cranshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">185</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Investigating how experienced UX designers effectively work with malearning</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Scuito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Designing Interactive Systems Conference</title>
				<meeting>the 2018 Designing Interactive Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="585" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Reexamining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult Design</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Rosé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 chi conference on human factors in computing</title>
				<meeting>the 2020 chi conference on human factors in computing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">How do data science workers collaborate? roles, workflows, and tools</title>
		<author>
			<persName><forename type="first">X</forename><surname>Amy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
