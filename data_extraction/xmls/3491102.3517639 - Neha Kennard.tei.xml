<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Opportunities and Challenges of Automatic Speech Recognition Systems for Low-Resource Language Speakers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Electra</forename><surname>Wallington</surname></persName>
							<email>electra.wallington@ed.ac.uk</email>
							<idno type="ORCID">0000-0003-2078-6699</idno>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dani</forename><surname>Kalarikalayil</surname></persName>
							<email>daniel@studiohasi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
							<email>o.klejch@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Pearson</surname></persName>
							<email>j.pearson@swansea.ac.uk</email>
							<idno type="ORCID">0000-0001-7657-7373</idno>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Robinson</surname></persName>
							<idno type="ORCID">0000-0002-9597-9615</idno>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Reitmaier</surname></persName>
							<email>thomas.reitmaier@swansea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dani</forename><forename type="middle">Kalarikalayil</forename><surname>Raju</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matt</forename><surname>Jones</surname></persName>
							<email>matt.jones@swansea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Bell</surname></persName>
							<email>peter.bell@ed.ac.uks.n.w.robinson@swansea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University University of Edinburgh</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Opportunities and Challenges of Automatic Speech Recognition Systems for Low-Resource Language Speakers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3491102.3517639</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-07-22T05:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech/language</term>
					<term>automatic speech recognition</term>
					<term>mobile devices</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yiza nemifuno xa ubuya</head><p>Transcription: isiXhosa Figure 1: Artist's representation of the Automatic Speech Recognition systems we developed and feld-tested in partnership with two communities in South Africa and India to transcribe isiXhosa (left) and Marathi (right) voice messages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Researchers in the feld of Automatic Speech Recognition, or ASR, are increasingly turning their attention towards so-called lowresource languages. 1 At the same time companies like Facebook and Google are developing their own ASR approaches (e.g., Facebook's wav2vec <ref type="bibr" target="#b64">[66]</ref>) and language models (e.g., Google's Cloud Speech-to-Text <ref type="bibr" target="#b29">[31]</ref>).</p><p>Such 'unsupervised' approaches no longer require the accompaniment of 'gold-standard' transcriptions but depend on large amounts of training data instead. As critical scholarship reveals, such approaches raise salient questions surrounding data privacy and data collection practices <ref type="bibr" target="#b85">[87]</ref>. For instance, unsupervised approaches often rely on web-scraping techniques to collect and augment training data; these work on the problematic assumption that such language data is interchangeable -regardless of where it is found <ref type="bibr" target="#b11">[13]</ref>. Furthermore, training Natural Language Processing (NLP) models on such big datasets comes with substantial additional costs: cf. the cloud computing bills that run into the hundreds of thousands of dollars to train, test, and tweak AI models <ref type="bibr" target="#b65">[67]</ref>, and the associated climate <ref type="bibr" target="#b70">[72]</ref> and rare earth mineral <ref type="bibr" target="#b11">[13]</ref> impacts that such large-scale computations require.</p><p>While the current state-of-the-art in 'low-resource' ASR methodologies is seeing burgeoning interest and rapid development, we worry that these are motivated by the intellectual challenge <ref type="bibr" target="#b6">[8]</ref> and are increasingly distant from users and communities themselvesfar removed from the specifc needs and functions of the speakers of such low-resource languages-and are being developed upon datasets that do not adequately represent the ways in which people speak and communicate. Given HCI's 20-year history of working with marginalised and minority users in the Global South <ref type="bibr" target="#b24">[26]</ref>, in this paper we develop a HCI perspective to collaborate with ASR researchers and language experts: to leverage their advances, but also to contextualise, situate and develop use-cases and datasets for ASR in partnership with two communities of minority language speakers.</p><p>Looking beyond technical challenges and using the lenses afforded by post-colonial computing frameworks <ref type="bibr">[25,</ref><ref type="bibr" target="#b36">38]</ref> we can interrogate what it means that Google's Cloud Speech-to-Text service now supports three languages spoken in South Africa-Afrikaans, English, and isiZulu-and raise critical questions, such as why these three and not one of the country's other eight ofcial languages? In a post-colonial setting such as South Africa, with legacies of racist ideologies and eugenic science that justifed colonial and apartheid rule, such questions rarely have neutral answers. For the twin projects of colonialism and apartheid entrenched various forms of inequality that, sadly, persist to this day: racial, economic, digital, but also linguistic inequality. It is great to see South Africa's second most widespread language supported: isiZulu, which is predominately spoken in the economically prosperous Gauteng province. However, isiXhosa, which is mostly spoken by people from the less prosperous Eastern Cape, is technically the third most widespread language, rather than Afrikaans, which next to English is the second main language spoken by White South Africans of Dutch, French, and German descent <ref type="bibr" target="#b52">[54]</ref>.</p><p>Our research is motivated by addressing this class of gap and, in this instance, developing a baseline isiXhosa ASR language model; but, just as importantly, we uncover the challenges, opportunities, and use-cases for such an ASR system in one particular marginalised South African community. We are also mindful of HCI scholarship that identifes text-input as a barrier to digital participation for low-resource language speakers in a diferent setting: Marathispeaking 'emergent users' in India <ref type="bibr" target="#b18">[20]</ref>. The Devanagari script of Marathi means that its speakers are often forced to transliterate their messages into the Latin script, or to install custom keyboards, which presents its own set of challenges such as more complex UI hierarchies <ref type="bibr" target="#b80">[82]</ref>. What role could ASR-enabled technologies play here?</p><p>To address these questions, we engage with communities directly and report and refect on interviews and co-design workshops involving data-collection exercises and technology probes. Through these diverse activities we identify opportunities, challenges, usecases, and implications for collaborations both across our felds of research and crucially through involving community partners in an isiXhosa-speaking township in the outskirts of Cape Town, South Africa and with Marathi-speaking residents of an informal settlement in the heart of Mumbai, India.</p><p>Through our research we identify a novel use-case for voice technologies in such communities-voice message transcription-and demonstrate pervasive and creative use of WhatsApp voice messaging. Finally, we develop prototypes of baseline Speech Recognition systems and feld-test these to further engage user communities. We review user perspectives collaboratively, and consider how challenges-particularly surrounding data collection and transcription-could be addressed from both ASR and HCI perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CONTEXT</head><p>We begin our inquiry by introducing the communities and localities we partnered with, namely Langa and Dharavi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Langa, Cape Town, South Africa</head><p>With about 50,000 residents Langa is the oldest township in Cape Town, a peripheral locality in the Cape Flats that was set aside for nonwhites during apartheid rule. As the provenance of the place itself is rooted in discrimination and racism, Langa is classifed as a previously disadvantaged area and, sadly, the issues of structural inequalities, poverty and crime established during apartheid not only persist to this day, but often (though no longer exclusively) fall along racial lines and exhibit a strong spatial dimension <ref type="bibr" target="#b83">[85]</ref>.</p><p>While Langa is inhabited largely by frst-language isiXhosa speakers, the linguistic landscape of public spaces-local newspapers, advertising, business names, government and community notices, etc.-is dominated by English and Afrikaans <ref type="bibr" target="#b12">[14]</ref>. Co-designing and carrying out formative and summative evaluations of novel technologies in Langa over the past ten years we have also come to know and appreciate the hospitality, good humour and honesty of its community members. As COVID-19 travel restrictions and lockdowns rendered co-located design activities impossible, we relied on the relationships and trust we have formed over many years as well as on local facilitator embedded in the community to support unfamiliar, technologically-mediated design workshops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dharavi, Mumbai, India</head><p>Our team has similar longstanding and equally formative experiences of partnering with community members in Dharavi, which we again relied on as we pivoted to remote interviews and technology deployments. In addition, our team also consists of a researcher (and author of this paper) who lives in a nearby suburb in Mumbai and is a fuent Hindi speaker.</p><p>Dharavi, colloquially referred to as "the largest slum in Asia, " is a neighbourhood in the centre of Mumbai, right next to the fnancial centre of India, and therefore occupies some of the most expensive and sought after real estate in the world <ref type="bibr">[40, p.264</ref>]. Situated on initially undesirable marshland, Dharavi attracted migrants and settlers from across India, who worked hard and reinvested their savings into improving their housing and locality without gaining any clear legal title <ref type="bibr">[10, p.47]</ref>. Over generations, settlers developed thriving fshing, tannery and textile industries, as well as associated supply chains and satellite businesses. With the land surrounding Dharavi now formally developed, Dharavi has become extremely densely populated, estimated to be around 20 times the population density of London, or 500 times the density of Miami <ref type="bibr" target="#b8">[10]</ref>. The migrant history of Dharavi is refected in its cultural and linguistic diversity. Less than half of Dharavi's residents hail from Maharashtra and speak Marathi, the ofcial language of the State; about 30 % of residents came from the Gangetic plains and 20 % from Tamil Nadu, so "Marathi, Hindi, Tamil, and Urdu are the most commonly spoken languages in Dharavi" <ref type="bibr">[64, p.45]</ref>. In this context, Hindi triumphs over Marathi (and English) as a unifying language that is "more or less understood by Dharavi's entire population and facilitates day-to-day communication" <ref type="bibr">[64, p.45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Research at the intersections of AI and HCI is a hot topic at CHI, eliciting critiques <ref type="bibr" target="#b11">[13]</ref> and agenda-setting positions from some of the most prominent commentators and visionaries of our feld <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b66">68]</ref>. While we are mindful of these critical perspectives, and advance critical debates surrounding AI within our feld and society writ large in our conclusion, we turn frst and foremost to the work of scholars local to the communities we partnered with to situate our contribution. In Cape Town, the work of the linguist Ana Deumert and media scholar Marion Walton shows particular depth and breadth. And in Mumbai, we draw upon the pioneering work of Devanuj and Joshi to make technology more accessible to 'emergent users' and who identifed text-input as a distinct barrier to digital participation <ref type="bibr" target="#b18">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sociolinguistics &amp; Materiality</head><p>Deumert's book on "Sociolinguistics and Mobile Communication" is particularly relevant as it focuses on digital communication and multi-lingual data and is grounded in ethnographically-informed case studies <ref type="bibr" target="#b16">[18]</ref>. Given its unique emphasis on African perspectives and datasets, it is no surprise that creativity and inequality emerged as key themes of her research. Of course, drawing attention to creative and vernacular forms of design-in-use are pervasive and celebrated qualities of ethnographic HCI research <ref type="bibr" target="#b71">[73]</ref>. On this point, Deumert advises "to look carefully at the kinds of creativities we see in new media environments in order to understand the possibilities for novelty as well as the constraints within which writers/speakers operate" <ref type="bibr">[18, p.170]</ref>.</p><p>Given her focus on African case-studies, Deumert furthermore points out that "the issue of inequality [ . . . ] matters whenever we write about mobile communication" <ref type="bibr">[18, p.172</ref>]. Studying, theorising and intervening to address issues surrounding inequality are, of course, dominant genres within the kindred felds of ICT for Development (ICT4D) <ref type="bibr" target="#b34">[36]</ref> and HCI for Development (HCI4D) <ref type="bibr" target="#b14">[16]</ref> research that often surface and address questions surrounding the material conditions of digital access. Such a material perspective remains important in recognising issues that occur 'after access', but are nevertheless shaped it <ref type="bibr">[23]</ref>. Here, South African media scholar Marion Walton operationalises the apposite term 'pavement internet' to critique the metaphor of mobile platforms, which in her view are not fat but highly unequal and render some user groups 'digitally invisible' <ref type="bibr" target="#b79">[81]</ref>. Walton shows how high data costs associated with accessing streaming video platforms are a barrier to participation for economically marginalised users. Comments on streaming video content, such a videos associated with breaking news stories, often include requests to "plz watsapp it 4 m @ [phone number anonymised]"; this shows that such users "wanted to pass it to their own WhatsApp contacts or smuggle it via Bluetooth through the cracks of the pavement internet" <ref type="bibr" target="#b79">[81]</ref>.</p><p>HCI commentators are increasingly recognising that how information is materially represented shapes how it can be put to work, and that consequently the "material arrangements of information [ . . . ] matter signifcantly for our experience of information and information systems" <ref type="bibr">[24, p.4]</ref>. Particularly for mobile telecommunication applications, it is a mistake to treat the text, voice and multimedia messages that people send as a purely immaterial digital form made of bits that encode information; that is, from an information theory perspective as articulated by Claude Shannon, the prevailing mythology of the digital realm. After all, as Richard Harper explains, "communication acts are not to be thought of as, say, a transfer of information [ . . . ] but as acts that alter the moral fabric of the relationship between the senders and the receivers" <ref type="bibr" target="#b32">[34]</ref>.</p><p>Such performative values, which are part and parcel of all communication acts, are important to consider. They come into play when we consider an additional form of inequality that Deumert reveals and that the HCI4D and ICT4D research communities are less familiar with: linguistic inequality.</p><p>Here, Deumert demonstrates that only few languages shape the linguistic diversity of virtual spaces in general. And concretely, there is a dearth of isiXhosa content online, and the laudable initiatives that aim to increase isiXhosa content online, often use a form of language that "is not a representation of a real, existing language-in-use"; in efect such initiatives can reproduce rather than challenge global inequalities <ref type="bibr" target="#b16">[18]</ref>. In the Global North and in venues like CHI or Interspeech we may be keen to recognise, support and celebrate linguistic diversity and multilingualism, but can easily make the assumption that "people will necessarily want to access material in their frst or 'native' language" <ref type="bibr">[18, p.75]</ref>. To avoid locking already marginalised people into one scale-level-the local [74]-Deumert's words of warning are critical to take to heart, even if they are difcult to hear: However, in an increasingly global and interconnected world, where most people speak more than one language, such monoglot ideologies have little currency. This is particularly true for postcolonial societies where everyday multilingualism is the norm, and where the process of becoming literate is usually linked to acquiring profciency in the former colonial language <ref type="bibr">[18, p.75]</ref>.</p><p>The canonical text situated at the intersections of language, culture, identity, and coloniality is "Decolonising the Mind" by the great Kenyan writer and scholar Ngug ˜ ĩ wa Thiong'o <ref type="bibr" target="#b53">[55]</ref>. Through his book, Ngug ˜ ĩ makes the impassioned argument that African mother-tongues are vehicles of culture and identity and to 'decolonise the mind' African literature ought to be written in African mother-tongues:</p><p>Language is thus inseparable from ourselves as a community of human beings with a specifc form and character, a specifc history, a specifc relationship to the world <ref type="bibr">[55, p.15</ref>].</p><p>Our research is situated at the intersection of these contrasting perspectives, which we use as lenses to navigate a difcult and contentious terrain. But most of all, we empathise with people who navigate through these issues as they go about living their lives. And ultimately, we take our cues from them.</p><p>Research on SMS messaging practices in South Africa furthermore illustrates this dialectic. Poly-lingual users prefer texting and accessing online information in English, making use of predictive text as well as frequently abbreviate English words <ref type="bibr" target="#b17">[19]</ref>. Less frequently, the same users send isiXhosa messages as a matter of pride and principle. Compared to English messages, these are harder to type without predictive-text dictionaries and in the context of sociolinguistic norms that discourage abbreviating isiXhosa <ref type="bibr" target="#b17">[19]</ref>. These norms and choice of language not only resonate with Ngug ˜ ĩ's perspectives, but also point to the performative values of communication, namely those going beyond conveying information, for which an abbreviated English message would have sufced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WhatsApp, Voice Messaging &amp; Text-Input Barriers</head><p>Before the arrival of widespread and low-cost messaging apps, unabbreviated isiXhosa messages often required message content to be spread across multiple SMS messages, which came at additional cost when compared to abbreviated English messages conveying the same information. Applications like WhatsApp have since drastically reduced costs, provided richer capabilities such as voice, photo, and video messaging, and ultimately transformed telecommunication practices across the Global South. For instance, in South Africa WhatsApp is now the most popular mobile app, with 58 % of all mobile phone owners using it <ref type="bibr" target="#b69">[71]</ref>. Such statistics are also refected in a diary study of less-connected mobile users across South Africa, where de Lanerolle et al. found that "the instant messaging application WhatsApp was by far the most frequently used Internet-based application" <ref type="bibr" target="#b13">[15]</ref>. Widespread WhatsApp usage in South Africa is also reported among refugees <ref type="bibr" target="#b78">[80]</ref> and fnanciallyexcluded entrepreneurs <ref type="bibr" target="#b40">[42]</ref>. Further, an ethnographic study of chat at work in India and Kenya that, like our research, is situated across Sub-Saharan Africa and the Indian sub-continent, reveals widespread usage of WhatsApp <ref type="bibr" target="#b51">[53]</ref>. In India this comes as no surprise, considering that there are almost half a billion Indian WhatsApp users 2 . Furthermore, WhatsApp is credited with removing barriers to digital participation in India specifcally <ref type="bibr" target="#b3">[4]</ref>. Of these varied contributions situated in the Global South, only de Lanerolle et al.'s diary study of less-connected South African users <ref type="bibr" target="#b13">[15]</ref> and Balkrishan et al.'s Indian WhatsApp study <ref type="bibr" target="#b3">[4]</ref> report that some users use WhatsApp for voice messaging. However, other than such passing references, research has not unpacked specifc practices or motivations.</p><p>It is, however, generally accepted that text entry in Indian languages poses challenges <ref type="bibr" target="#b3">[4]</ref>. Even looking back to the days of physical keyboards, typing in such languages was complex due to the structure of Indic scripts and large number of characters involved <ref type="bibr" target="#b39">[41]</ref>. There have been advances in creating dynamic keyboards for mobiles, with good success. For instance, in trials, the Swarachakra keyboard <ref type="bibr" target="#b48">[50]</ref> has proven to be an improvement over the standard Indian script keyboard layout (InScript) in terms of speed, accuracy and user ratings. However, research has shown that for some users, bad experiences using Indic language keyboards on phones in the past had led to them giving up and returning to the standard QWERTY Latin-script model <ref type="bibr" target="#b3">[4]</ref>. Furthermore, the entry-level barrier of such Indic scripts is particularly prevalent for novice users <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b28">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LANGA WORKSHOPS</head><p>Given the context and background discussed above, we now report on two co-design workshops we conducted in Langa. The aims of the Langa Workshops were two-fold: to explore and develop potential use-cases for an isiXhosa ASR system; and, to collect more representative data of isiXhosa 'language-in-use' <ref type="bibr" target="#b17">[19]</ref>. Compared to unsupervised ASR approaches that require large datasets, we were interested in exploring whether smaller amounts of 'in-domain' data could instead be utilised for ASR development. That is, data which more accurately refects the style, speed, locale or topic of conversation, and is therefore invaluable for testing and evaluating language models during development.</p><p>We partnered with an experienced workshop facilitator and translator, local to Langa, with whom we have a long history of collaboration. The facilitator recruited participants, ensured that design activities followed local COVID-19 restrictions, and fed back on workshop plans, in particular to keep discussions and activities focused. This latter suggestion also matched the appeal from the ASR researchers on our team: to identify and focus on particular "domains of speech" and to, at least initially, avoid more general-purpose and open-ended speech-driven Intelligent Personal Assistant (IPA) application areas, such as those found in Google's Assistant or Apple's Siri. Before the frst Langa workshop, the HCI, ASR and facilitator team collaboratively developed two initial use-cases and discussion points that leveraged a user-engagement and data-collection probe, as outlined below.</p><p>The frst use-case centred around a simple IPA system to support isiXhosa voice reminders. We hoped that this focused usecase would present an interesting but also delineated 'domain of speech'-times, people, places and activities-and convey to participants some of the core capabilities of an isiXhosa ASR &amp; NLP system: the ability to transcribe spoken language and respond to well-structured spoken commands.</p><p>The second use-case explored a simple information retrieval example whereby an ASR-enabled system could surface isiXhosa mobile media content in response to spoken queries. We posited that this use-case might reveal a speech domain of entertainment and play <ref type="bibr" target="#b26">[28]</ref>, as well as shedding light on some of the isiXhosa media content that participants store, access, and share on their mobiles (cf. <ref type="bibr" target="#b79">[81]</ref>).</p><p>Finally, we also wanted the workshop to provide space for participants to share their own ideas and potential use-cases.</p><p>4.1.1 Participants. Twelve (six male; six female) isiXhosa-speaking residents of Langa (see Table <ref type="table" target="#tab_0">1</ref>) and the nearby township of Khayelitsha participated in the workshop. All participants were recruited by the local facilitator using the following criteria: frst-language isiXhosa-speaker, mixed age, mixed gender, active user of What-sApp. After obtaining their consent, participants were asked to engage in discussions and activities using the WhatsApp Probe described below. Participants received a data bundle and were compensated an appropriate amount for their time, which was determined by the facilitator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">WhatsApp Data-Collection &amp; Engagement</head><p>Probe. We were inspired by Lambton-Howard et al.'s notion of 'Unplatformed Design' that "frames existing platforms as material, with material qualities" and that platforms like WhatsApp "can be appropriated and designed with" <ref type="bibr" target="#b44">[46]</ref>. Not only does this notion resonate with the material perspective we developed earlier, but it allowed us to leverage the fact that WhatsApp is already installed and used by many people, especially in South Africa and India, thus lowering the barrier to participation. It is furthermore an authentic form of expression -it is what people use to communicate with their friends, family, and community. There is also precedent for using WhatsApp as a data-collection and engagement platform, such as Given the familiarity and authenticity of WhatsApp as a platform, we were eager to explore if the probe would be a viable alternative to more traditional approaches to collecting speech data, such as in-person or in-studio recordings that are increasingly difcult to conduct due to the COVID-19 pandemic. More novel approaches to collecting speech data, such as from interactive voice forums <ref type="bibr" target="#b61">[63]</ref>, were infeasible at scale because of the high cost of voice calls in South Africa. Crowd-sourced approaches have furthermore previously been shown to be better suited to gathering speech data on delineated topics (e.g., recording survey responses, repeating pre-defned sentences, saying isolated digits <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b60">62]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Setup.</head><p>Within our workshop the local facilitator matched six pairs of participants and created a WhatsApp group for each pair named Group 1-6 respectively. In addition to the participant pairs, each group also included the facilitator and a researcher. For the data-collection and engagement activities, we asked participants to create isiXhosa voice recordings and share media items to the group. As part of the consent process, and again before each activity, we reminded participants that all content shared within the WhatsApp group would be transcribed, analysed and used to test and train isiXhosa ASR models.</p><p>Due to ongoing COVID-19 travel restrictions, the workshop was conducted through a Zoom teleconferencing link (see Fig. <ref type="figure" target="#fig_0">2</ref>) to connect co-located researchers in one location with co-located workshop participants and facilitator in Langa. Both researchers and facilitators were responsible for adhering to COVID-19 regulations in their respective places. At the beginning of the workshop we also obtained participant consent and permission to record the workshop. Participants were encouraged to respond in their choice of either isiXhosa or English, and so the facilitator also translated participants' comments during the workshop. Finally, an isiXhosaspeaking note-taker also audited the workshop, with whom we subsequently shared the Zoom recording. After the workshop we exported all group chat content, from which we extracted, transcribed, and translated voice recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Activities.</head><p>The frst two design activities centred around the voice reminder and media retrieval use-cases. Each began with a discussion about participants' current practices, followed by a hands-on activity using the probe where participants would alternate driving the interaction using a voice message shared to their WhatsApp group and with their paired partner subsequently generating a simulated appropriate system response, again using a voice recording. This is an adapted, participatory twist to Wizard of Oz approaches commonly used to showcase and demonstrate novel voice user interfaces <ref type="bibr" target="#b10">[12]</ref>. For the data-collection side, we hoped that pairing queries-for media items or to remind the user of something-with simulated responses would generate a useful, multi-speaker dataset, that was recorded on participants' own devices and therefore exhibits similar audio fdelity and noise to those an ASR system would be confronted with when deployed.</p><p>For the reminder use-case, we began by discussing what tools-if any-participants currently use to remember things, to refect on what sorts of things they need to remind themselves of, and in what language. We then asked participants to use their respective What-sApp groups to create isiXhosa reminders of things they presently want to remind themselves of, or have needed to remind themselves of recently. For instance, "remember to bring an umbrella tomorrow as it will be raining". The paired participant was then instructed to respond as an IPA would, restating the voice reminder: "OK, I'll remind you tomorrow to bring an umbrella". Participants were asked to alternate roles for the next 20 minutes. Before taking a mid-morning break, we asked all participants to create a perfect-aspossible transcription of the most interesting voice note that their partner created, listening as many times as necessary.</p><p>For the isiXhosa mobile media use-case, we asked participants how they fnd isiXhosa content to look at or listen to on their mobiles; what type of content they access (e.g., pictures/memes, audio, videos/GIFs, etc.); and, how they access, store, and share their content (e.g., through links and streaming, or by downloading, uploading, 'Bluetoothing, ' etc.). After learning about their current practices, we asked participants to share examples of isiXhosa media (e.g., a music video from popular hip-hop artist Zanzolo) in their respective groups and for the receiver to then craft a corresponding spoken language query that might return the media item as a result: "may I have Zanzolo music videos". We again asked participants to alternate roles for the next 20 minutes.</p><p>Before breaking for lunch, we asked participants to think about situations where they used voice recordings and voice messaging or wish they had recorded them. After lunch, we engaged in lively discussions on this and other topics, where participants were also encouraged to envisage scenarios and use-cases of their own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Results &amp; Reflections.</head><p>For the reminder use-case, participants reported using a variety of digital and physical tools to help them remember things. On the mobile this was mostly through builtin phone functions (e.g., contacts, calendar, notes) and bespoke reminder and todo apps. One participant described herself as "old school" as she still makes use of paper diaries and not her mobile device to set reminders. Another shared how he records his daily reminders in bullet-points and not full sentences. For example, he would create a calendar entry titled "gym" at a specifc time to remind himself to work out. Eleven of the twelve participants shared that they prefer recording their daily reminders in English; however, participants also agreed that there are times when they use both English and isiXhosa to record their reminders.</p><p>One participant shared that she records her family events and meeting reminders in isiXhosa as it allows her to thoroughly articulate and show respect to the meaning of the event (such as a funeral or circumcision ceremony) set to take place. However, the same participant also stated that it is easier to record community meeting notes in English. In contrast, another participant articulated his clear preference for English:</p><p>Putting a reminder on my diary or phone in isiXhosa means I'm still living in the past. I'm young and I need to move with the times, so my reminders need to be in English. I'm imagining if it says that I'm going to the Eastern Cape in isiXhosa that's kind of embarrassing. I cannot do it in isiXhosa, because I'm young. These discussions demonstrate the contentious terrain that language represents in post-colonial and post-apartheid South Africa (see Sections 2 and 3). The clear preference for English, even for those who sometimes created isiXhosa reminders, also meant that participants did not complete the data-collection exercise that followed in the ways that we anticipated they might. Most participant pairs ended up simply chatting with one another via voice messaging after sending and responding to a few voice reminders. Here we benefted from our 'unplatformed' <ref type="bibr" target="#b44">[46]</ref> WhatsApp probe and the authentic and familiar form of expression it presented to participants. These messages consequently yielded a rich dataset, which the facilitator and note-taker later transcribed and translated into English so all of the research team could review content. An example of one such message is shown in Table <ref type="table" target="#tab_1">2</ref>, demonstrating creative language use by multilingual speakers, who use elements of both English and isiXhosa syntax 3 and phonology 4 as they converse with one another, a process referred to as code-switching <ref type="bibr" target="#b67">[69]</ref>. However, compared to the simpler recordings we were expecting, such as "remind me to bring an umbrella, " these conversational recordings also proved more difcult to transcribe by the workshop note-taker in order to form part of the ASR test dataset (see Section 6).</p><p>The media retrieval discussion revealed that cost of access remains a barrier to online participation. For instance, participants mentioned fnding and sharing funny content on their Facebook feeds and specifc local or isiXhosa-speaking groups. At the same time, the set of practices that surround Walton's apposite term 'pavement internet' (cf. <ref type="bibr" target="#b79">[81]</ref>) remain prevalent. Participants mentioned taking screenshots of jokes or memes-taking the content of the Facebook platform-and sharing them with people or groups of people via WhatsApp. They also download content to their phones when they have access to free WiFi, sometimes supplied through government initiatives. Once content is taken ofine (or if it is created directly on their phones), 25 % of participants said they recently sent or received media via Bluetooth, while 75 % of participants recently used SHAREit, a cross-platform fle sharing app that leverages WiFi connections which is faster, but also less reliable than Bluetooth according to participants. We collected less data during the media retrieval exercises in comparison to the earlier reminder exercise. This was in large part due to participants adhering to the initial task description of sharing and generating spoken queries for isiXhosa media items. Not only did fnding content on their phones take time, but participants also concurrently uploaded that content to the paired WhatsApp groups using the WiFi connection at the workshop venue, which became overloaded. The screenshots, images, memes, music and videos that participants shared pointed at a rich isiXhosa mobile media ecology that exists largely outside of platforms such as Facebook, Twitter, YouTube, TikTok and so on that dominate the hyperdeveloped world <ref type="bibr" target="#b71">[73]</ref>.</p><p>On the topic of recording things in everyday life, participants immediately mentioned voice messaging. All participants reported that they send voice messages every day, without fail. One mentioned that it is convenient way to communicate, as they do not have to worry about misspelling words or whether the person they are sending the message to will understand it. According to the participants, who often return to more rural areas in the Eastern Cape during holidays and for ceremonies, voice messaging is even more widespread there. In this sense, voice messaging was seen as positive and equalising, broadening participation and facilitating communication within the participant's local social circles, but also with family in rural areas.</p><p>Participants also reported that it can be tricky keeping up with the voice messaging activity in larger WhatsApp groups including those local to Langa, where the majority of members send voice (rather than text) messages. Although they appreciated the key beneft of voice messaging, namely that it allows everyone to clearly understand the message, fnding older voice messages proved challenging to participants. We also observed workshop participants often deleting voice messages immediately after sending them, which we later learned was because people wanted to re-record the messages. As there is no way of listening to a voice message before sending it, nor efective ways of fnding older voice messages, this suggests that current voice messaging user interfaces such as WhatsApp do not adequately support the needs of this particular user group.</p><p>Given that our initial use-cases engaged, but did not inspire, participants, we refected with them on the workshop and potential next steps. Overall, participants appreciated the exposure to Zoom and the focus on thinking about application areas of advanced technologies in their context. With regard to ASR application areas, the facilitator remarked after consulting the group of participants: "Voice Messaging is the way to go". Lets say 3 o'clock then at the Clocktower. Listen, look here, I know him, I know him, I got him <ref type="bibr">[INAUDIBLE]</ref>. I will say to him let's go to Waterfront, I won't tell him that I'm meeting a friend, but I know he won't mind when we meet up with you. He will buy us drinks and some lunch then we'll have fun man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Materiality of Mobile Voice vs. Text Messaging</head><p>From this frst workshop we could see how participants are already balancing and leveraging the diferent material qualities of voice messages and text messages: they appreciated how the modality of voice gave everybody equal chance to participate, but conceded that it can be difcult to keep on top of messages, especially in larger groups. Of course, the asynchronous nature of voice messages is itself a property that was the exclusive provenance of written forms <ref type="bibr" target="#b56">[58]</ref> and demonstrates how voice messages are an emerging hybrid form of communication. However, in the mobile domain text still reigns supreme, for it can be edited, searched, copy-pasted; it is data-light, requiring little bandwidth to send or share, and is easier to back-up at low cost. A series of text messages, or a long message, can also be quickly skimmed, which is far harder with audio. Of course, text-input, especially on mobile devices, can be cumbersome.</p><p>However augmenting text-input with AI (e.g., auto-correct, autosuggest, or voice keyboards) can alleviate some of these issues, albeit with varying success <ref type="bibr" target="#b33">[35]</ref> and not for all languages. We could not fnd any research on voice messaging practices of users in the Global South, except for a Brazilian case-study of a publicly available dataset of a large WhatsApp group that was analysed to assess how (mis-)information spreads through the network <ref type="bibr" target="#b49">[51]</ref>. Research on avid voice messaging users in the US has shown how "voice messages can be recorded faster than typing a text message, allow for greater expressiveness, and deeper emotional bonding", but also concedes that they are cumbersome to review and edit, tedious to scan, and inherently public during recording and playback (unless using headphones) <ref type="bibr" target="#b31">[33]</ref>. In the US at least, voice messages carried a connotation as they "shift the efort necessary for communication towards the receiver, when compared to texting", and are a more niche practice reserved for communicating with a low number of intimate friends, family, or partners. In contrast, our participants revealed that voice messaging is not only pervasive, but that they are happy to take on that extra efort required, as it allows everyone to participate. Bidwell et al.'s research, though conducted before WhatsApp released voice messaging, shows how prototypes that support recording and sharing asynchronous voice recordings in a rural South African community, and on a bespoke application running on a communally owned tablet, were especially popular with women, despite men controlling access <ref type="bibr" target="#b5">[7]</ref>.</p><p>Creative appropriations of technology are familiar terrain in South Africa. Although not specifc to South Africa, intentionally missed calls are often used to convey information (e.g., "I'll give you a missed call when I arrive"), but are also used to signal and reinforce connection between intimate contacts <ref type="bibr" target="#b20">[22]</ref>. A uniquely South African phenomena at the time, people leveraged USSD-based 'callbacks' to send highly constrained messages at no cost. Intended by cell phone networks to allow people without airtime to request a return call from a contact who did have airtime credit available, users appropriated the service to send very short messages, such as "ME.N.U.4EVER", which is delivered to the contact's phone as "Please call ME.N.U.4EVER"; a previously unreported and underdesigned-for practice that Bidwell et al. reveal in their eponymous paper <ref type="bibr">[6]</ref>.</p><p>We believe that the pervasive, situationally aware and inclusive practices surrounding voice messaging that participants reported and we engage with here deserve similar attention from CHI. To further that aim and rather than pursuing our initial imagined usecases, we conducted a follow-up workshop focused specifcally on voice messaging and involving a bespoke probe exemplify ASR capabilities in relation to WhatsApp voice messaging. At the same time, we transcribed the data we had collected in order to develop, test and refne an isiXhosa ASR model, reported in this paper (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">WS2: WhatsApp Voice Messaging &amp; ASR Probe</head><p>second Langa workshop centred around a bespoke ASR probe, consisting of an Android app that connects to a cloud service (see Fig. <ref type="figure">3</ref>). After engaging in a series of discussions surrounding voice practices on WhatsApp, we leveraged the probe to allow participants to explore ASR capabilities in relation to their voice messages, a topic we explored collaboratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The ASR Probe. Android app we developed-Voice</head><p>Notes-can receive and process audio data from other apps, including WhatsApp (see Fig. <ref type="figure">3</ref>). When users long-press on a voice message and click the share icon in a WhatsApp group or conversation, they can then select the Voice Notes app from a list of Share Targets that usually includes other common apps, such as Email, SHAREit, Bluetooth, etc.. The app creates a copy of the voice message's content and uploads it to our cloud service. The cloud service then creates an asynchronous transcription request using Google's Cloud Speech-to-Text service using the English (South African) language model, and the transcription results are sent back to the app. Audio recordings are subsequently deleted from the server, but transcripts are stored in a database that is kept synchronised with the app on a per user basis. However, the research team has committed to not look at or analyse the database in order to user privacy. The duration of the entire transcription process is roughly proportional to the length of the submitted voice message. The voice notes with their corresponding transcripts are presented to the user in a scrollable and searchable list, and clicking on an item takes the user to a details screen where they can see the transcript or listen to the original recording, as well as delete and share the voice message and transcript.</p><p>Since the act of sharing a voice message is deliberate, and it is clear to the user at the point of interaction what will be transcribed, no special permissions are required the app. This contrasts with data collection apps other researchers have developed in order to analyse WhatsApp data, which require users to expansive permissions to the entire WhatsApp folder, causing issues with fnding people who will agree to participate given the personal and sensitive nature of chat histories despite data protection assurances <ref type="bibr" target="#b31">[33]</ref>. A drawback of our approach is that only the audio content stream is accessible, and not any metadata of the voice recording, as the date it was created, if it was sent or received, in the context of which conversation or group. This has the knock-on efect that, in order to be able to robustly handle transcription processing and support playback or re-sharing, the Voice Notes app has to create a copy of the recording, which takes up storage space. While we beneft from the authentic form of expression that WhatsApp represents to participants, the drawback of such an 'unplatformed' approach <ref type="bibr" target="#b44">[46]</ref>, is that it's primary use was never a research platform, and obtaining and exporting data is more cumbersome and prone to gaps such as missing metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.2</head><p>Participants &amp; Setup. Twelve (six male; six female) isiXhosaspeaking residents of Langa participated in the second workshop, using the same recruitment process and criteria as the frst workshop. Half of the participants had previously taken part in the frst workshop. All participants were active users of WhatsApp with an Android phone and, after obtaining their consent, were asked engage in discussions and experiment with the ASR Probe app. Participants received a data bundle and were again compensated an appropriate amount for their time, which was determined by the facilitator.</p><p>To adhere to stricter COVID-19 restrictions in South Africa, the workshop was again enabled through a Zoom teleconferencing link to connect co-located researchers in one location with participants and the facilitator in Langa. However, unlike the frst workshop, participants could not be co-located and so each participant nected to the Zoom meeting on their own. Accordingly, before the workshop we organised phone-calls and practice sessions for the participants to familiarise themselves with the Zoom interface and turn-taking (e.g., mute/unmute, raising hand, etc.). At the beginning of the workshop we also obtained participant consent and permission to record the event. Participants were again encouraged to respond in either isiXhosa or English, translated by the facilitator and observed by the isiXhosa-speaking note-taker.</p><p>Participants used their phones-their primary computing and internet access device-to connect to Zoom, and while on the Zoom call could not, for instance, check a WhatsApp conversation to look up the answer to our questions (e.g., how many participants are in a typical group they are a member of, and in what language did they send their most recent voice message?). They also could not use the ASR Probe while on the Zoom call, so after the lunch break we sent video instructions and voice message samples to a WhatsApp group consisting of everyone the Zoom meeting. Although the pragmatics of our remote workshop setup were more challenging than the frst workshop, participants mentioned to us afterwards that they benefted from the experience and could practice and familiarise themselves with Zoom and its unnatural turn-taking mechanics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results &amp; Refections</head><p>Participants reported that voice messages are easier and quicker to send, and that they use voice, rather than text messages when they are in a rush or conveying a message or sentiment that is harder to explain through typing. Instructions and directions were seen as particularly suitable to voice messages, which contrasts to the reported usage habits of US-based users <ref type="bibr" target="#b31">[33]</ref>. Tone and emotion were discussed -through voice messages participants could for example more clearly express anger, or whisper if they were gossiping. While recipient design <ref type="bibr" target="#b27">[29]</ref> had the strongest infuence over choice, participants mostly used isiXhosa in their voice messages and usually only reverted English if the recipient could not understand isiXhosa, or they wanted to keep the conversation short; isiXhosa sentences are often three times longer than their English equivalents <ref type="bibr" target="#b5">[7]</ref>. This was especially the case when communicating with family, although one participant reported mixing languages in a family group chat. Another participant also reported additionally sending SeSotho and SeTswana to friends.</p><p>Although many voice messages fade away into the chat history, never be listened to again, all participants reported that they do revisit some voice messages, either 'in the moment' to check that the voice message they just sent contained all the right information, or to revisit older content. For example, participants reported recording and later relistening to meetings or church sermons through voice messages. They might also reference older voice messages if someone backtracks on what they said, or there is a need to check particular directions or instructions. One participant regularly keeps the voice messages of the person they are dating and re-listens to them again as a reminder of their voice.</p><p>To fnd an older voice message requires persistence: "I go to my WhatsApp chat and scroll up and keep listening to all the messages until I locate the one I'm looking for". Another participant lamented that "fnding a VN is so tough because I have to scroll all the way up, past everyone's messages to fnd that one VN". They agreed that remembering the approximate date and sender helps locate message. Another strategy employed is to search for a text message that was sent at around the same time as the voice message to at least be able to jump to a more promising starting point and continue their search from there. Those that are shared a group with many voice messages are far harder to fnd; participants agreed that it is not worth the efort required fnd voice messages older than about three months. Participants also need to routinely clear space on their phone to make space for new content, and often cannot aford the cost of cloud backups (or the required internet connectivity), so older voice messages may have already been deleted. One tech-savvy participant reported a creative workaround, where he accesses voice messages through the 'Files' app on his phone. From here it is easier to sort all voice recordings date and listen to them. Once the date has been identifed, he returns to WhatsApp to select the correct message in order to, for instance, reply to it or hold someone to account.</p><p>Over we asked participants to install the Voice Notes ASR on their phones. We also sent video recordings introducing the app, how it works, and the data that it collects, emphasising that we do not listen to the voice messages, nor read message transcripts generated by the ASR system. We also sent sample voice messages to the WhatsApp group and encouraged participants to practice with transcribing these examples. We then asked participants to experiment transcribing some of their own English messages before reconvening Zoom workshop which participants again accessed using their phones.</p><p>Participants felt that it would be useful if the app could translate between diferent languages of South Africa, and expressed some interest in automating transcriptions within WhatsApp. They also enquired about how much data the app requires, which we responded that it was about the same amount as sending or receiving a voice message when initially transcribing, and negligible amounts to refresh the list of transcripts. Refecting on how the app might help them, participants immediately recognised the value of being able to 'listen' to a voice message privately by looking at the transcript rather than listening when in a public setting. Others mentioned that by looking at the transcript of a voice message that they had just they could see if they made a mistake or transcript could also be for someone who does not understand word-either because it is difcult to hear or the meaning is unknown-they could see the word clearly in the transcription and understand it better look up its meaning elsewhere. Multiple participants suggested that the app could furthermore be helpful when people have unfamiliar accents that were hard to understand or follow.</p><p>Given the current state-of-the art of ASR systems, we might dismiss many of these remarks as unrealistic, and focus only on the use-case that is nearest at hand: to support looking at a voice message while in public. However, we pause and look at these comments through a diferent lens, we can see that they are also visions of future AI capabilities that are similar to the rhetoric, hype, and promise that surround many contemporary AI products and services, which also extend far past their current capabilities <ref type="bibr" target="#b25">[27]</ref>. We might instead, draw inspiration from allied eforts to diversify future-making <ref type="bibr" target="#b57">[59]</ref> or reconstitute utopian visions in other domains of computing <ref type="bibr" target="#b47">[49]</ref>, and recognise the importance of engaging with specifc, local instances that taken together can create global forms.</p><p>To highlight the research that needs to be done at the intersections of HCI &amp; ASR research, we deliberately juxtapose these ambitious proposals for the ASR-enabled systems that participants articulated with our comparatively modest eforts to develop and put ASR-enabled technologies in the hands of people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DHARAVI STUDY &amp; MARATHI ASR SYSTEM</head><p>While we concurrently developed an isiXhosa Language Model (see Section 6), we drew on a longstanding community partnership with Marathi-speaking residents in Dharavi to provide a comparative perspective. Scholars of everyday (e.g., <ref type="bibr" target="#b45">[47]</ref>) have long recognised value of comparative study and assessment, particularly for phenomena, such as language <ref type="bibr" target="#b16">[18]</ref>, chat <ref type="bibr" target="#b51">[53]</ref> or digital infrastructures <ref type="bibr" target="#b22">[24]</ref>, that are largely taken-for-granted. We took the opportunity to deploy the best performing Marathi model <ref type="bibr" target="#b43">[45]</ref> that was developed the low-resource Indian language challenge at MUCS 2021 <ref type="bibr" target="#b19">[21]</ref> in a real-world setting. Following the principles of the 'itinerative design' methodology <ref type="bibr" target="#b57">[59]</ref>, which demonstrates the benefts of pivoting between diferent communities in the context of a research project, we did not repeat all phases, but picked up the line of research from its current point in South Africa, albeit at a slightly smaller scale. So we also invited Marathi-speakers in Dharavi to refect on their voice and text messaging practices and experiment with the ASR Probe, confgured with a Marathi ASR system based on the MUCS competition-winning Marathi model. The diferentiating factor of that particular language model is that it augmented training data supplied by the competition with crawled data from YouTube improve performance <ref type="bibr" target="#b43">[45]</ref>. This technique was successful for Marathi, not only because there are 100 more people that speak Marathi than isiXhosa, but also because mobile data is an order of magnitude more afordable in India ( $2.50 per month for 42GB of prepaid data) than in South Africa ( $19 per month for 3GB of prepaid data). While the low-resource moniker refers to the dearth of readily available transcribed training data, everyday voices-in the form of YouTube video, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Participants &amp; Method</head><p>Ten male, fve female; see Table <ref type="table" target="#tab_2">3</ref>) Marathi-speaking residents of Dharavi participated in this study, recruited through a community liaison who also determined an appropriate participation compensation. As the study was conducted remotely, we individually gave participants a brief about the research and demonstrated the use of the Voice Notes app using a video. After watching the video, an 11th participant who had initially volunteered decided not to proceed the study; the remaining ten participants consented. We conducted phone-based semi-structured interviews about voice note practices lasting approximately 20 minutes, then shared the Voice Notes app and assisted participants to install it. We asked participants to experiment with the Voice Notes app for a period of one week and reminded them that their voice messages and transcripts are entirely private, unless they choose to share them with the research team during subsequent interviews or by forwarding individual voice messages and transcript screenshots via WhatsApp. A week later we followed with participants to ask about usage impressions and refections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Initial Interviews</head><p>Initial interview questions revolved around mobile voice and text messaging practices and language preferences. Speed and ease of use were themes that participants immediately recognised: e.g., "in one minute we can talk a lot more than if we type text" [M4]. However, one participant this also intertwined with being inclusive: "my uncle and sister are not educated -I don't get a reply fast if I send typed messages. I do get an immediate reply if I send a voice note" [F1]. Installing, confguring and typing on a Marathilanguage keyboard and/or switching between diferent languages on Indic script keyboard requires more advanced digital skill-sets. Other participants reinforced the notion that voice messages remove the barrier of text input (cf. <ref type="bibr" target="#b3">[4]</ref>)-"my father has a mobile recharging shop -many of his customers are illiterate, so he uses voice notes"-but also recognised even voice messages require some digital skills: "many people don't know how to use voice notes in WhatsApp -they touch [the record button] and leave it. [They] don't know the idea of long press" [F3]. Another participant remarked that people better understand voice messages, or they send voice notes "when I need to precisely communicate messages with colleagues" [F4]. Finally, participants also reported issues fnding salient information within voice messages compared to text messages -for instance, "recently during the [COVID-19] vaccination drive, I received voice note with the gate number of the hospital where the vaccination centre was set up, so I had to search a to fnd the gate" [F3].</p><p>Participants also refected on how they dynamically negotiate whether to converse in Marathi or Hindi. For a nascent conversation might begin in Marathi, but then shift to Hindi if the Marathi-speaker detects that their partner is more comfortable speaking Hindi. The opposite phenomena also occurs, whereby a conversation begins in Hindi but then is continued in Marathi if the participants realise that both are native Marathi speakers. Further, while Dharavi is overall culturally and linguistically diverse, living areas are often grouped along linguistic lines, so Marathi-speaking residents will generally prefer to speak Marathi especially in their home environments and only revert to Hindi to accommodate others <ref type="bibr" target="#b62">[64]</ref>. However, a language is settled upon, this tends to remain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ASR Probe Results</head><p>Participants generally saw the value of accessing transcripts. A participant who works as an educator said that "as the classes are now online, being a primary school coordinator, I am overwhelmed voice notes from children" [F5]; along with another participant, they asked if we could keep the app running beyond the trial period, which we agreed to. The educator also refected on how she currently records voice messages for her pupils that she then sends to their parent's mobile phone. However, also creates a short text message summarising the content, which she hopes will ensure that busy parents won't miss the message, but still allows children to beneft from the increased accessibility of the voice message. Listening discreetly was also seen as valuable. For instance another participant remarked: "I was in a meeting and got a message from my supervisor. I use the app to check what is in the voice recording" Multiple participants used the term 'dictionary, ' suggesting ideas about how to improve the app. They recognised most commonly-used words are correctly transcribed, but certain words that are not quite mainstream, though still commonly used avi, are "not-so-common words like 'Sahishnuta' [Marathi:</p><p>are not available on the app; only very frequently used words are available". Accents and pronunciations also afected transcription accuracy according to -for instance, one participant said that they tried re-creating a voice message: "if I try to speak slowly with the right enunciation, works perfectly" [M2]. Another participant remarked that transcriptions were of better quality for short instructions in comparison to a longer voice message containing a reading from a poem that was not transcribed successfully.</p><p>Finally, one participant requested a way of recovering of the metadata that is lost when the voice message is moved between WhatsApp and the Voice Notes app: "if I can add a keyword a note to the transcribed fle then it would be very helpful, as I won't be remembering the text content while searching for it weeks later, but I will remember the person or keyword have added" [F2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">THE ISIXHOSA ASR MODEL &amp; LANGA TRIAL</head><p>Concurrent to the Dharavi our multidisciplinary team, led by ASR researchers and language experts, developed an isiXhosa ASR system. ASR development was guided by the fndings from the Langa workshops and the high-level goal of supporting the transcription of WhatsApp voice messages, and the need to cope with everyday speech -that is, fast-paced, conversational and code-switching speech that refects real 'language-in-use' <ref type="bibr" target="#b17">[19]</ref>. It is important to acknowledge here however unexpected lenges (discussed henceforth) meant that the pace of initial Xhosa model development was slower than expected: such the model we deployed Langa for the trial was not necessarily one that yet completely fulflled all of these capabilities. We nevertheless expand upon the details of this deployed baseline model and, importantly, dissect the challenges we met surrounding data and transcriptions which led to the discrepancy between baseline and goal system. We also ofer refections on this challenge, which illuminate further areas of research at the intersection of HCI/ASR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline isiXhosa model</head><p>While precise technical ASR details are beyond contribution's scope, for readers with an ASR background, the isiXhosa ASR model is a 'hybrid' one that uses a factorised time delay neural network (TDNN) <ref type="bibr" target="#b58">[60]</ref> and is built using the Kaldi toolkit <ref type="bibr" target="#b59">[61]</ref> with MFCCs and iVectors as input features and without grapheme-to-phoneme conversion. Hybrid models were chosen over models because the latter are known to struggle with limited amounts of data, and are notably more black-box-like, it is harder to adapt or tune individual parts to the use-case.</p><p>To train our isiXhosa model we utilised the NCHLT SA speech corpus [5] which, just like the data for the model <ref type="bibr" target="#b43">[45]</ref>, consisted of 50 hours of read speech. dataset has previously been used in the isiXhosa ASR literature (cf. <ref type="bibr" target="#b7">[9,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b73">75]</ref>). While the NCHLT data matches our use-case in terms of 'recording environment' (recorded a phone), read speech generally slower-paced and difers extensively from the more informal, fast-paced conversational speech that one would expect from voice messaging between community members, and also does not feature code-switching. Necessarily, then, this places a ceiling on what kind of quality of acoustic model can be achieved training on read data-sets, which also lack natural intonation and prosody (changes in pitch or loudness) or coarticulation efects (where speech sounds are afected by those that precede or follow it) common to spontaneous or continuous speech. Furthermore, the dataset we collected during the Langa workshop was comparatively modest in size (see Table <ref type="table" target="#tab_3">4</ref>), so we decided to use it as testing rather than training data. Finally, although it might have been possible to tweak existing models of another language within the family of Bantu languages (e.g., isiZulu) to isiXhosa, we did not try this approach as we did not have access to a high-quality isiZulu model (those available, such as Google's, are not open); and, our attempts to incorporate data from the 'nearby' isiZulu and Sesotho languages had previously failed to improve model performance.</p><p>The mobile media samples we learned about during the frst Langa workshop and that Walton refers to as content that is traffcked through the pavement internet (cf. <ref type="bibr" target="#b79">[81]</ref>) are the exception rather than the rule in publicly-available isiXhosa content. Namely, what isiXhosa content that is available online is not refective of actual language use <ref type="bibr" target="#b16">[18]</ref>, and the little publicly-available supervised data that has been previously collected for low-resource languages is often mismatched in to real-world use cases. Consequently, augmenting supervised data through Commoncrawl 5typically a useful source of language-specifc text data-or by scraping YouTube for raw audio data revealed only similarly mismatched data but was also particularly noisy for isiXhosa, as it features a lot of 'junk' and out-of-language content.</p><p>The challenges we encountered developing the isiXhosa Language Model (LM) are representative large of the issues ASR researchers encounter when working on low-resource languages. For instance Both isiXhosa and Marathi exhibit linguistic properties that are not found in the 'high-resource' languages for which ASR systems are typically developed. is, models or approaches labelled 'state-of-the-art' when recognising English may not be directly applicable to low-resource languages without considerable re-working, re-tuning or even re-imagining of entire architectures. For instance, in isiXhosa the letters 'q', 'k' &amp; 'c' are pronounced as diferent clicks and contain phonemes and sounds that are considerably diferent in articulatory features from any phoneme found in English. It is unclear how such features afect model performance and how they might be accounted for.</p><p>Both isiXhosa and Marathi also exhibit agglutinative morphology <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b42">44]</ref>: what would often constitute separate words in, say, English are strung together to form longer words. Because of this, the isiXhosa model we created had to be modelled at the sub-word rather than level. There is a fairly rich inventory of 'code-ASR literature <ref type="bibr" target="#b67">[69]</ref>. However, of the earlier work focuses on languages for which large 'code-switch datasets' could be gathered (e.g., Mandarin-English <ref type="bibr" target="#b77">[79]</ref>), or utilises existing linguistic tools (syntactic parsers; part-of-speech taggers) for both languages <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Others have exploited 'translation'-based methodologies to some success; however, methods rely on the existence of translation dictionaries <ref type="bibr" target="#b9">[11]</ref> or large parallel corpora for the two languages <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b82">84]</ref>. All these methods then are somewhat at odds with our overarching to develop approaches that could work across 'low-resource' contexts. And whilst many recent papers on code-switching ASR have advocated 'end-to-end' approaches (e.g., for Mandarin-English <ref type="bibr" target="#b81">[83,</ref><ref type="bibr" target="#b84">86]</ref>), these too are known to be unstable when applied to smaller datasets that characterise 'low-resource' This 'linguistic-property' discrepancy extends to the level of language-in-use too; consider Section 3's discussion of "everyday multilingualism", and Section 4's frst-hand evidence of such 'code-switching' naturally occurring in isiXhosa speech. If we are to produce an ASR model which recognises language as it is actually spoken in voice messages, then, this switching between languages at the sentence or even word level (cf. Table <ref type="table" target="#tab_1">2</ref> 'eClocktower') must be accounted for. But understanding how to model this code-switching in an ASR model is not trivial. Our isiXhosa model currently employs only a very basic code-switching solution:</p><p>we trained separate English LMs and isiXhosa LMs on monolingual data and interpolated. This approach will not adequately model the nuances of when switches between languages are more likely to occur; for this, we really must consider more subtle linguistic and structural factors, and also the historical and socio-cultural aspects (cf. Section 4's discussion on English being more for the 'young'). Especially in post-apartheid South Africa, the unequal power namics between languages, who speaks them, and where they are spoken also play into what vocabulary taken from which language. We can therefore already posit that our Marathi model will perform better on account of code-switching being less prevalent in Dharavi.</p><p>We had initially hoped that the Langa workshops would provide a rich 'in-context' data-set that we could utilise as a robust test data-set to evaluate against. However, we found that though audio from the workshops was collected successfully (see Table <ref type="table" target="#tab_3">4</ref>), obtaining accurate transcriptions for this data was a lot harder than expected (due to the more conversational nature of the discussion; participants' non-familiarity with the task of 'transcribing'; a lack of clarity regarding exactly what was being asked of the participants with regards to producing transcripts). Recall that we had paid the workshop note-taker to take on this task, but we were expecting to produce simpler and voice messages containing media queries and reminders. The conversational and code-switched voice messages participants generated instead are much more challenging to transcribe on a spreadsheet and using a phone-based media player designed for music playback. We thought that the beneft of familiarity of using existing tools would outweigh the challenges, but in retrospect, we wish we could have supported the person transcribing the data more by scafolding the task and combining better tools. We have no doubt that the transcriptions and translations we received refected the meaning of what people said. However, contexts where codeswitching occurred, or when people repeated words or restarted a sentence, transcriptions were not verbatim enough.</p><p>Unfortunately, we only discovered issues with participantgenerated testing data quite late on in the development process: after having already begun model tuning based on the initial provided transcripts, a phenomena that Sambasivan et al. <ref type="bibr" target="#b63">[65]</ref> refer to as a data cascade. Once we realised that these transcripts did not necessarily refect ground-truth, this cascaded to word-error-rate (WER) calculations and the decisions that are made in response to these WERs. This made it harder to assess whether our sub-word modelling techniques were better than our word modelling techniques; whether the 'noise' from our Commoncrawl data-set was impeding overall speech recognition accuracy. We returned to the person transcribing the data to ask if they could review the output. We empathised that the task was more challenging than we envisioned and how it might be demotivating to revisit work. We also ofered more tips and techniques and carefully explained the need for precisely verbatim transcriptions in more detail using examples from the dataset. Finally, we suggested that transcription accuracy is more important than quantity. So, we ended up with fewer transcripts to tune the isiXhosa model than we had initially expected. deployed isiXhosa ASR system (tested using the limited retranscribed data-set) had a WER of 87.52 and character-error-rate (CER) of 40.7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Langa Refections</head><p>We debated within the team if it was worth deploying the isiXhosa system, given the shortcomings we already identifed, and the poorer performance compared to the Marathi model. However, we quickly recognised we are indebted to participants to showcase the modest progress we had made and to invite formative feedback and refections. Consequently, we deployed the isiXhosa model to the Voice Note app and server and asked participants from the second workshop to experiment with it. Unsurprisingly, they found that the isiXhosa system was prone to errors, particularly for the "informal isiXhosa" that participants use from time to time. Participants also noticed "a few typos with spelling", an issue which relates to isiXhosa's agglutinative morphology that necessitated sub-word modelling. Typos therefore can emerge when the isiXhosa model incorrectly transcribes a sub-word prefx, but correctly transcribes the main word. Given the sociolinguistic norm of correctly-spelled and unabrreviated isiXhosa <ref type="bibr" target="#b17">[19]</ref>, at least in its written form, we were glad to have positioned the Voice Note app as a helpful tool for an individual rather than developing a bot that, say, transcribes voice messages in a WhatsApp group automatically and publicly as suggested the Langa workshops. This choice resonates with Shneiderman's that position Human-Centred AI systems as powerful tools to give users a high degree control rather than only seeking automation <ref type="bibr" target="#b66">[68]</ref>.</p><p>Participants also identifed false positives with English codeswitching: "it somehow transcribed some words in English even though no English was used". Here isiXhosa phrases were being transcribed using similarly sounding, but in the context nonsensical, English words. When asked if it would be better remove the ability to transcribe English and focus purely on isiXhosa, the participant emphasised that tend to mix English and isiXhosa" and that worth supporting both languages. Another participant agreed, "we tend to use both However more work to be when the switch happens". Participants reported enjoying experimenting with advanced AI capabilities in their mother-tongue, which one participant thought was "pretty cool" and prompted him to identify other opportunities for NLP tools that could be useful for him -for instance to translate an isiXhosa message into SeSotho to facilitate communication with a Sotho person.</p><p>Participants also identifed audio segments where the system performed well and saw value and purpose beyond the shortcomings of the ASR prototype's current implementation. They also encouraged us to improve the system. Such comments, are generally treated suspiciously within the feld of HCI4D research as a form of response bias <ref type="bibr" target="#b15">[17]</ref>. However, community members did not hesitate to critique a prototype implementation in an unrelated project that embody ideas that could only be rudimentarily, similar to our ASR prototype. to our longstanding engagements with the community and the facilitator's skill, we are therefore inclined to take these comments at face value. In contrast to the critiqued prototype, the area where the ASR prototype succeeded was integrating content that is close to participants' everyday lived experience; that of communicating through voice messages on WhatsApp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY, RECOMMENDATIONS AND OUTLOOK</head><p>Across the two deployments, participants in Langa and Dharavi demonstrated how switching between voice and text modalities can augment and support pervasive voice messaging practices. In Dharavi, where commercial ASR systems already support some Indian languages, participants mentioned using the voice typing featuring of the keyboard built into their Android phone. To be sure, readers might have similarly used ASR technologies to send hands-free messages (for instance while driving or otherwise occupied); or, perhaps, have experienced the frustration (or humour) when the ASR system does not get the transcription quite right. Our research shows the benefts of combining (and switching between) voice and text modalities when messaging. Given that all ASR systems are blighted by recognition errors to some extent, sending the audio recording the 'voice-typed' message could alleviate user frustrations more broadly -that is, beyond the specifc communities in Langa and Dharavi we partnered with. Or, vice-versa, Langa participants mentioned, being able to access transcriptions of voice messages allows one to discretely 'peek' at a voice message's content, for instance while using public transport, but still be able to later appreciate the tone and emotion of the message, which is better conveyed speech. Novel commercial video/podcast editing software such as Descript 6 already demonstrates the creative and collaborative potential enabled by pivoting between textual and spoken representations of content. However, low-resource languages are currently unsupported, and marginalised users-who also typically lack access to PCs-would be unable aford such a subscription service. We recommend further research and inclusive interface innovations targeting, the integration of voice and text modalities. At a more basic level, we recommend allowing users the option to preview a voice message before sending it, not only to conserve precious but also to avoid potentially embarrassing social situations if a voice message was already listened to before it could be deleted or edited by the sender.</p><p>Although ASR literature groups together isiXhosa and Marathi languages under the low-resource moniker, much like HCI4D research groups together users from Dharavi and Langa using the term 'emergent user' <ref type="bibr" target="#b18">[20]</ref>, our research further shows the importance of attending and accounting for the nuances of the language-in-use and media ecologies of the specifc communities we engage with. Not only are more language resources available for Marathi ASR development than isiXhosa, but the history and geography of Dharavi means that rather than code-switching between Marathi and Hindi or English, participants in our studies tended to speak Marathi with their friends and family and only fell back onto Hindi if sensed that their conversational partner didn't understand them. Consequently, Dharavi participants not only benefted from a more robust Marathi ASR system, but that system did not have to cope with code-switched conversation.</p><p>We are now investigating other methods to improved codeswitched modelling; in particular, to better model likely 'switch points' from isiXhosa to English and back again: see, for instance, the data augmentation method technique discussed in We have recently identifed an additional source of more time natural, code-switched isiXhosa-English training data, sourced from South African soap opera corpus <ref type="bibr" target="#b54">[56]</ref>. This provides a further 2.68 hours of training data with a mixture of English utterances, isiXhosa utterances and intra-sentential code-switched (i.e., switching between languages within a sentence) isiXhosa-English utterances. Particularly apt at illustrating the diference between this and the NCHLT data-set we used for the work reported here is the speaking-rate (phones-per-second), which is 8.77 in the read NCHLT corpus [5] compared to 19.98 for the Soap Opera corpus <ref type="bibr" target="#b54">[56]</ref>.</p><p>In 'low-resource' contexts, such linguistic challenges are exacerbated by the paucity of representative accurately transcribed data. The 'unplatformed' <ref type="bibr" target="#b44">[46]</ref> approach we took in our research efectively engaged participants and generated a dataset of authentic, representative conversational speech from participants. Within HCI it is common practice to co-create user interfaces, scenarios and use-cases to ensure that new technologies address the needs of people. Our research demonstrates that collaborations across HCI and NLP that emphasise community engagement can, feasibly, also generate invaluable datasets: we recommend that future research in this space co-create use-cases, interfaces and datasets.</p><p>With our emphasis on ethical and transparent data-collection, we did not enlist the help of professional transcription services, which came at the expense of transcription accuracy. Increasingly, calls to decolonise speech and language technology not only draw attention how labour-intensive, incomplete and theorythe transcription process is, but just as importantly, "take seriously the sovereignty local people over data" <ref type="bibr" target="#b6">[8]</ref>. Where our work fell short is providing better support for the transcription task when generating new datasets. So we furthermore recommend to assess transcription quality early to avoid data cascades <ref type="bibr" target="#b63">[65]</ref> and to develop inclusive tools to enable communities to generate their own high-quality transcripts, retaining data sovereignty. research in this space has shown that mobile-friendly transcription tools, such as Respeak <ref type="bibr" target="#b75">[77]</ref>, Recall [76] and BSpeak <ref type="bibr" target="#b76">[78]</ref>, simplify the transcription task to the beneft and empowerment marginalised and excluded communities. However, currently require users to iteratively either read a sentence or listen to a spoken audio segment and subsequently clearly 're-speak' it, which a welltrained ASR system subsequently transcribes. Such tools not only present ASR development with a chicken-and-egg problem, but also have not been designed or evaluated with code-switched data in mind. We recommend that future ASR/HCI research develop friendly, inclusively designed, and code-switching compatible tools to generate the high-quality transcripts that are imperative for building robust ASR system.</p><p>It may be a stretch to think that such data-collection and transcription approaches are scaleable enough to, on their own, generate the supervised data required for hybrid ASR methodologies (and those that similarly tune models to take linguistic knowledge and contextual insight into account more generally). However, 'low-resource' languages are not 'zero-resource' languages, so it is often possible to leverage existing datasets. The advantage of a co-created, authentic, and representative dataset in this context is invaluable during ASR development and can guide the myriad of small but consequential decisions and trade-ofs that are made along the way <ref type="bibr" target="#b25">[27]</ref>. Here our research demonstrates that even modest amounts of testing data are already useful, but we also stress the importance of accurate transcription to avoid data-cascades <ref type="bibr" target="#b63">[65]</ref>.</p><p>ASR approaches such as ours have recently fallen out of favour, compared 'unsupervised' end-to-end approaches and larger multilingual speech models, which rely on very high volumes of data. Of course, high volumes of data require equally high (and expensive) computational resources, so more popular 'state-of-the-art' approaches are increasingly becoming the exclusive provenance of large organisations able to aford the cost of computation and data, or are otherwise able to collect or extract it <ref type="bibr" target="#b11">[13]</ref>. We believe, are working towards, critical alternatives to this approach that could enable smaller entities-such as universities, smaller organisations embedded into communities with specifc ASR use-cases, or someday perhaps communities themselves-to develop their own ASR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>Through our technical and creative methods to engage communities and develop and deploy ASR models of low-resource languages we have demonstrated opportunities and challenges of ASR system development. Most notably these revolve around the pervasive WhatsApp voice messaging practices of two communities of marginalised users in South Africa and India. Here, ASR-enabled systems have the potential to reduce information overload <ref type="bibr" target="#b32">[34]</ref>, broaden digital participation <ref type="bibr" target="#b3">[4]</ref>, and aford people enhanced opportunities to (re)discover and retrieve older voice message content, a form of digital possession <ref type="bibr" target="#b55">[57]</ref>.</p><p>Our research further reports on a series of creative voice messaging practices, such as combining both text and voice messages, and we call for further studies of such practices across the Global South.</p><p>Our research also aligns with rare but critically important scholarship on the challenges of developing high-stakes AI systems in the Global South <ref type="bibr" target="#b63">[65]</ref>, and shows that key insights, such as how low-resource contexts have "a pronounced lack of readily available, high-quality datasets" and the challenges of taking on such 'data work' also apply to developing ASR-enabled systems for lowresource languages and in postcolonial contexts <ref type="bibr" target="#b6">[8]</ref> characterised by linguistic inequalities <ref type="bibr" target="#b16">[18]</ref>.</p><p>Refecting on our research makes us now think of data, in both spoken and transcribed forms, as a boundary object <ref type="bibr" target="#b68">[70]</ref> rather a hand-over point <ref type="bibr" target="#b71">[73]</ref> that can bind together the concerns of diferent communities of research-in our case HCI and critically also extends into and engages with actual communities. Such collaborations have a role to play in future, for instance to better support collecting high-quality testing data of language-inuse annotated by accurate human transcriptions that also ensures that human eforts are leveraged to their fullest potential.</p><p>We are also mindful of critical AI commentators, such as Kate Crawford, whose "Atlas of AI" reconfgures AI as an industry that extracts and abstracts data away from the material conditions and the relationship it has with people and place <ref type="bibr" target="#b11">[13]</ref>. Our attempts to augment data through commonly-used web-scraping techniques is an example of such practices which, as we reported, also less efective for isiXhosa. However, recurring theme identifed by both Langa and Dharavi users of our ASR probe is a desire to 'listen' privately and discreetly by reading voice message transcripts. This demonstrates an intimate and sensitive relationship that users have to their voice messages. Furthermore, participants felt comfortable sharing voice message samples with us as part our longstanding engagements with their communities. Here we can draw inspiration from the anthropologist Tim Ingold, who reminds us that the original meaning of 'collecting data' is to receive something that is given or ofered, and not extracting what was not <ref type="bibr" target="#b35">[37]</ref>. Of course, such acts implicate both giver and receiver in the norms, expectations obligations of social life writ large as Marcel Mauss argues so beautifully in his seminal essay "The Gift" <ref type="bibr" target="#b50">[52]</ref>. We experienced such obligations frst hand, as we grappled with our decision to the imperfect isiXhosa model. The clear recommendation of our work, then, is to make possible a future of ASR-enabled impacts through multidisciplinary collaboration and community partnership relies more on data excellence and data ethics than data mining or scraping.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The remote setup used during Workshop 1, with researchers shown on the left and workshop participants and facilitator to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure fow of the ASR Probe: users select a WhatsApp voice message (a1), tap the share icon (a2), and (b) select the Voice Notes app. (c) Users then select a transcription language: (Langa Workshop), isiXhosa (Langa Study), or Marathi (Dharavi Study). The app then (d) uploads and (e) transcribes the voice messages.</figDesc><graphic url="image-4.png" coords="9,53.80,82.67,504.42,172.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,53.80,306.75,504.43,139.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Workshop 1 isiXhosa participant demographics.</figDesc><table><row><cell cols="5">ID Gender Age ID Gender Age</cell></row><row><cell>M1</cell><cell>Male</cell><cell>26</cell><cell>F1 Female</cell><cell>39</cell></row><row><cell>M2</cell><cell>Male</cell><cell>41</cell><cell>F2 Female</cell><cell>38</cell></row><row><cell>M3</cell><cell>Male</cell><cell>24</cell><cell>F3 Female</cell><cell>21</cell></row><row><cell>M4</cell><cell>Male</cell><cell>33</cell><cell>F4 Female</cell><cell>43</cell></row><row><cell>M5</cell><cell>Male</cell><cell>22</cell><cell>F5 Female</cell><cell>20</cell></row><row><cell>M6</cell><cell>Male</cell><cell>21</cell><cell>F6 Female</cell><cell>36</cell></row><row><cell cols="5">4.1 W1: Exploring Use-Cases &amp; Data-Collection</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Original transcription and translation of a WhatsApp voice message showcasing dynamic and creative code-switching between isiXhosa (upright text) and English (italic text).</figDesc><table><row><cell>isiXhosa/English Transcript</cell><cell>English Translation</cell></row><row><cell>Masithi 3 o'clock ke eClocktower. Mamela kyk hier ndiyamazi I know</cell><cell></cell></row><row><cell>him, I got him [INAUDIBLE]. Ndizithi kuye masiye eWaterfront I</cell><cell></cell></row><row><cell>won't tell him that I'm meeting a friend, but ndiyayazi he won't mind</cell><cell></cell></row><row><cell>xasidibana nawe. He will buy us drinks and some lunch then sonwabe</cell><cell></cell></row><row><cell>wethu.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>demographics.</figDesc><table><row><cell cols="5">ID Gender Age ID Gender Age</cell></row><row><cell>M1</cell><cell>Male</cell><cell>58</cell><cell>F1 Female</cell><cell>36</cell></row><row><cell>M2</cell><cell>Male</cell><cell>31</cell><cell>F2 Female</cell><cell>28</cell></row><row><cell>M3</cell><cell>Male</cell><cell>34</cell><cell>F3 Female</cell><cell>32</cell></row><row><cell>M4</cell><cell>Male</cell><cell>48</cell><cell>F4 Female</cell><cell>35</cell></row><row><cell>M5</cell><cell>Male</cell><cell>21</cell><cell>F5 Female</cell><cell>32</cell></row><row><cell cols="5">or user-generated text and speech content-are much better rep-</cell></row><row><cell cols="5">resented in online spaces. The deployed model was</cell></row><row><cell cols="5">not tweaked further, had a word-error-rate of 15.79, and produced</cell></row><row><cell></cell><cell cols="3">as its output [45].</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Summary of testing dataset collected from the Langa due to the low accuracy of initial transcripts we do not have all details for row 1.)</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Utterances per recording Total tokens Mixed/code-switched</cell></row><row><cell>Langa dataset</cell><cell>25.02</cell><cell>118</cell><cell>-</cell><cell>-</cell></row><row><cell>Re-transcribed subset</cell><cell></cell><cell>36</cell><cell>577</cell><cell>17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here and elsewhere in this contribution we adopt the ASR term 'low-resource languages' to draw attention to the dearth of publicly-available data and to build bridges to ASR researchers working in that area.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://techcrunch.com/2021/01/11/youtube-and-whatsapp-inch-closer-to-half-abillion-users-in-india/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Structural properties of language, such as arrangement of words and phrases to create well-formed sentences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">'22, April 29-May 2022, New USA Reitmaier, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://commoncrawl.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.descript.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Minah and Manik as well as the workshop participants in Langa &amp; Dharavi for their contribution to this work. This work was supported by Engineering and Physical Sciences Research Council grants EP/T024976/1 &amp; EP/M022722/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdsourcing Speech Data for Low-Resource Languages from Low-Income Workers</title>
		<author>
			<persName><forename type="first">Basil</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Siddarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Jyoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.lrec-1.343" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the 12th Language Resources and Evaluation Conference</title>
				<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2819" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Syntactic and semantic features for code-switching factored language models</title>
		<author>
			<persName><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="431" to="440" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent neural network language modeling for code switching conversational speech</title>
		<author>
			<persName><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8411" to="8415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making and Breaking the User-Usage Model: WhatsApp Adoption Amongst Emergent Users in India</title>
		<author>
			<persName><forename type="first">Devanuj</forename><surname>Balkrishan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudha</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandni</forename><surname>Rajendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedof the 8th Indian Conference on Human Computer Interaction (IHCI &apos;16)</title>
				<meeting>eedof the 8th Indian Conference on Human Computer Interaction (IHCI &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Nazreen Nizam, Chinmay Parab, and Sujit Devkar</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The NCHLT speech corpus of the South African languages</title>
		<idno type="DOI">10.1145/3014362.3014367</idno>
		<ptr target="http://www.isca-speech.org/archive/sltu_2014/sl14_194.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the Tenth International Workshop on Internationalisation of Products Systems. Kutching</title>
				<editor>
			<persName><forename type="first">Nicola</forename><forename type="middle">J</forename><surname>Bidwell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gary</forename><surname>Marsden</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bongiwe</forename><surname>Dlutu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Senzo</forename><surname>Ntlangano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Azola</forename><surname>Manjingolo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">D</forename><surname>Tucker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matt</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Robinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Elina</forename><surname>Vartiainen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iraklis</forename><forename type="middle">2011</forename><surname>Klampanos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Me</forename><forename type="middle">N U</forename><surname>Call</surname></persName>
		</editor>
		<meeting>the Tenth International Workshop on Internationalisation of Products Systems. Kutching<address><addrLine>New York, NY, USA; St. Petersburg, Russia; Malaysia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="0200">2014. 2014. May 14-16, 2014. ISCA, 194-200</date>
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
	<note>4th Workshop on Spoken Language Technologies for Under-resourced Languages</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Designing Social Media for Community Information Sharing in Rural South Africa</title>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">J</forename><surname>Bidwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elina</forename><surname>Vartiainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masbulele</forename><forename type="middle">Jay</forename><surname>Siya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Reitmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Southern African Institute for Computer Scientists and Information Technologists Annual Conference (SAICSIT &apos;14)</title>
				<meeting>the Southern African Institute for Computer Scientists and Information Technologists Annual Conference (SAICSIT &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="104" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decolonising Speech and Language Technology</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.313</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.coling-main.313" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3504" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving ASR for Code-Switched Speech in Under-Resourced Languages Using Out-of-Domain Data</title>
		<author>
			<persName><forename type="first">Astik</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewald</forename><surname>Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Niesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Febe</forename><surname>De Wet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLTU</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="122" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Making of Dharavi&apos;s &apos;Citysystem</title>
		<author>
			<persName><forename type="first">Jeb</forename><surname>Brugman</surname></persName>
		</author>
		<editor>Dharavi: The City Within, Joseph Campana</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Harper Collins India</publisher>
			<pubPlace>New Delhi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantics-based language modeling for Cantonese-English code-mixing speech recognition</title>
		<author>
			<persName><forename type="first">Houwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu Ting</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 7th International Symposium on Chinese Spoken Language Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garaialde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emer</forename><surname>Gilmartin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Schlögl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Aylett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><surname>Benjamin R Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1093/iwc/iwz016</idno>
		<ptr target="https" />
		<title level="m">The State of Speech in HCI: Trends, Themes and Challenges. Interacting with</title>
				<imprint>
			<date type="published" when="2019-06">2019. June 2019</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="349" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">2021. of AI: Power, and the Planetary Costs of Artifcial Intelligence</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<imprint>
			<publisher>Yale University Press</publisher>
			<pubPlace>New Haven</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language in Public Spaces: Language Choice in Two IsiXhosa Speaking Communities (Langa and Khayelitsha)</title>
		<author>
			<persName><surname>Andiswa Mesatywa Dantile</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Stellenbosch, South Africa</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Stellenbosch</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Izolo: Mobile Diaries of the Less Connected. he Institute of Development Studies</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Indra De Lanerolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alette</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><surname>Schoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Brighton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Ins and Outs of for Development</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 2016 CHI Conference on Human Factors in Computing Systems (CHI &apos;16)</title>
				<meeting>the 2016 CHI Conference on Human Factors in Computing Systems (CHI &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2220-2232. 2858081</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yours Is Better!&quot;: Participant Response Bias in HCI</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidya</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Indrani</forename><surname>Medhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Thies</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208589</idno>
		<ptr target="https://doi.org/10.1145/2207676.2208589" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12)</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sociolinguistics and Mobile Communication</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Deumert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Edinburgh University Press</publisher>
			<pubPlace>Edinburgh</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mobile Language Choices -The Use of English and isiXhosa in Text Messages (SMS) Evidence from a</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Deumert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sibabalwe</surname></persName>
		</author>
		<author>
			<persName><surname>Masinyana</surname></persName>
		</author>
		<idno type="DOI">10.1075/eww.29.2.02deu</idno>
		<ptr target="https://doi.org/10.1075/eww.29.2.02deu" />
	</analytic>
	<monogr>
		<title level="j">Bilingual South African Sample. English World-Wide</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="147" />
			<date type="published" when="2008-04">2008. April 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Technology Adoption by &apos;Emergent&apos; Users: The User-Usage Model</title>
		<author>
			<persName><forename type="first">Anirudha</forename><surname>Devanuj</surname></persName>
		</author>
		<author>
			<persName><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 11th Asia Pacifc Conference on Computer Human Interaction (APCHI &apos;13)</title>
				<meeting>the 11th Asia Pacifc Conference on Computer Human Interaction (APCHI &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="28" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vivek Seshadri, Sunayana Sitaram, Samarth Bharadwaj, Jai Nanavati, Raoul Nanavati, and Karthik Sankaranarayanan. 2021. MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Vaideeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasa</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Unni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiranjeevi</forename><surname>Rajpuria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Yarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanta</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Kumar Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalika</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><surname>Bali</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2021-1339</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2021-1339" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2446-2450</title>
				<meeting>Interspeech 2446-2450</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Rules of Beeping: Exchanging Messages Via Intentional &quot;Missed Calls&quot; on Mobile Phones</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Donner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Mediated Communication</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2007-10">2007. Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">After Access: Inclusion, Development, and a More Mobile Internet</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Donner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Stuf of Bits: An Essay on the Materialities of Information</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dourish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ubicomp&apos;s Colonial Impulse</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dourish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">D</forename><surname>Mainwaring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 2012 ACM Conference on Ubiquitous (UbiComp &apos;12)</title>
				<meeting>the 2012 ACM Conference on Ubiquitous (UbiComp &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Indra&apos;s Net: HCI in the Developing World</title>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">M</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Kotzé</surname></persName>
		</author>
		<idno type="DOI">10.1145/637848.637860</idno>
		<ptr target="https://doi.org/10.1145/637848.637860" />
	</analytic>
	<monogr>
		<title level="j">Interactions</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2003-03-28">2003. March 28-37</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Situating Methods in the Magic of Big Data and AI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Elish</surname></persName>
		</author>
		<author>
			<persName><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.1080/03637751.2017.1375130</idno>
		<ptr target="https://doi.org/10.1080/03637751.2017.1375130" />
	</analytic>
	<monogr>
		<title level="j">Communication Monographs</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="57" to="80" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Why Play? Examining the Roles of Play in ICTD</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Ferreira</surname></persName>
		</author>
		<idno type="DOI">10.7146/aahcc.v1i1.21264</idno>
		<ptr target="https://doi.org/10.7146/aahcc.v1i1.21264" />
	</analytic>
	<monogr>
		<title level="j">Aarhus Series on Human Centered Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015-10">2015. Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Harold</forename><surname>Garfnkel</surname></persName>
		</author>
		<title level="m">Studies in Ethnomethodology</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Polity Press</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text Entry in Indian Languages on Mobile: User Perspectives</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudha</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2676702.2676710</idno>
		<ptr target="https://doi.org/10.1145/2676702.2676710" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the India HCI 2014 Conference on Human Computer Interaction</title>
				<meeting>the India HCI 2014 Conference on Human Computer Interaction<address><addrLine>New Delhi, India; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
	<note>IndiaHCI &apos;14)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Speech-to-Text: Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/speech-to-text" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Gowlett</surname></persName>
		</author>
		<title level="m">Routledge</title>
				<editor>
			<persName><forename type="first">The</forename><surname>Bantu Languages</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Derek</forename><surname>Nurse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gérard</forename><surname>Philippson</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="609" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">They Like to Hear My Voice: Exploring Usage Behavior in Speech-Based Instant Messaging</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gugenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Ole</forename><surname>Rixen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rukzio</forename></persName>
		</author>
		<idno type="DOI">10.1145/3379503.3403561</idno>
		<ptr target="https://doi.org/10.1145/3379503.3403561" />
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;20)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Texture: Human Expression in the Age of Communications Overload</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Harper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambirdge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Role of HCI in the Age of AI</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Harper</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2019.1631527</idno>
		<ptr target="https://doi.org/10.1080/10447318.2019.1631527" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1331" to="1344" />
			<date type="published" when="2019-09">2019. Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The ICT4D 2.0 Manifesto: Where Next for ICTs and International Development? SSRN Scholarly Paper</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Heeks</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3477369</idno>
		<idno>ID 3477369</idno>
		<ptr target="https://doi.org/10.2139/ssrn.3477369" />
	</analytic>
	<monogr>
		<title level="j">Social Science Research Network</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From science to art and back again: the pendulum of an anthropologist</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Ingold</surname></persName>
		</author>
		<idno type="DOI">10.7340/ANUAC2239-625X-2237</idno>
		<ptr target="https://doi.org/10.7340/ANUAC2239-625X-2237" />
	</analytic>
	<monogr>
		<title level="j">Anuac V</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5" to="23" />
			<date type="published" when="2016-08">2016. Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Postcolonial Computing: A Lens on Design and Development</title>
		<author>
			<persName><forename type="first">Lilly</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Vertesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dourish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">E</forename><surname>Grinter</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753522</idno>
		<ptr target="https://doi.org/10.1145/1753326.1753522" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10)</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multilingual transfer of acoustic word embeddings when training on languages related to the target zero-resource language</title>
		<author>
			<persName><forename type="first">Christiaan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12834</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mumbai&apos;s Shadow City</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Jacobson</surname></persName>
		</author>
		<editor>Dharavi: The City Within, Joseph Campana</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Harper Collins India</publisher>
			<pubPlace>New Delhi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Keylekh: a keyboard for text in indic scripts</title>
		<author>
			<persName><forename type="first">Anirudha</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Chand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mathur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;04 extended abstracts on Human factors in computing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="928" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">WhatsApp-Operated Stokvels Promoting Youth Entrepreneurship in Durban, South Africa: Experiences of Young Entrepreneurs</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kariuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizzy</forename><forename type="middle">Oluwatoyin</forename><surname>Ofusori</surname></persName>
		</author>
		<idno type="DOI">10.1145/3047273.3047397</idno>
		<ptr target="https://doi.org/10.1145/3047273.3047397" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the 10th International Conference on Theory and Practice of Electronic Governance (ICEGOV &apos;17). Association for Computing Machinery</title>
				<meeting>the 10th International Conference on Theory and Practice of Electronic Governance (ICEGOV &apos;17). Association for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="253" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Engagement of Pregnant Women and Mothers over WhatsApp: Challenges and Opportunities Involved</title>
		<author>
			<persName><forename type="first">Jasmeet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpendra</forename><surname>Asra Sakeen Wani</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3311957.3359481</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing (CSCW &apos;19). Association for Computing Machinery</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3359481</biblScope>
			<biblScope unit="page" from="236" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Phonology and Morphology of Marathi</title>
		<author>
			<persName><forename type="first">Ashok Ramchandra</forename><surname>Kelkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<pubPlace>Ithaca</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The CSTR System for Multilingual and Code-Switching ASR Challenges for Low Indian Languages</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Electra</forename><surname>Wallington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2021-1035</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2021-1035" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2021. ISCA</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2881" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">WhatFutures: Designing Large-Scale Engagements on WhatsApp</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lambton-Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Garbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Hazeldine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Kharrufa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Nappey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rhythmanalysis: Space, Time and Everyday Life. Bloomsbury</title>
		<author>
			<persName><forename type="first">Henri</forename><surname>Lefebvre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7368" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">the Utopian Vision of Making: HCI After Technosolutionism</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Lindtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaowen</forename><surname>Bardzell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey</forename><surname>Bardzell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858506</idno>
		<ptr target="https://doi.org/10.1145/2858036.2858506" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI &apos;16)</title>
				<meeting>the 2016 CHI Conference on Human Factors in Computing Systems (CHI &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1390" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Testing the Efcacy of an Indic Script Virtual Keyboard: Swarachakra</title>
		<author>
			<persName><forename type="first">Nirav</forename><surname>Malsattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagraj</forename><surname>Emmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshi</forename></persName>
		</author>
		<idno type="DOI">10.1145/2676702.2677203</idno>
		<ptr target="https://doi.org/10.1145/2676702.2677203" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the India HCI 2014 Conference on Human Computer Interaction</title>
				<meeting>the India HCI 2014 Conference on Human Computer Interaction<address><addrLine>New Delhi, India; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="160" to="165" />
		</imprint>
	</monogr>
	<note>IndiaHCI &apos;14)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Analyzing the Use of Audio Messages in WhatsApp Groups</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Maros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jussara</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrício</forename><surname>Benevenuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380070</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380070" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
				<meeting>The Web Conference 2020<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>WWW Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3005" to="3011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The Gift: The Form and Reason for Exchange in Archaic Societies</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Mauss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>W.W. Norton</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Talking about Chat at Work in the Global South: An Ethno-Study of Chat Use in India and Kenya</title>
		<author>
			<persName><forename type="first">Moira</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">J</forename><surname>Bidwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidya</forename><surname>Sarangapani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Appavoo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jacki O'</forename><surname>Neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m">Andries Nel. 2021. South Africa -Languages. Encyclopedia Britannica</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Decolonising the Mind: Politics of Language in African Literature</title>
		<author>
			<persName><surname>Ngug ˜ Ĩ Thiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Zimbabwe Pub. House</publisher>
			<pubPlace>Harare, Zimbabwe</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A South African corpus of multilingual codeswitched soap opera speech</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the Eleventh International Conference on Language Resources and Evaluation</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Placelessness, Spacelessness, and Formlessness: Experiential Qualities of Virtual Possessions</title>
		<author>
			<persName><forename type="first">William</forename><surname>Odom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 2014 Conference on Designing Interactive Systems (DIS &apos;14)</title>
				<meeting>the 2014 Conference on Designing Interactive Systems (DIS &apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><surname>Ong</surname></persName>
		</author>
		<title level="m">Orality and Literacy: The Technologizing of the Word</title>
				<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>reprinted ed.. Routledge</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Diversifying Future-Making Through Itinerative Design</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Reitmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudha</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341727</idno>
		<ptr target="https://doi.org/10.1145/3341727" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2019-07">2019. July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Yarmohammadi</surname></persName>
		</author>
		<author>
			<persName><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1417</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2018-1417" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 3743-3747</title>
				<meeting>Interspeech 3743-3747</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Karamad: A Voice-based Crowdsourcing Platform for Underserved Populations</title>
		<author>
			<persName><forename type="first">Tallal</forename><surname>Shan M Randhawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agha Ali</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Raza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 2021 CHI Conference on Human Factors in Computing Systems. Number 569</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems. Number 569<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rapid Collection of Spontaneous Speech Corpora Using Telephonic Community Forums</title>
		<author>
			<persName><forename type="first">Agha</forename><surname>Ali Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awais</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Randhawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zain</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Bilal</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haris</forename><surname>Bin Zia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umar</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roni</forename><surname>Rosenfeld</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1139</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2018-1139" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018. ISCA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1021" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Dharavi: From Mega-Slum to Urban Paradigm</title>
		<author>
			<persName><forename type="first">Marie-Caroline</forename><surname>Saglio-Yatzimirsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Everyone Wants to Do the Model Work, Not the Data Work&quot;: Data Cascades in High-Stakes AI</title>
		<author>
			<persName><forename type="first">Nithya</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Kapania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Highfll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Akrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><forename type="middle">M</forename><surname>Aroyo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445518</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445518" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the 2021 CHI Conference on Human Factors in Computing Systems (CHI &apos;21)</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems (CHI &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Stefen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<idno>arXiv:1904.05862</idno>
		<title level="m">Wav2vec: Unsupervised Pre-Training for Speech Recognition</title>
				<imprint>
			<date type="published" when="2019-09">2019. Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The Cost of Training NLP Models: A Concise Overview</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08900</idno>
		<imprint>
			<date type="published" when="2020-04">2020. April 2020</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Human-Centered Artifcial Intelligence: Three Fresh Ideas</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIS Transactions on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2020-09">2020. Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName><surname>Khyathi Raghavi Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Rallabandi</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00784</idno>
		<title level="m">A Survey of Code-switched Speech and Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">This Is Not a Boundary Object: Refections on the Origin of a Concept</title>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">Leigh</forename><surname>Star</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology &amp; Human Values</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="601" to="617" />
			<date type="published" when="2010-09">2010. Sept. 2010</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Most Popular Mobile Apps Used in South Africa</title>
		<author>
			<persName><forename type="first">Statista</forename><surname>Research</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Energy and Policy Considerations for Deep Learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<idno>arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019-06">2019. June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Practice-Based Design of Information Systems: Notes from the Hyperdeveloped World. The Information Society</title>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Suchman</surname></persName>
		</author>
		<idno type="DOI">10.1080/01972240290075066</idno>
		<ptr target="https://doi.org/10.1080/01972240290075066" />
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Friction: An Ethnography of Global Connection</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Lowenhaupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsing</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, N.J</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Synthesised bigrams using word embeddings for code-switched ASR of four south african language pairs</title>
		<author>
			<persName><surname>Ewald Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas R Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">ReCall: Crowdsourcing on Basic Phones to Financially Sustain Voice Forums</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Vashistha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Respeak: Voice-Crowd-Powered Speech Transcription System</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Vashistha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. Association for Machinery</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems. Association for Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1855" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">BSpeak: An Accessi-Voice-based Crowdsourcing Marketplace for Low-Income Blind People</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Vashistha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A frst speech recognition system for Mandarin-English code-switch conversational CHI &apos;22</title>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dau-Cheng Jochen</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Blaicher</surname></persName>
		</author>
		<author>
			<persName><surname>Eng-Siong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-04-29">2012. April 29-May 5, 2022</date>
			<biblScope unit="page" from="4889" to="4892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">How Refugees in South Africa Use Mobile Phones for Social Connectedness</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Vuningoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Rosa</forename><surname>Lorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wallace</forename><surname>Chigona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">C&amp;T &apos;21: Proceedof the 10th International Conference on Communities &amp; Technologies -Wicked Problems in the Age of Tech (C&amp;T &apos;21)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="128" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pavement Internet: Mobile Media Economies and Ecologies for Young People in South Africa</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Routledge Companion to Mobile Media</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Goggin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Larissa</forename><surname>Hjorth</surname></persName>
		</editor>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Visual Literacy&apos; as Challenge to the Internationalisation of Interfaces: A Study of South African Student Web Users</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Vukovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marsden</surname></persName>
		</author>
		<idno type="DOI">10.1145/506443.506465</idno>
		<ptr target="https://doi.org/10.1145/506443.506465" />
	</analytic>
	<monogr>
		<title level="m">CHI &apos;02 Extended Abstracts on Human Factors in Computing Systems -CHI &apos;02</title>
				<meeting><address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">530</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Towards end-to-end automatic code-switching speech recognition</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12620</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Code-switched language models using neural based synthetic data from parallel sentences</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08582</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Overcoming Poverty and Inequality in South Africa</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>World Bank</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">On the end-to-end solution to mandarin-english code-switching speech recognition</title>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yerbolat</forename><surname>Khassanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihua</forename><surname>Van Tung Pham</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Eng Siong Chng</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00241</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power</title>
		<author>
			<persName><forename type="first">Shoshana</forename><surname>Zubof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>PublicAfairs</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>frst edition ed.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
