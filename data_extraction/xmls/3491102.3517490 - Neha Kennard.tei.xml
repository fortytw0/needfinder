<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;It Feels Like Taking a Gamble&quot;: Exploring Perceptions, Practices, and Challenges of Using Makeup and Cosmetics for People with Visual Impairments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Franklin</forename><forename type="middle">Mingzhe</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franchesca</forename><surname>Spektor</surname></persName>
							<email>fspektor@andrew.cmu.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Xia</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mina</forename><surname>Huh</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Cederberg</surname></persName>
							<email>pcederbe@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yuqi</forename><surname>Gong</surname></persName>
							<email>yuqigong@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kristen</forename><surname>Shinohara</surname></persName>
							<email>kristen.shinohara@rit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Carrington</surname></persName>
							<email>pcarrington@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Ceder</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">KAIST Pittsburgh</orgName>
								<address>
									<settlement>Pittsburgh, Daejeon</settlement>
									<region>Pennsylvania, Pennsylvania</region>
									<country>USA, USA, Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">KAIST Carnegie Mellon University Carnegie Mellon University</orgName>
								<address>
									<settlement>Daejeon</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Republic of Korea Pittsburgh</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania, Pennsylvania</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Rochester Institute of Technology Rochester</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;It Feels Like Taking a Gamble&quot;: Exploring Perceptions, Practices, and Challenges of Using Makeup and Cosmetics for People with Visual Impairments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3491102.3517490</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-07-22T04:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Makeup</term>
					<term>Cosmetics</term>
					<term>People with Visual Impairments</term>
					<term>Accessibility</term>
					<term>Assistive technology</term>
					<term>Qualitative study</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Makeup and cosmetics ofer the potential for self-expression and the reshaping of social roles for visually impaired people. However, there exist barriers to conducting a beauty regime because of the reliance on visual information and color variances in makeup. We present a content analysis of 145 YouTube videos to demonstrate visually impaired individuals' unique practices before, during, and after doing makeup. Based on the makeup practices, we then conducted semi-structured interviews with 12 visually impaired people to discuss their perceptions of and challenges with the makeup process in more depth. Overall, through our fndings and discussion, we present novel perceptions of makeup from visually impaired individuals (e.g., broader representations of blindness and beauty). The existing challenges provide opportunities for future research to address learning barriers, insufcient feedback, and physical and environmental barriers, making the experience of doing makeup more accessible to people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Human-centered computing → Empirical studies in accessibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>For the estimated 44% of the US population that regularly uses cosmetic products <ref type="bibr" target="#b52">[53]</ref>, a makeup practice constitutes one of the most important avenues toward self-expression and self-care. Despite this fact, there is a near-absent representation of makeup use by people with visual impairments, a worldwide population of at least 2.2 billion <ref type="bibr" target="#b98">[96]</ref>. Makeup practices are equally prevalent and meaningful for people with visual impairments, and embedded in a rich constellation of culture and identity signifers <ref type="bibr" target="#b66">[66]</ref>. "When I frst lost my eyesight, I was quite sad that I couldn't look in the mirror. Applying makeup is a way that I can control my appearance again," said Lucy Edward, CoverGirl's frst blind beauty ambassador <ref type="bibr" target="#b66">[66]</ref>. Although visually impaired people utilize various tips and tricks in makeup application <ref type="bibr" target="#b38">[39]</ref>, there are profound barriers to participating fully in such eforts of self-expression; purchasing, using, and vetting makeup remains inaccessible <ref type="bibr" target="#b79">[79]</ref>. For instance, makeup products have a high reliance on visual information, as many manufacturers do not tactilely diferentiate between colors and formulas in their product lines <ref type="bibr" target="#b82">[81]</ref>. Nonetheless, people with visual impairments are attentive to their appearance in the same proportion as sighted peers, especially in spaces that are guided by social norms around makeup <ref type="bibr" target="#b79">[79]</ref>. With Edward's testimony in mind, we believe there remain many opportunities for research that furthers the self-expression and personal well-being of people with visual impairments (e.g., <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b89">88]</ref>).</p><p>To support people with visual impairments in cosmetics, companies like L'Occitane and Proctor &amp; Gamble have made some progress by attaching braille labels <ref type="bibr" target="#b91">[90]</ref> or non-braille tactile markers <ref type="bibr" target="#b80">[80]</ref> on their cosmetic packages. Furthermore, existing assistive technologies for people with visual impairments in navigation <ref type="bibr" target="#b0">[1]</ref>, text recognition <ref type="bibr" target="#b21">[23]</ref>, object detection <ref type="bibr" target="#b9">[10]</ref>, blind photography <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46]</ref>, and color detection <ref type="bibr" target="#b69">[69]</ref> may be helpful for individuals researching cosmetic products online and in-store, as well as using products independently. Many existing accessible crowdsourcing platforms, such as BeMyEyes <ref type="bibr" target="#b7">[8]</ref> and Aira <ref type="bibr" target="#b1">[2]</ref>, connect people with visual impairments to sighted volunteers, which may provide more detailed feedback and instruction on makeup application <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. While research in this area is growing, assistive technologies are still limited in their scope to functional activities-such as browsing online content (e.g., <ref type="bibr" target="#b57">[57]</ref>), or indoor and outdoor navigation (e.g., <ref type="bibr" target="#b0">[1]</ref>)-rather than expressive activities. For instance, even where assistive technologies can be helpful in diferentiating basic colors, none are currently sophisticated enough to diferentiate tones in a nude eye-shadow palette, much less match to a user's skin color or outft. Because these existing technology approaches do not support users in all important aspects of the task, we frst seek to understand cosmetic practices as an essential mode of expression for people with visual impairments. We then focus on the perceptions and challenges of makeup as an everyday task, which may be enhanced through technological development. As such, the aim of this work is to address the knowledge gap around efectively supporting the expression and independence of people with visual impairments during the makeup process. To achieve this, we explore the following research questions from the perspective of people with visual impairments:</p><p>• RQ1: What are the existing practices around makeup and cosmetics? • RQ2: What is the importance and perception of doing makeup and cosmetics? And why? • RQ3: What are the existing challenges around makeup and cosmetics? And how could HCI research contribute to solving challenges with makeup and cosmetics for people with visual impairments?</p><p>To understand RQ1, we frst conducted a YouTube video analysis with 145 videos relevant to how people with visual impairments do their makeup. We show nuanced information related to unique practices of makeup by people with visual impairments before, during, and after doing makeup (Section 3.2). Based on the makeup practices extracted from the video analysis, we then conducted semistructured interviews with 12 people with visual impairments who have experience with makeup and cosmetics to explore RQ2 and RQ3 in-depth. The interview fndings further illuminate individuals' perceptions of makeup; both the meaning of makeup to people with visual impairments and its role in social interactions (Section 4.2). We then present novel challenges that people with visual impairments are currently facing (i.e., learning barriers, insufcient feedback, physical and environmental barriers) (Section 4.3). We further discuss the design guidelines and potential opportunities in considerations for assistive makeup technology based on makeup meanings and perceptions, and solutions towards novel challenges (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we describe related literature to provide a foundation for our work. First, we present existing literature that explores the meaning of makeup and cosmetics to people with visual impairments (Section 2.1). We then show the enabling and assistive technologies for people with visual impairments that may beneft specifc steps of the makeup process (Section 2.2). In the last section, we describe existing technologies and research for makeup (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Social Meanings of Makeup for People with Visual Impairments</head><p>Makeup and cosmetics contribute to a consumer culture regarding personal reinvention and transformation, forming rituals that allow people to express diferent aspects of their identities <ref type="bibr" target="#b42">[43]</ref>. Makeup may reify gender roles as a rite of passage toward adulthood, or otherwise subvert gender through its applications in queer performance spaces and fashion <ref type="bibr" target="#b44">[45]</ref>. Makeup has also historically functioned as a proxy for professionalism, and upward mobility for female-presenting employees <ref type="bibr" target="#b20">[22]</ref>-social pressures which have been repeatedly highlighted by critical feminist scholarship <ref type="bibr" target="#b83">[82]</ref>.</p><p>In response to critiques of socially-mandated makeup use, many makeup-lovers have recently reclaimed their capacity for personal expression. In the widespread 2015 YouTube movement "The Power of Makeup, " creator NikkieTutorials challenges "makeup shaming" by stating that makeup can instead "transform you into whom you want to be" <ref type="bibr" target="#b48">[49]</ref>. Indeed, individuals may spend signifcant time developing makeup practices to express evolving standards of beauty <ref type="bibr" target="#b72">[72]</ref>, or feel themselves as part of a community <ref type="bibr" target="#b28">[29]</ref>. Diferent cosmetic styles could also assist individuals in changing how their facial appearance is externally perceived <ref type="bibr" target="#b63">[63]</ref>. Throughout these facets of meaning, it is clear that makeup is a socially complex, embodied practice that reaches far beyond the visual gaze.</p><p>For visually impaired individuals, the ways in which makeup furthers identity are equally important. Yet, due to the subjective nature of evaluating one's own visual appearance, conducting a beauty regime may not always be as straightforward as it is for sighted individuals <ref type="bibr" target="#b79">[79]</ref>. Overall, there is a lack of overall understanding of the social complexity of makeup for people with visual impairments, including its implications for self-perception and in-group status. As such, we explore how individuals' makeup practices change according to common social contexts (e.g., employment, shopping <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b89">88]</ref>), including when people with visual impairments might ask sighted people for assistance with diferent aspects of the makeup process <ref type="bibr" target="#b79">[79]</ref>. We ofer this in-depth exploration of makeup-related practices and challenges from the perspective of people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Enabling Technologies for People with Visual Impairments</head><p>Existing accessibility issues in makeup and cosmetics infuence the ways that blind people navigate their makeup purchases and how they use makeup <ref type="bibr" target="#b79">[79]</ref>. In the HCI and Accessibility communities, prior literature explored technologies that assist people with visual impairments in various activities of daily living (e.g., cooking, gaming) <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b62">62]</ref>. Existing research such as web accessibility <ref type="bibr" target="#b97">[95,</ref><ref type="bibr" target="#b101">99]</ref>, media accessibility <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b86">85]</ref>, object recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>, tactile design <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>, color identifcation <ref type="bibr" target="#b74">[74]</ref>, and indoor navigation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b99">97]</ref> may be adapted to improve the makeup process for people with visual impairments. We divide this prior work into three categories-online learning by people with visual impairments, visual assistance for interacting with inaccessible interfaces or environments, and physical and tactile design for visually impaired individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Online</head><p>Learning by People with Visual Impairments. Prior research has explored online accessibility issues and created guidelines for web accessibility. Makeup products are available online both for shopping and browsing, and existing research that focused on making web interfaces accessible <ref type="bibr" target="#b97">[95,</ref><ref type="bibr" target="#b101">99]</ref> would assist people with visual impairments in searching for the makeup product that they are interested in. Beyond searching for makeup products, many people with visual impairments now use online media content and MOOCs (i.e., Massive Open Online Courses) for learning (e.g., <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b86">85]</ref>). In addition to searching and learning from online media content, existing work on supporting people with visual impairments to share their images or videos may further improve the overall experiences of learning makeup styles and products <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b86">85]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Visual Assistance for Interacting with Inaccessible Interfaces</head><p>or Environment. To interact with inaccessible spaces or interfaces, prior research has used computer vision to identify objects in the environment and components in digital interfaces (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b70">70]</ref>). For example, Bigham et al. <ref type="bibr" target="#b9">[10]</ref> leveraged crowd workers to help visually impaired individuals recognize various objects. Guo et al. <ref type="bibr" target="#b30">[31]</ref> introduced Vizlens, an accessible mobile app that supports people with visual impairments to interact with inaccessible interfaces through crowdsourcing and computer vision. Beyond recognizing a specifc object or interacting with interfaces, color identifcation applications (e.g., <ref type="bibr" target="#b74">[74]</ref>) through computer vision could also be enhanced to assist people with visual impairments in choosing and recognizing products with diferent colors (e.g., lipsticks).</p><p>To purchase makeup products, it might be inevitable for people with visual impairments to visit stores (e.g., Sephora) physically. Therefore, prior work on indoor navigation (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b99">97]</ref>) could assist people with visual impairments when navigating from a mall entrance to a specifc makeup store.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Tactile Design for Visually Impaired Individuals. Beyond using mobile technology and computer vision to support people with visual impairments with daily activities, many researchers explored tactile design for people with visual impairments as a more inclusive approach (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b56">56]</ref>). For example, Guo et al. <ref type="bibr" target="#b31">[32]</ref> presented a crowdsourced fabrication pipeline to assist people with visual impairments to create physical labels with 3D printed augmentation of tactile buttons. Furthermore, He et al. <ref type="bibr" target="#b34">[35]</ref> introduced a novel toolchain to create tactile overlays for touchscreens.</p><p>Our research builds upon prior work of enabling technologies for people with visual impairments. Although existing technologies might be benefcial toward aiding people with visual impairments with some makeup and beauty steps (e.g., object recognition), there is a lack of domain-specifc nuance and capabilities to support visually impaired individuals in makeup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Existing Technology for Makeup</head><p>Prior research has explored diferent approaches to reduce the efort of makeup steps through technology, such as providing makeup recommendations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b76">76]</ref>, recording and sharing makeup logs <ref type="bibr" target="#b71">[71]</ref>, improving makeup creativity <ref type="bibr" target="#b95">[93]</ref>, developing instructional makeup videos <ref type="bibr" target="#b96">[94]</ref>, enabling interactive makeup experiences <ref type="bibr" target="#b47">[48]</ref> and supporting marginalized groups in makeup <ref type="bibr" target="#b18">[20]</ref>. For example, Jain and Bhatti <ref type="bibr" target="#b43">[44]</ref> developed a multimodal cosmetic advisory system that leveraged face recognition and color detection to provide recommendations based on skin tones. To support the creativity of makeup processes, Treepong et al. <ref type="bibr" target="#b95">[93]</ref> introduced an interactive face makeup system that combined 3D face modeling, tangible interfaces, projection mapping techniques, and a drawing system that allowed users to interactively design their makeup that enhanced creativity. As additional examples in learning makeup styles, Truong et al. <ref type="bibr" target="#b96">[94]</ref> showed the approach of combining computer vision with transcript text analysis to provide hierarchical tutorials from instructional makeup videos automatically, and Chang et al. <ref type="bibr" target="#b15">[17]</ref> used content-based voice navigation for how-to videos in makeup tutorials. Beyond supporting makeup just for the general public, Chong et al. <ref type="bibr" target="#b18">[20]</ref> created a makeup recommendation system for transgender individuals through automatic facial recognition systems. Although prior research explored technologies for makeup, these applications and solutions are highly reliant on vision. For example, the interactive face makeup system that Treepong et al. <ref type="bibr" target="#b95">[93]</ref> showed would require certain visual capabilities to use, such as positioning the face in the camera's feld of view and placing precise commands in the system. Therefore, there remains work to be done around technologies that beneft people with visual impairments not only in domain-specifc ways, but also through accessible means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">YOUTUBE VIDEO ANALYSIS</head><p>To understand the overall makeup practices for people with visual impairments, we choose to frst perform a content analysis on YouTube videos. We focus on YouTube both as a rich online space for accessibility research <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b60">60]</ref>, and as a hub for a diverse community of makeup users and beauty-focused content creators. Specifcally, many blind YouTubers, such as Molly Burke [13] and Lucy Edwards <ref type="bibr" target="#b24">[25]</ref>, create content around the signifcance of wearing makeup as visually impaired individuals, while also flming tutorials for members of their community. As such, our research in this space allowed us to understand the practices of makeup use directly from an active community of visually impaired creators. Our analysis of YouTube videos on blind makeup provides rich examples of practices that complement and present more diverse and nuanced information that we leverage in our interviews with visually impaired people for in-depth understandings of perceptions and challenges. In this section, we frst show both searching and fltering procedures of the YouTube video analysis (Section 3.1.1). We then describe the coding process and data analysis approach (Section 3.1.2). Finally, we present fndings from our YouTube video analysis (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>3.1.1 Video Searching and Filtering Procedures. To broadly understand the overall makeup practices for people with visual impairments, we used the depth-frst random sampling method to fnd videos relevant to the makeup practices of visually impaired people. Similar to prior research of searching steps (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b61">61]</ref>), we combined vision-related keywords (e.g., blind, visually impaired, visual impairment, low vision) and makeup related keywords (e.g., makeup, cosmetics, beauty)) to serve as our searching keywords. To come up with these, researchers frst started with basic searches (e.g., blind makeup) then gradually added other combinations of keywords from resulting video titles or descriptions (Table <ref type="table" target="#tab_0">1</ref>). Adopting the approach of Komkaite et al. <ref type="bibr" target="#b51">[52]</ref>, we terminated the search with the specifc keyword if the resulting page no longer included any relevant videos. Our initial video dataset covered 160 relevant videos collected before July 14th, 2021. We then fltered out individual videos if: 1) the video did not include any makeup practice nor tips; 2) both the person applying and the person getting makeup did not have visual impairments (e.g., blindfolded); 3) the video had poor audio and video quality; 4) the video was duplicated; 5) the video was not English-based. After the fltering, we ended up with 145 videos (V1 -V145) in the fnal dataset. Among the 145 videos in our dataset, most videos were uploaded in 2019 <ref type="bibr" target="#b34">(35)</ref>, while others were uploaded in 2020 <ref type="bibr" target="#b32">(33)</ref>, 2018 <ref type="bibr" target="#b27">(28)</ref>, 2021 <ref type="bibr" target="#b13">(15)</ref>, 2017 <ref type="bibr" target="#b10">(11)</ref>, 2015 <ref type="bibr" target="#b8">(9)</ref>, 2014 <ref type="bibr" target="#b6">(7)</ref>, 2016 <ref type="bibr" target="#b4">(5)</ref>, 2013 <ref type="bibr" target="#b0">(1)</ref>, and 2012 <ref type="bibr" target="#b0">(1)</ref>. The 145 videos were from 77 YouTube channels with the highest number of videos (9) coming from the "Lucy Edwards" channel <ref type="bibr" target="#b22">[24]</ref>. The average length of videos was 904 seconds (ranging from 60 seconds to 4130 seconds).</p><p>3.1.2 Data Analysis. The video analysis mainly consisted of two steps: open-coding <ref type="bibr" target="#b16">[18]</ref> and afnity diagramming <ref type="bibr" target="#b33">[34]</ref>. To analyze the 145 videos we collected, three researchers frst open-coded <ref type="bibr" target="#b16">[18]</ref> all videos independently. Then the coders met and went through the codes of each video. When there was confusion or confict on any code, the coder explained that code, and then three coders discussed until they reached a consensus then modifed the code. A list of codes was consolidated after the discussion. The same three researchers then performed afnity diagramming <ref type="bibr" target="#b33">[34]</ref> to group the codes into candidate themes and refne the themes in terms of defnition and naming, iteratively. Finally, we generated four themes and 23 codes. We describe and report the fndings based on the four themes in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Findings: Practices of Makeup by People with Visual Impairments</head><p>In this section, we present the practices of makeup by people with visual impairments from YouTube video analysis. We frst present how people learn makeup and available resources. We then describe how visually impaired individuals select and identify makeup products and styles. Afterward, we show the practices often utilized by people with visual impairments in applying makeup. Last, we present how they self-assess their makeup and ask for feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Learning Makeup.</head><p>Throughout the makeup learning process, people with visual impairments often refer to online content developed by other blind people. By watching makeup videos from content creators with visual impairments, they were not only motivated (V45, V101), but also learned about existing blindnessspecifc tips and practices (V21, V45). For instance, V101 mentioned: "[I was] obsessed with makeup when I was a young girl and watched YouTube videos about makeup tutorials. That's where the passions come from. " Makeup videos created by people with visual impairments tended to be more accessible in terms of details in the narration, and most of them explicitly described the product information and the look of the product when it is applied (V34, V35, V51, V66, V92).</p><p>In addition to watching video tutorials from people with similar disabilities, we found that people also prefer learning from people with similar complexion or demographic background to reduce the efort of asking sighted people for feedback and help. In V26, the content creator commented: "I always like to watch the makeup tutorial videos or product reviews from another content creator who has exact skin color as I do. That reduced my concerns of whether a specifc product will look good on my skin color. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Makeup</head><p>Selection and Identification. We found that visually impaired individuals select products with which they can easily discern the amount used during the makeup process. For example, they used products with pump buttons to measure out how much was coming out (V69, V91). In V69, the content creator commented: "By using the pump buttons of my CC cream, it allows me to easily track the exact amount I have applied or need to apply." Another criterion in choosing products was the likelihood of encountering errors due to vision barriers. In order to prevent errors, people with visual impairments preferred tools with short handles that they can control easily to mitigate limitations in depth perception (V11, V58, V65), old dry mascaras that leave fewer clumps (V5, V66, V71), and sponges that leave fewer streaks on the face (V65, V75). Additionally, V89 further mentioned the importance of having long-lasting solutions to reduce the efort of re-application.</p><p>We distilled several strategies that people with visual impairments use to distinguish products they own. First, we found that they leverage the physical shape of the products to diferentiate them (e.g., V34, V51) (Fig. <ref type="figure" target="#fig_0">1(a)</ref>). The creator of V1 mentioned the importance of the unique tactile shape of the product: "Any tactile diferentiation is a game-changer. " For products that lack unique physical shape, people with visual impairments added custom tactile markers (Fig. <ref type="figure" target="#fig_0">1(b</ref>)), such as braille stickers (V4, V6, V22, V30, V64), bump dots (V33, V69, V137), or rubber bands (V2). Some people with visual impairments use the placement to distinguish makeup products. They always lined up tools in order (V12, V15, V34, V107, V136) or put them in diferent compartments of a bag (V26, V39, V40) (Fig. <ref type="figure" target="#fig_0">1(d)</ref>). Additionally, other sensory inputs such as sound and smell were utilized to fnd the right product to compensate the visual impairment. People with visual impairments added audio stickers (Fig. <ref type="figure" target="#fig_0">1(c</ref>)) to label the product with its name and color (V109), or smelled the product to make sure the correct product was being used (V36, V72). V72 further mentioned: "I use smell to distinguish makeup products with diferent scents, especially when they have similar physical packaging, such as lipsticks. "</p><p>We found people with visual impairments prefer asking family members or professionals who know their needs for recommendations that satisfy their personal preferences. For example, they might consult the makeup store staf in fnding the right foundation for sensitive skin and products that are more accessible. V21 commented: "I always have that one person from the store to help me choose the product that is more accessible or easily identifable to me, such as products with diferent shapes or tactile. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Makeup Application.</head><p>We identifed various unique practices from people with visual impairments when applying their makeup. We found visually impaired individuals rely heavily on their hands or fngers as an input rather than various makeup tools. Specifcally, we found they usually frst apply the makeup product (e.g., eye shadow (V7, V42), foundation (V2, V29), lip products (V38, V96)) on their hand or fnger, then apply the makeup on the face. They also use their hand or fnger to distinguish a specifc amount and quantity in makeup. For example, some people store makeup products in the fridge to distinguish the amount of the product by squeezing them on a hand or fnger to feel the temperature diference (e.g., cream (V69)). Furthermore, they often used their palm or fnger to feel the specifc location of the face and guide tools for makeup (e.g., V92, V99, V100) (Fig. <ref type="figure" target="#fig_1">2</ref>). For instance, V56 showed how visually impaired individuals use fngers to feel where the little hairs are under the eyebrow. Additionally, they often clean their fngers or move the products back to the original place after each makeup step to avoid losing track of specifc steps and unwanted mess.</p><p>Besides using a hand or fnger as a guide, people also used specifc tools as guides while applying makeup. For example, we found that they preferred to have tape-or band-aid guidance, such as using tape to create guides on the face for contouring (V19) (Fig. <ref type="figure" target="#fig_1">2</ref>). Moreover, the reaction from specifc tools (e.g., vibration, sound) could provide feedback to visually impaired individuals. One blind content creator commented on her practices of listening to her mascara wand: "that is how I know that I am grabbing my lashes and it is being coated." To ensure more accurate applications in makeup, some visually impaired individuals preferred using mini tools or travel-size kits for precision and accuracy (V15, V31, V61, V116). For example, V61 commented on choosing brushes of small size to reduce the risk of poking eyes.</p><p>During the makeup application process, we learned that blind makeup users often rely on memorization to recall their makeup progress. For some specifc measurement steps, individuals often memorize the number of strokes or how many pumps of makeup to use to control the amount, such as counting the number of strokes (V19), tapping the brush onto the blush for specifc times to pick it up, and memorizing how many pumps of foundation to use. Beyond memorizing the specifc amount, we found that they also use specifc positions on the face as an anchor or reference point during makeup. For example, people with visual impairments memorize scars with fnger-distance from facial parts (V92) and specifc places on her face to stipple (V145). Due to the difculty of memorization, we found that they tend to over-do certain steps in makeup, such as blending (e.g., V8, V89). For example, some blind content creators mentioned that they prefer blending more than sighted people just to make sure it is done (e.g., V4, V30).</p><p>In some cases, visually impaired individuals asked other people for help during makeup processes. We found that they sometimes ask for help with specifc makeup steps, such as eyeliner or mascara that requires more precision and depth perceptions (e.g., V2, V4, V35). The content creator of V35 commented: "I normally would have someone to do eyeliner for me because I cannot do it very well myself. " 3.2.4 Self-assessment and Feedback. While applying and fnishing their makeup, visually impaired individuals explored various approaches for self-assessing and asking for feedback. First of all, we learned that they often use touch and feel to check while doing makeup after specifc steps. For example, V74 mentioned using fngers to feel if the primer is properly applied and blended. For people who can see contours or colors through magnifers, we found they rely on a mirror with magnifer for checking makeup (e.g., V42, V69). V69 commented: "I am legally blind and can see color contours with magnifers, I often use the huge 10x magnifcation mirrors for help with knowing colors and contours while doing makeup and checking after makeup. " As cameras are getting higher resolution and magnifcation, we found that people with visual impairments use their phones to take selfes with specialized zooming tools for checking their makeup (e.g., V41, V56).</p><p>After fnishing the makeup, we found that people with visual impairments tend to have someone to provide feedback on makeup. First, they ask their friends or family members to check their look when done. In the YouTube videos, they also mentioned that they would prefer not only overall feedback of the makeup, but also the detailed pros and cons about their makeup (e.g., V5, V14). In terms of how they received feedback, they mentioned either using Facetime or asking in person. Beyond asking their friends or family for feedback, very few videos mentioned they would use crowd-based apps, such as Be My Eyes or Aira, for feedback (e.g., V2, V30). However, they mentioned that they still preferred more personalized feedback from their friends and family, if they are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEMI-STRUCTURED INTERVIEWS</head><p>Based on the fndings describing makeup practices from the YouTube video analysis (e.g., how people with visual impairments learn makeup. identity makeup product, apply makeup, and check makeup quality) (Section 3.2), we conducted semi-structured interviews with people with visual impairments to further explore perceptions and challenges of makeup, such as their personal perceptions on the meaning of makeup, and challenges that are related to visual information through various makeup procedures. We frst present the methodology of our semi-structured interviews by showing information about participants (Section 4.1.1), study procedures (Section 4.1.2), and data analysis (Section 4.1.3). We then describe our fndings for both the perceptions (Section 4.2) and challenges (Section 4.3) of makeup shared by people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method</head><p>4.1.1 Participants. To understand the perceptions and challenges of applying makeup faced by people with visual impairments, we conducted semi-structured interviews with 12 visually impaired volunteers who have experience with makeup and cosmetics. Although our recruitment form asked for participants of any gender identity, all 12 participants who responded identifed themselves as female, with an average age of 33, ranging from 20 to 48 years old. Four of them are totally blind, and eight are legally blind. Regarding their frst time doing makeup, the distribution is quite even, with half of the participants starting before the loss of vision, and the other half after. Participants were recruited via social platforms (e.g., Reddit, Twitter, Facebook). To participate in our interview, participants had to satisfy the following requirements: 1) be 18 years old or above; 2) have visual impairments; 3) have experience with makeup and cosmetics; 4) be able to communicate in English. The interviews were conducted virtually via Zoom, and each one took around 75 -90 minutes. Participants were compensated with a $20 Amazon gift card for completing the interview. The entire recruitment and interview process was approved by the Institutional Review Board (IRB).</p><p>4.1.2 Study Procedure. In the interview, we frst inquired about participants' demographic information and past experience with cosmetics, such as when and how they frst started applying makeup, obstacles they encountered along the way, and how they dealt with them. Our fndings in the YouTube video analysis have shown different makeup practices before, during, and after doing makeup (e.g., how to distinguish between products (Section 3.2.2), and how do they check their makeup (Section 3.2.4)), but these fndings did not provide much insight into how people with visual impairments got interested in makeup, and how they fgured out what worked best. We asked participants to discuss their perceptions of wearing makeup both for personal meanings and infuence over social interactions (e.g., How does wearing makeup make you feel? What do you think is the role of makeup in society?). Furthermore, we asked participants to share their experiences and any challenges they encounter throughout the entire makeup routine, such as selecting and purchasing products (e.g., Where and how do you purchase makeup products?), learning or following specifc practices (e.g., How do you learn makeup styles?), and checking and correcting a makeup look (e.g., How do you check your makeup after it's done?). Finally, we asked participants to describe how they experimented with new makeup, and how makeup could be used to help them express their creativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Data Analysis.</head><p>The interviews were recorded and transcribed. Two researchers independently performed open-coding <ref type="bibr" target="#b16">[18]</ref> on the interview transcripts. Then the coders met together and discussed their codes. When the coders found a confict, such as a missing code, they explained the rationale for their codes to each other and discussed together to resolve the confict. Eventually, both coders reached a consensus and consolidated the list of codes. After fnishing the coding process, they performed afnity diagramming <ref type="bibr" target="#b33">[34]</ref> to group the codes and summarize emerging themes. Overall, we established six themes and 35 codes. The results introduced in the next section are organized based on our six themes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Findings: Perceptions of Makeup by People with Visual Impairments</head><p>In this section, we demonstrate the importance, perceived benefts, and concerns of makeup from the perspectives of people with visual impairments. We will frst show the perceptions of makeup itself and how that relates to self-expression (Section 4.2.1 -4.2.2). We then describe the perceptions of doing makeup with respect to social interactions from visually impaired individuals (Section 4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Broader Representations of Blindness and Beauty.</head><p>From the interviews, we learned of misperceptions from the general population about the relationship between beauty and blindness. Over We also learned that aesthetic expression is as multifaceted for blind individuals as for sighted peers, and encompasses practices beyond makeup use. For instance, three participants emphasized the importance of looking complete and put together by combining makeup with clothing. However, color and style matching makeup with clothing may also pose additional complexity. P8 commented on this: "...Some people might think doing makeup is already a difcult task for the blind population. However, it is even more complicated because makeup is usually combined with clothes, which added more barriers for us in matching colors and choosing personalized makeup... " Despite a lack of representation for blind aesthetic expression, our interviewees asserted that blindness does not diminish one's access to beauty. P11 further commented: "...I have had experiences when planning to use a new product, and others just reacted like it was not necessary to me. Just because we are blind, it does not mean we cannot make ourselves feel beautiful... " 4.2.2 Relationship between Makeup and Self-image. We learned that there exists both self-confdence and self-consciousness in doing makeup for visually impaired individuals. In terms of selfconfdence, we frst realized there is a positive feedback loop between and confdence for people with visual impairments. Self-confdence is important for visually impaired people in a world of ableist challenges <ref type="bibr" target="#b90">[89]</ref>, fve participants emphasized that makeup can be a confdence booster in their daily lives. P1 commented on her feelings of makeup: "...Makeup makes me feel like a better version of myself. And it is not just more confdent, it also makes me look more alive and awake... " makeup can be confdence enhancing, having increased confdence can also make someone more likely to play with and enjoy their makeup process of self-care (P3, P7, P9). Specifcally, P7 recounted appearance-based bullying throughout her early years, which prevented her from experimenting with makeup (so as not to further stand out). It was not until P7 was able to regain the confdence to be expressive her appearance that she began gravitating toward bright makeup. Glitter and blue lipsticks became a source of joy and creativity for her-regardless of others' perspectives. P3 further explained that compliments from other people encouraged her to do more makeup, and she thought of makeup as a way to put her best foot forward because she knows she looks good: "...Every time when I hear from other people talking about how beautiful I am with my makeup, I become more excited and confdent in trying new makeup styles and willing to spend more time and money on it!... "</p><p>In contrast, six participants also mentioned feeling self-conscious about makeup, which discourages them from doing makeup or makes them become more conservative in trying new things. Three interviewees are resistant to being creative in makeup due to concerns of error. P5 commented on this fact: "...I usually just stick to the same color and same techniques I know. I think less is more, and I just want to be comfortable in makeup processes. The consequences of doing new makeup wrong are way more than just following simple makeup routines... "</p><p>We further learned that two participants were often cautious at the beginning and afraid of messing things up. P6 also followed up on the discouragement from the complexity of makeup for people with visual impairments: "...There is so much discouragement in makeup. So many steps of makeup are frustrating, especially when I just started to learn makeup. It is also confusing with the tools, the steps and also diferent makeup products... " Finally, we learned that these challenges might be further exacerbated by broader misperceptions of blind makeup use. Our participants expressed that they felt that society attaches ableist assumptions to less-than-perfect makeup from an individual with visual impairments. For example, P8 commented on how imperfect makeup signifes her blindness, rather than a rushed morning: "...When I do my makeup, it needs to be great, because if it is not, it will signify my blindness and give me a lot of pressure. That is why I tried to do things simple and natural... "</p><p>This additional pressure may cause some individuals to remain more conservative in their makeup use, so as to avoid ableist assumptions <ref type="bibr" target="#b12">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Relationship between makeup and social interaction.</head><p>In the previous section, we noted how social perceptions infuenced how and when participants used makeup (Section 4.2.2). We also found that participants' makeup played a role in their day-to-day social interactions. In this section, we discuss how visually impaired individuals leverage makeup to control their visibility in sighted settings and engage in the community. We also consider how familiarized surroundings afect one's motivation toward makeup.</p><p>Controlling Visibility: Our participants mentioned makeup could enable them to adjust their visibility in social interactions. "Makeup helps to control how visible I am!" said P12. P12 further explained that having a disability often makes her overtly visible to others, such as when wearing sunglasses indoors or using a cane. In contrast to fndings mentioned in Section 4.2.2, makeup also disrupts assumptions about what blind people do. As such, makeup is one especially important way she is able to change her visibility in diferent contexts: "...Makeup can help to control how I would like to present myself in front of others. For example, if I am happy today and would like to socialize more at a party, I would put on more colorful makeup. Otherwise, I would keep a more natural-looking, and it will be less noticeable... "</p><p>Two also commented on hesitating to change their makeup or going out without makeup to prevent unwanted attention. P2 explained: "...I normally keep my appearance the same as before the loss of vision. This makes people focus less on my face and my visual impairments... "</p><p>As a specifc example of changing visibility, fve participants described how wearing makeup allows them to be perceived as professional under diferent contexts. P10 emphasized the beneft of makeup in making her feel more professional: "...I have a babyface, and it sometimes causes concerns under some professional contexts, such as interviews. I always wear makeup to feel more professional and confdent during formal interviews and presentations... " these participants, we learn that makeup is often one way in which visually impaired individuals may modulate how much attention they receive on account of their blindness. Some individuals choose to shun attention, while others use makeup to harness positive attention, such as during a job interview.</p><p>Community &amp; Belonging: In our interviews, nine participants mentioned that makeup becomes a way to connect visually impaired individuals with people, and to form friendships with women. P3 commented: "...Makeup is a big thing for women, I can use makeup to show other people my feelings for each day, and it can become a social medium between people and me, both sighted and visually impaired populations... " P4 further mentioned her experiences of exchanging diferent makeup products and exploring makeup styles with her friends: "...I often exchange my makeup products with my friends, which defnitely helps our friendship. We read magazines together and even try new products on each others' faces... " P1 talked about how makeup allows her to connect to her sighted female friends and teach them new tips as a makeup enthusiast. P8 further mentioned using makeup to guard and enforce her femininity in social interactions, such as putting on a powerful full-face makeup look during a breast cancer checkup-a moment where her womanhood threatened.</p><p>Importance of Support &amp; Motivation: we found that the support and motivation from familiarized surroundings are important to visually impaired makeup users. We found that the social pressure of makeup is highly related to the surrounding environments. For example, P9 is a student in computer science, and she mentioned that the limited number of women in her feld made her not that interested or motivated to do makeup every day. P10 also mentioned how pressure from social media made her feel unmotivated: "...The pressure of doing makeup starts at a young age, around middle school. Social media often has perfect makeup, and it becomes an expectation for us. However, this feels like a lot of pressure because of the complexity of for people with visual impairments... "</p><p>In addition to social environments, we found that family attitudes afect how people with visual impairments apply makeup. Four participants mentioned that their family environments either motivated or discouraged them from doing makeup. While P7's conservative family upbringing caused her to come to love makeup later in life, P11's cousin worked at MAC <ref type="bibr" target="#b68">[68]</ref> and taught her a lot of makeup skills when she was a teenager.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Findings: Challenges of Makeup by People with Visual Impairments</head><p>We now key challenges that are currently faced by people with visual impairments, which provide novel opportunities to researchers-learning barriers, insufcient feedback, and physical and environmental barriers.</p><p>4.3.1 Learning Barriers. We previously mentioned that people with visual impairments prefer using online video tutorials to learn new practices and products (Section 3.2.1). However, they also mentioned that they highly rely on friends and professionals for recommendations and feedback due to the limitation of online resources (Section 3.2.2). From the interviews, we uncovered various lenges and opportunities to improve video content to better support people with visual impairments in makeup. Most complaints with the video tutorials were about the lack of descriptive data in makeup videos, and content creators often assumed their audiences did not have visual impairments Beyond YouTube videos from blind content creators, our participants also complained about the missing detailed information online on how to apply the makeup product step by step. They proposed online information should include every detailed instruction that a person might get from actually talking with a makeup specialist from a physical store. P1 explained: "...It should include everything you may imagine you can get from actually talking with a specialist, such as whether it is great for day or night, how long does it last... "</p><p>Other than the needs of descriptions, our interviewees also mentioned the inaccessible and irrelevant content from online makeup product pages, such as moving images (P10) and information overloading from too many ads (P9). Therefore, mentioned that she avoids buying makeup products online, unless she is already familiar with the product. At last, we also learned that online reviews or recommendations lack specifcation and customization. Six participants mentioned the inconvenience and efort of exploring and reviews one by one for makeup products and the lack of specifcations of reviewer's demographic information, such as skin type and skin tone. P6 further explained her experiences of going over reviews recommendations: "...I like to use online web pages for browsing makeup products these days during the pandemic. However, many of the reviews and recommendations systems do not take my personal information into consideration, such as whether my skin is sensitive? Whether I have a dark skin tone? In addition, many online makeup platforms do not require people to include their skin type or skin tone when they leave a comment on the product review page. This made me frustrated because I do not know whether I should listen to the person or not. " Four participants emphasized the need for tactile makeup learning materials for people with visual impairments. This includes having makeup magazines with tactile versions and instructions on using new tools for people visual impairments. P10 commented on this: "...Many people now still prefer reading beauty magazines on physical covers. However, I barely see any of them having tactile versions available. Again, new instructions on how to apply new tools are still described to sighted populations. It would be nice of makeup companies to include instructions on how blind people may use the makeup tools... " the efort of learning and practicing makeup, we found re-learning makeup after the loss of vision is challenging because people have to abandon their conventional ways of measuring makeup, applying makeup, and performing self-assessment with vision assistance. Also, doing makeup usually requires a long time to practice and adopt a preferred and style. In our interview, we had six participants who learned makeup before they experienced loss. For example, P11 expressed difculties of learning new ways of doing makeup after becoming legally blind and how that is diferent from before the vision loss: "...Applying makeup was a daily routine for me before the loss of vision, I became legally blind three years ago, and it took me a lot of efort from knowing what makeup products are more accessible, which makeup tools are easier for me to use...Of course, it needs a lot of practice and exploration... " Moreover, all six participants who learned makeup before the vision loss emphasized the difculties of maintaining their previous makeup styles right after the vision loss to prevent unwanted attention of the makeup change from the general public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Insuficient Feedback.</head><p>From the interview, we further uncovered existing challenges of having feedback-color identifcation before makeup, tracking specifc steps during makeup, and overall feedback after makeup. We found visually impaired individuals not only have a hard time doing color matching before applying makeup, but also fnd it difcult to acquire color information through product descriptions, especially in online content. For example, both P5 and P10 mentioned that there is a lack of standards for describing colors, "Nobody understands what the color 'moonlight' refers to!" said P5. P10 further commented on this: "...I wish it could be better for explaining colors. I want more descriptive information. For example, the defnition of 'neutral tone' might be diferent between makeup products. "</p><p>From the interview, six participants mentioned the difculties of identifying color independently. All participants mentioned that they had used color identifcation apps before. However, four participants complained about the technical limitations of recognizing the detailed description of colors from color identifcation apps, including exact color code and color shade. P12 commented on this: "...I have used diferent color identifcation apps. However, none of them can tell me the detailed color information, I understand the light condition might afect the exposure of the phone camera, I would like the color identifcation app to tell me more information the color shade or exact RGB value... "</p><p>Beyond lacking a standardized way of describing colors, two participants mentioned the problem of detecting a group of colors by using color identifcation apps, such as colors on a palette. P11 further elaborates on this: "...Many color identifcation apps can detect single color from the camera. However, I found it is difcult to recognize colors on my palette. It also needs better ways describing a of colors through audio... "</p><p>In addition, eight participants also expressed strong frustration about color blending. We found that people with visual impairments often have difculties in blending several making sure the color blending is even. For example, P1 and P5 mentioned that it is hard for them to follow a specifc order of diferent colors with an exact amount. P9 further explained the of color blending evenly: "...Blending colors on a color pan is already very hard. However, doing makeup often requires me to blend colors on my face. This made me frustrated sometimes because it is very hard to make sure I blended both sides of my face evenly... "</p><p>Besides the challenges with color matching and blending, our participants also mentioned the difculties of following makeup steps or having precise control over tools, especially for mascara (P4, P5, P11) and eyeliner (P2, P4, P6, P7, P10, P11, P12). P12 complained about the process of applying eyeliners: "There are both eyeliner pencils and liquid eyeliners available. However, both ways are extremely hard for me, I am always afraid of having ink in my eyes while using the liquid eyeliner and worried about sticking the eyeliner pencil to my eyes. It is so hard to tell where the eyeliner goes. " applying makeup, our participants also mentioned the diffculty of tracking makeup status and memorizing specifc steps, which includes the number of swipes that have been applied on each face (P2, P4, P5) and the layout of makeup products (P5). P5 further commented: "...For blush, I have to exactly count how many times I applied on each face to make sure they are even, I also have to be consistent with the starting and ending point...While using my palette, I have to memorize the layout of all the colors... " Due to the difculties of following and memorizing specifc steps, we found some participants avoided doing makeup or following specifc makeup steps due to the efort. The most common problem comes from the time consumption of makeup, which includes learning (P1, P5), practicing (P5, P6), and actually applying the makeup in a daily routine (P3). P5 commented on this concern: "...Makeup is not just a simple application. It requires to spend a lot of time to both learn and do your own practices again and again before you can actually wear it to go outside... " interview fndings corroborated YouTube video analysis about the roles of other people in providing useful feedback about how makeup looked (Section 3.2.2). Through the interview with 12 participants with visual impairments, we learned that getting feedback through electronic devices can be problematic when the person they want to ask for feedback is not physically around. First of all, three participants mentioned that getting feedback through video calls can be difcult. P1 explained her difculties of doing FaceTime with her mom for feedback due to the camera's feld of view and light conditions: "...When my husband and my mom are not around, I have to call them through FaceTime to get feedback on my makeup. I usually have a hard time zooming in and out for a specifc part of my face. And the light condition afects the automatic exposure of my camera which makes my face look a slightly diferent color... "</p><p>Beyond asking the person they know for feedback, our participants also mentioned that they leverage crowd-based apps, such as Aira <ref type="bibr" target="#b1">[2]</ref> or BeMyEyes <ref type="bibr" target="#b7">[8]</ref>, for feedback. However, they mentioned the difculty of fnding the right person who is qualifed and trustworthy for makeup feedback. For example, P9 complained that she experienced unknowledgeable volunteers jumping in and providing dishonest feedback to her while using BeMyEyes: "...I have experienced that a random person came without any background knowledge or even vocabulary of makeup while I use BeMyEyes for help. I wish they could pre-flter some people based on my needs... " Furthermore, our participants also mentioned that it is difcult to receive immediate feedback from the crowd-based systems. Three participants commented on the need for having an automatic makeup feedback system that can provide specifc and personalized feedback to people with visual impairments. P2 further elaborated on this: "...I wish there were that can alert me if my lipstick goes way out, and can just let me know my makeup looks OK today... "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Physical and Environmental</head><p>Barriers. In addition to learning barriers and insufcient feedback, there are also physical and environmental barriers, such as the lack accessible product design, limited technology, and the complexity of diferent contexts (e.g., makeup shopping aisles). Seven participants complained about the lack of accessible product design, which includes unreadable product descriptions, lack of tactile features on the package, and uniform shape of diferent products. P10 commented the lenges of acquiring product information from packaging: "...I understand makeup packages are really condensed in terms of the product information. Some products are not even readable through OCR. I wish there could be more approaches, such as having a QR code, to help me get the product information easier... " P5 further describes the tactile and shape design of products: "...Currently, very few products have tactile dots or markers. We rely on the shape design of makeup products to recognize specifc products. However, existing makeup products, such as lipsticks, the same product with diferent colors all have the same shape and tactile design... " We realized that existing technology (e.g., color identifer apps, object recognition apps) is limited for makeup by people with visual impairments, because makeup usually requires precision adaptability of complex steps. For example, P3 mentioned that her apps stopped working sometimes, and it is cumbersome to use one technology for a specifc makeup step, and additional technologies for other makeup steps. P4 further commented that the of technology forced her to rely on low tech or no tech solutions: "...There are so many limitations of the current nologies. It may just not work under a specifc context, or it may break down at any point. This is why I prefer low-tech or no-tech solutions... " Moreover, we learned that the complexity of diferent contexts also limits the makeup experiences to people with visual impairments, especially for shopping experiences. For instance, our participants complained about the inaccessible experiences of exploring beauty products in drug due to the lack of in-person support (P7, P12), and the layout of beauty aisles is too complicated for people with visual impairments (P2, P8). P8 further explained: "...I always try to avoid in-store shopping for makeup products, especially for stores like Target, I cannot just wander up, and down the beauty aisle, all the products are so close to each other, and sometimes the specifc product I want is locked... " Finally, participants had difculty modifying makeup when they were outside, such as re-applying lipsticks after dinner or adding more eye shadow or mascara (P2, P3). P3 commented on this: "...I often have a hard time doing makeup modifcations outside of my place. It often requires me to go to the bathroom and ask another lady for help. I wish I had a small magnifcation mirror with me all the time... "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>From the interview fndings, we describe existing perceptions how people visual impairments think about the meaning of makeup and how it afects social interactions. We also showed under-explored challenges that are currently faced by visually impaired individuals when doing makeup. Based on these fndings, we further discuss the design considerations and potential opportunities for assistive makeup technology based on makeup meanings and perceptions of makeup and the unique challenges faced by people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Considerations for Assistive Makeup Technology Based on Makeup Meanings and Perceptions</head><p>Critical disability scholar Alison Kafer argues that there is a "persistent and pervasive that disabled people's of technology are more assistive than creative" <ref type="bibr" target="#b46">[47]</ref>. We have seen this in the relative lack of beauty-related assistive technologies (e.g., problems of detecting a group of colors) (Section 4.3.2), which has been further maintained by historically sparse mainstream representation <ref type="bibr" target="#b35">[36]</ref>. However, as disability beauty and makeup use is becoming increasingly visible <ref type="bibr" target="#b67">[67]</ref>, we see new possibilities for assistive technologies to celebrate and uphold personal expression (Section 4.2.2). Many assistive technologies are still often evaluated on the basis of their "ability to move bodies and minds into heightened productivity, efciency, normalcy, and speed" <ref type="bibr" target="#b46">[47]</ref> rather than how well they attend to the emotional experiences of their users (Section 4.2.2). Drawing from our interviews, we frst recognize that beauty practices can be emotionally impactful practice for many blind individuals, intersecting ableism, femininity, belonging, and self-image <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b25">26]</ref>. As we see the need for technologies that take into consideration the emotional and social factors of makeup use which may be uniquely exacerbated for blind individuals. Below, we ofer a set of questions to help developers build accessible makeup tools toward pleasure, expression, and community support: Is the experience of using a tool calming and confdencebuilding? We know from our interview data that makeup has a strong and enduring relationship with self-image (Section 4.2.2). To help users build confdence, consider using product design to ofer a calm and luxurious experience and allow users the opportunity to go over to train up their skills <ref type="bibr" target="#b94">[92]</ref>. To cut down on overwhelm, automatic suggestions could take the form of capsule makeup kits-neutral starter packs <ref type="bibr" target="#b5">[6]</ref> with basics that individuals can be assured will work for them in any combination.</p><p>Does the tool account for social context and personal style? We heard from several users about the importance of holistic style, the combination of makeup, clothing, hair, and accessories. This requires that tools be sensitive to a user's context <ref type="bibr" target="#b29">[30]</ref>, as well as the various elements of their look. Consider crowdsourcing styles that are contextually embedded <ref type="bibr" target="#b92">[91]</ref>: a smokey eye guide for a night out with friends, a tool that matches lipstick shades to one's accessories, or a trend-inspired clothing and makeup palette.</p><p>Does the tool make a user feel included in a community or social group? It is important to account for makeup as a community practice (Section 4.2.3), in addition to individual practice. Because blind individuals rely on friends or infuencers of a similar age, skin color, and ethnicity to vet their makeup (Section 3.2.1), a tool could also allow individuals to connect to one another remotely by sharing images and asking for feedback from friends, etc. Consider also how a tool could be used by multiple users, such as a group of friends who are all getting ready together.</p><p>Is the tool culturally competent and accounts for unique practices by visually impaired people? While there may exist as many application styles as there are users, we have distilled several common practices that may be unique to blind makeup users. Tools can be sensitive to common problem areas such as symmetry, help users count of the number of strokes, or suggest application tips like keeping products cold to help measure the amount used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improve Support for Better Makeup Learning Experiences</head><p>From our fndings of both YouTube analysis and interviews, we illuminated that people with visual impairments highly rely on online content for learning purposes, and there exist various learning barriers that are unique to makeup tutorial videos (e.g., overuse of demonstrative pronouns) (Section 4.3.1). We also found that people with visual impairments prefer using online content made by content creators who also have visual impairments. Our research further extends existing research of enabling visually impaired people content creation, such as photo sharing on social media <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b102">100]</ref>, video editing on YouTube <ref type="bibr" target="#b85">[84,</ref><ref type="bibr" target="#b87">86,</ref><ref type="bibr" target="#b88">87]</ref>, and music composing <ref type="bibr" target="#b78">[78]</ref>. For example, leveraging personal-aesthetic recognition and contextual saliency detection <ref type="bibr" target="#b102">[100]</ref> for makeup-related content would reduce the efort of content creators with visual impairments in editing their photos or videos. Moreover, in video editing by content creators with visual impairments, makeup tutorial videos will require more advanced editing <ref type="bibr" target="#b87">[86]</ref> to enhance certain products or processes. This would further require more research and development into supporting visually impaired content creators with more specialized editing instructions. We also learned that there exist challenges for people with visual impairments to understand video tutorial content for makeup (Section 4.3.1), such as the lack of descriptive data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b100">98]</ref> in makeup tutorial videos and overuse of demonstrative pronouns in videos. Specifcally for makeup tutorial videos, our participants mentioned that detailed applications and product information are essential for their purposes of watching them (e.g., brand, volume, where and how to (Section 4.3.1), which has diferent focuses comparing with general videos. For example, a general video may just need audio descriptions such as "A person is applying mascara." However, makeup video tutorials would need detailed descriptions such as "The person is holding a [brand][product], and apply [product rather than "this"] from the [distance] to [specifc location rather "that"] for [number] times." Therefore, we suggest future research on designing accessible makeup videos by creating a database <ref type="bibr" target="#b102">[100]</ref> to store the detailed information and leverage human-in-the-loop approaches <ref type="bibr" target="#b100">[98]</ref> to obtain specifc answers to questions from visually impaired individuals as an to the video stream synchronously. This database would need to contain not only product-specifc keywords, but also domain information relevant to a visually impaired audience, such as describing a mascara wand's shape. Furthermore, the database could also leverage demographic information from content creators (e.g., age, skin type, skin color) <ref type="bibr" target="#b3">[4]</ref> to provide more specialized answers to questions and recommendations to visually impaired makeup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Improve Provisions for Makeup Feedback</head><p>our study, we showed existing challenges of insufcient feedback before, during, and after doing makeup (Section According to prior research in blind photography <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>, future research could explore various "feedback" modes <ref type="bibr" target="#b45">[46]</ref> that are specifcally for makeup purposes, such as the number of colors presented in the camera's feld of view, light exposure, camera focus, and face area occupied in the photo. This would support people with visual impairments in taking a themselves to send to their friends or family members for better identifcation personalized feedback.</p><p>Furthermore, we also learned that individuals with visual impairments have to use BeMyEyes <ref type="bibr" target="#b7">[8]</ref> or Aira <ref type="bibr" target="#b1">[2]</ref> for assistance while their family or friends are not around (Section 4.3.2). However, our participants mentioned that it is often the case that the volunteers from these services are often not qualifed to provide sufcient feedback or that using such services is time-consuming to be practical (Section 4.3.2). To reduce the efort of providing feedback, future research should consider creating an automatic feedback system that leverages computer vision <ref type="bibr" target="#b40">[41]</ref> or other approaches to provide instructional systems that allow people visual impairments to follow along their hands are occupied, 2) feedback systems that check the overall quality of the makeup to make sure it would not cause unwanted attention, and 3) feedback systems that provide detailed recommendations on makeup revision and guidelines based on examples provided by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Create Workarounds for Physical Barriers</head><p>From our interview results, we showed that there exist signifcant barriers to accessing products by people with visual impairments, such as lack of awareness between beauty and disability from the general public, design of uniformly shaped products, inaccessible product descriptions, and complexity of context (Section 4.3.3). To enable easier object detection and information retrieval by people visual impairments, future product design could leverage RFID <ref type="bibr" target="#b4">[5]</ref> or QR code <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b58">58]</ref> as assistance to reduce the efort of using OCR to scan the product information on makeup packages (Section 4.3.3). Upon having QR codes on product packaging, future research could also leverage augmented reality on mobile devices with audio feedback to assist people with visual impairments in exploring the beauty aisle in stores (e.g., <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b36">37]</ref>). Exploring the beauty aisle can be more complex than a clear ground space for augmented reality. This would further bring more consideration of providing additional contextual factors and having more detailed contextual descriptions <ref type="bibr" target="#b36">[37]</ref> in beauty-specifc applications.</p><p>Beyond easier recognition and navigation through assistive technologies, there are more opportunities for fabrication research to contribute in building accessible makeup add-ons <ref type="bibr" target="#b53">[54]</ref> and other tactile design <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>, which further improve the accessibility of doing makeup. For instance, future research could also leverage crowdsourcing and computer vision to provide automated 3D printing to help people with visual impairments to organize and label their makeup products under a reasonable cost <ref type="bibr" target="#b31">[32]</ref>. Finally, we acknowledge many barriers that consumer-focused technologies alone may not be able to solve. For this reason, we simultaneously encourage product manufacturers to ease the burden on individuals by leveraging diferentiated shapes and embossing to create tactile-accessible products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND FUTURE WORK</head><p>All of the participants we recruited for the interview were either legally blind or totally blind. However, we agree that people with low vision might have diferent challenges in makeup (e.g., more often using magnifers or other accessible tools). Furthermore, while all participants who volunteered to our study happened to identify themselves as women, we would be interested in research that accounts for the diferent preferences, practices, and challenges held by visually impaired participants of other genders. Finally, we only analyzed the practices through YouTube videos that are based in English, but there might exist other makeup practices that were spoken in other languages and happen across diferent cultures. Therefore, we are also interested in further discovering how cultural diferences afect people with visual impairments in makeup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we frst describe the results of our content analysis with 145 YouTube videos featuring visually impaired individuals doing makeup and illuminate unique makeup practices for people with visual impairments (e.g., over-do certain steps in makeup). We further show the fndings from semi-structured interviews with 12 visually impaired participants and highlight the perceptions (e.g., personal meaning and social implications) and existing challenges (e.g., difculties in blending several colors, precise application, and acquiring feedback) of makeup from their perspectives. We then discuss considerations and potential opportunities for assistive makeup technology based on makeup meanings and perceptions, including supporting better makeup learning experiences, providing makeup feedback to people with visual impairments, and creating workarounds to overcome physical and environmental barriers. Overall, our fndings and discussion shed light on opportunities for future research and development of assistive technologies to empower people with visual impairments in makeup and cosmetics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diferent ways of distinguishing products. (a) Brushes with diferent handle shapes. (b) Eyeshadow pans customized with braille labels. (c) Customized audio stickers on an eyeshadow palette. (d) A person shows various products organized in diferent compartments of a bag.</figDesc><graphic url="image-2.png" coords="5,53.80,502.42,240.24,130.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: A person uses her hand to guide a brow gel applicator onto their eyebrow. Right: A person uses painter's tape on their cheekbone to help apply contouring.</figDesc><graphic url="image-3.png" coords="5,317.96,581.66,240.21,73.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The searching keywords used in the YouTube video analysis. A depth-frst random sampling method was used to fnd videos relevant to makeup practices of visually impaired people.</figDesc><table><row><cell>Searching Keywords</cell></row><row><cell>Blind Makeup, Blind Cosmetics, Blind Beauty, Blind GRWM,</cell></row><row><cell>Blind Makeup Tutorial, Blind Beauty Step-by-step, Visually</cell></row><row><cell>Impaired Makeup, Visually Impaired Beauty, Visually Impaired</cell></row><row><cell>GRWM, Visually Impaired Makeup Tutorial, Vision impairment</cell></row><row><cell>Makeup Tutorial, Low Vision Makeup</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Participants' demographic informationParticipant Age Gender Vision Impairment Description Learned Doing Makeup Before or After Vision Loss participants reported that they found sighted people assume blind people do not "need" makeup, so it is less mentioned or taught at home if none of their family members have visual impairments. According to what P1 and P9 commented, it is not common to see a blind person wearing makeup on social media in general, which creates the stereotype blind people do not wear makeup. This perspective continues to inform the sparse representation of blind beauty rituals in both media and research, and directly contributes to the lack of accessible tools. P4 further commented on this issue:</figDesc><table><row><cell>P1</cell><cell>39</cell><cell cols="2">Female Totally Blind, Acquired (23 yrs)</cell><cell>Before</cell></row><row><cell>P2</cell><cell>20</cell><cell>Female</cell><cell>Legally Blind, Acquired (2 yrs)</cell><cell>Before</cell></row><row><cell>P3</cell><cell>24</cell><cell>Female</cell><cell>Legally Blind, Congenital</cell><cell>After</cell></row><row><cell>P4</cell><cell>48</cell><cell>Female</cell><cell>Totally Blind, Acquired (6 yrs)</cell><cell>Before</cell></row><row><cell>P5</cell><cell>26</cell><cell>Female</cell><cell>Legally Blind, Acquired (9 yrs)</cell><cell>Before</cell></row><row><cell>P6</cell><cell>46</cell><cell cols="2">Female Totally Blind, Acquired (38 yrs)</cell><cell>After</cell></row><row><cell>P7</cell><cell>42</cell><cell>Female</cell><cell>Legally Blind, Congenital</cell><cell>After</cell></row><row><cell>P8</cell><cell>28</cell><cell>Female</cell><cell>Totally Blind, Congenital</cell><cell>After</cell></row><row><cell>P9</cell><cell>22</cell><cell>Female</cell><cell>Legally Blind, Congenital</cell><cell>After</cell></row><row><cell>P10</cell><cell>27</cell><cell cols="2">Female Legally Blind, Acquired (26 yrs)</cell><cell>After</cell></row><row><cell>P11</cell><cell>33</cell><cell>Female</cell><cell>Legally Blind, Acquired (3 yrs)</cell><cell>Before</cell></row><row><cell>P12</cell><cell>43</cell><cell cols="2">Female Legally Blind, Acquired (10 yrs)</cell><cell>Before</cell></row><row><cell>half of our</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>"...Many people that I talked with who are sighted were very surprised that I do makeup. Even my family were not aware of how important doing makeup is in my social life... "</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">'22, April 29-May 5, 2022, New Orleans, LA, USA Li et al.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Yunzhi Li, Frank Elavsky, JiWoong Jang, Michael Xieyang Liu, Zhen Li, Jiannan Li, Yan Chen for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NavCog: a navigational cognitive assistant for the blind</title>
		<author>
			<persName><forename type="first">Dragan</forename><surname>Ahmetovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cole</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironobu</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieko</forename><surname>Asakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services</title>
				<meeting>the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Home -Aira : Aira</title>
		<author>
			<persName><surname>Aira</surname></persName>
		</author>
		<ptr target="https://aira.io/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Utilizing QR code and mobile phones for blinds and visually impaired people. International Conference on Computers for Handicapped Persons</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hend</surname></persName>
		</author>
		<author>
			<persName><surname>Al-Khalifa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1065" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rule-based facial makeup recommendation system</title>
		<author>
			<persName><forename type="first">Taleb</forename><surname>Alashkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="325" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indoor navigational aid using active RFID and QR-code for sighted and blind people</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Saleh Alghamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Van Schyndel</surname></persName>
		</author>
		<author>
			<persName><surname>Alahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How to Build the Ultimate Capsule Makeup Collection | Real Simple</title>
		<ptr target="https://www.realsimple.com/beauty-fashion/makeup/(Accessedon12/20/2021" />
		<editor>Yelena Moroz Alpert. 2021</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing user-generated youtube videos to understand touchscreen use by people with motor impairments</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoojin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Findlater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
				<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1223" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Be My Eyes -See the world together</title>
		<author>
			<persName><surname>Bemyeyes</surname></persName>
		</author>
		<ptr target="https://www.bemyeyes.com/.(Accessedon06/14/2021" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How teens with visual impairments take, edit, and share photos on social media</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cynthia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">E</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith Ringel</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI conference on human factors in computing systems</title>
				<meeting>the 2018 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vizwiz: nearly real-time answers to visual questions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jefrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandrika</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjie</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aubrey</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandyn</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samual</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd annual ACM symposium on User interface software and technology</title>
				<meeting>the 23nd annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optical-to-tactile image conversion for the blind</title>
		<author>
			<persName><forename type="first">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Katcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond P</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Man-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="1970">1970. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual challenges in the everyday lives of blind people</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
				<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Contours of ableism: The production of disability and abledness</title>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Campbell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audiobased puzzle gaming for blind people</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Guerreiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><surname>Carriço</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Mobile Accessibility Workshop at MobileHCI (MOBACC)</title>
				<meeting>the Mobile Accessibility Workshop at MobileHCI (MOBACC)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strange beauty: Aesthetic possibilities for desiring disability into the future</title>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Ignagni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interdisciplinary Approaches to Disability. Routledge</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RubySlippers: Supporting Content-based Voice Navigation for How-to Videos</title>
		<author>
			<persName><forename type="first">Minsuk</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Constructing grounded theory: A practical guide through qualitative analysis. sage</title>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Charmaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding objects for blind people based on SURF features</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Chincha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops</title>
				<imprint>
			<publisher>BIBMW). IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="526" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring a Makeup Support System for Transgender Passing based on Automatic Gender Recognition</title>
		<author>
			<persName><forename type="first">Toby</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolwenn</forename><surname>Maudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuki</forename><surname>Harima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Igarashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AR4VI: AR as an accessibility tool for people with visual impairments</title>
		<author>
			<persName><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><surname>Miele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="288" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Makeup at work: Negotiating appearance rules in the workplace</title>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Dellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gender &amp; Society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="151" to="177" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real time text detection and recognition on hand held objects to assist blind people</title>
		<author>
			<persName><forename type="first">Samruddhi</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Revati</forename><surname>Shriram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1020" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Edwards</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m">YouTube. channel/UChhMknOcAZOQXC1rh0ler-Q</title>
				<imprint/>
	</monogr>
	<note>Accessed on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disability Justice and Beauty as a Liberatory Practice</title>
		<author>
			<persName><forename type="first">Mordecai</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WSQ: Women&apos;s Studies Quarterly</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="237" to="240" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using computer vision to access appliance displays</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Fusco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ender</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international ACM SIGACCESS conference on Computers &amp; accessibility</title>
				<meeting>the 16th international ACM SIGACCESS conference on Computers &amp; accessibility</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="281" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards computer-vision software tools to increase production and accessibility of video description for people with vision loss</title>
		<author>
			<persName><forename type="first">Langis</forename><surname>Gagnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Foucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maguelonne</forename><surname>Heritier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Byrns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Chapdelaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Laurendeau</surname></persName>
		</author>
		<author>
			<persName><surname>Nath Tan Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Universal Access in the Information Society</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The practice of using makeup: A consumption ritual of adolescent girls</title>
		<author>
			<persName><forename type="first">Elodie</forename><surname>Gentina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Hélène Fosse-Gomez</forename><surname>Kay M Palan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Behaviour</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="115" to="123" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06072</idno>
		<title level="m">Mukesh Mohania, and Ashish Kundu. 2021. BeautifAI-A Personalised Occasion-oriented Makeup Recommendation System</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vizlens: A robust and interactive screen reader for interfaces in the real world</title>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Anthony' Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieko</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual symposium on user interface software and technology</title>
				<meeting>the 29th annual symposium on user interface software and technology</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="651" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facade: Auto-generating tactile interfaces to appliances</title>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeeeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Anthony' Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Mankof</surname></persName>
		</author>
		<author>
			<persName><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5826" to="5838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3608" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The UX Book: Process and guidelines for ensuring a quality user experience</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Hartson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pardha</surname></persName>
		</author>
		<author>
			<persName><surname>Pyla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TacTILE: a preliminary toolchain for creating accessible graphics with 3D-printed overlays and auditory annotations</title>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Liang He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">E</forename><surname>Findlater</surname></persName>
		</author>
		<author>
			<persName><surname>Froehlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<meeting>the 19th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="397" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Locating the bodies of women and disability in defnitions of beauty: An analysis Dove&apos;s campaign for real beauty</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Heiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Disability Studies Quarterly</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Making mobile augmented reality applications accessible</title>
		<author>
			<persName><forename type="first">Jaylin</forename><surname>Herskovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accessible maps the blind: Comparing 3D printed models with tactile graphics</title>
		<author>
			<persName><forename type="first">Leona</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 chi conference on human factors in computing systems</title>
				<meeting>the 2018 chi conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Blind Girl Make-up Tips and Tricks -Life of a Blind Girl</title>
		<author>
			<persName><surname>Holly</surname></persName>
		</author>
		<ptr target="https://lifeofablindgirl.com/2018/07/29/blind-girl-make-up-tips-and-tricks/.(Accessedon09/07/2021" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fabrication, 3D Printing, and Making</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Accessibility</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="755" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An interactive virtual mirror to support makeup for visually impaired persons</title>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Ishikiriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object recognition for blind people based on features extraction</title>
		<author>
			<persName><forename type="first">Hanen</forename><surname>Jabnoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faouzi</forename><surname>Benzarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Amiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Image Processing, Applications and Systems Conference. IEEE</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Rise Of Disability Friendly Beauty Brands | British Vogue</title>
		<author>
			<persName><forename type="first">Lottie</forename><surname>Jackson</surname></persName>
		</author>
		<ptr target="https://www.vogue.co.uk/beauty/article/the-rise-of-disability-friendly-beauty-brands" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Accessed on 06/14/2021</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imaging-based cosmetics advisory service</title>
		<author>
			<persName><forename type="first">Jhilmil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Bhatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;09 Extended Abstracts on Human Factors in Computing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3781" to="3786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Role of Pop Culture in Popularizing Gender-Bending Fashion and Ideals of Beauty and Makeup</title>
		<author>
			<persName><forename type="first">Jaanvi</forename><surname>Jairath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhea</forename><surname>Daima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>n.d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Supporting blind photography</title>
		<author>
			<persName><forename type="first">Chandrika</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Crip kin, manifesting. Catalyst: Feminism, theory, technoscience</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Kafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chromoskin: Towards interactive cosmetics using thermochromic pigments</title>
		<author>
			<persName><forename type="first">Hsin-Liu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manisha</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Schmandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katia</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</title>
				<meeting>the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3703" to="3706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring YouTube as a transformative tool in the &quot;The Power of MAKEUP</title>
		<author>
			<persName><forename type="first">Ümit</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face recognition assistant for people with visual impairments</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Kianpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franklin</forename><forename type="middle">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khai N</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Providing synthesized audio description for online videos</title>
		<author>
			<persName><forename type="first">Masatomo</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentarou</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironobu</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieko</forename><surname>Asakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international ACM SIGACCESS conference on Computers and accessibility</title>
				<meeting>the 11th international ACM SIGACCESS conference on Computers and accessibility</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="249" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Underneath the Skin: An Analysis of YouTube Videos to Understand Insertable Device Interaction</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Komkaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liga</forename><surname>Lavrinovica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Vraka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><forename type="middle">B</forename><surname>Skov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Why women use makeup: Implication of psychological</title>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Korichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delphine</forename><surname>Pelle-De Queral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cosmetic science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kohl</forename><surname>Kreatives</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kohl</forename><surname>Kreatives</surname></persName>
		</author>
		<ptr target="https://www.kohlkreatives.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The accessibility of MOOCs for blind learners</title>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Królak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Norun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siri</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><surname>Kessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<meeting>the 19th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="401" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TDraw: a computer-based tactile drawing tool for blind people</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kurze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second annual ACM conference on Assistive technologies</title>
				<meeting>the second annual ACM conference on Assistive technologies</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Investigating the accessibility and usability of job application web sites for blind users</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abiodun</forename><surname>Olalere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Wentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Usability Studies</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="87" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">FMT: A wearable camera-based object tracking memory aid for older adults</title>
		<author>
			<persName><forename type="first">Franklin Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><forename type="middle">Laura</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khai N</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">I Choose Assistive Devices That Save My Face&quot; A Study on Perceptions of Accessibility and Assistive Technology Use Conducted in China</title>
		<author>
			<persName><forename type="first">Franklin Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><forename type="middle">Laura</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khai N</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Non-Visual Cooking: Exploring Practices and Challenges of Meal Preparation by People with Visual Impairments</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Franklin Mingzhe Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Cederberg</surname></persName>
		</author>
		<author>
			<persName><surname>Carrington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An Exploration of Captioning Practices and Challenges of Individual Content Creators on YouTube for People with Hearing Impairments</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Franklin Mingzhe Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khai N</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A gesturebased text input method for people with visual impairments</title>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khai N</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<meeting>the 19th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Strategy narratives and wellbeing challenges: The role of everyday self-presentation</title>
		<author>
			<persName><forename type="first">Chihling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debbie</forename><forename type="middle">Isobel</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">K</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="234" to="243" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Wow! you are so beautiful today! ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">What Makes Videos Accessible to Blind and Visually People</title>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Anthony' Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<ptr target="https://www.allure.com/story/blind-women-beauty-industry-tactile-packaging-for-visually-impaired" />
		<title level="m">Blindness &amp; Beauty: How Visually Impaired Women Are Changing the Industry | Allure</title>
				<imprint>
			<date type="published" when="2019">April Long. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Disability Representation in the Beauty Industry -Digital Beauty</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Mabry</surname></persName>
		</author>
		<ptr target="https://digitalbeauty.com/disability-representation-in-the-beauty-industry/.(Accessedon12/20/2021" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<ptr target="https://www.maccosmetics.com/products/13854/products/makeup/lips/lipstick.(Accessedon08/02/2021" />
		<title level="m">MAC Lipsticks | MAC Cosmetics -Ofcial Site</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards a natural user interface to support people with visual impairments in detecting colors</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Mascetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gerino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bernareggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Picinali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers Helping People with Special Needs</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Clearspeech: A display reader for the visually handicapped</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Blenkhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Crossey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="492" to="500" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Smart makeup system: supporting makeup using lifelog sharing</title>
		<author>
			<persName><forename type="first">Maki</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itiro</forename><surname>Siio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on Ubiquitous computing</title>
				<meeting>the 13th international conference on Ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="483" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cosmetics: They infuence more than Caucasian female facial attractiveness</title>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>George Fieldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hussey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Luc</forename><surname>Lévêque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied social psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="493" to="504" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">ViScene: A Collaborative Authoring Tool for Scene Descriptions in Videos</title>
		<author>
			<persName><forename type="first">Rosiana</forename><surname>Natalie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebrima</forename><surname>Jarjue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kotaro</forename><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">ColorADD: color identifcation system for color-blind people. In Injuries and Health Problems in Football</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Neiva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Smart mirror: Intelligent makeup recommendation and synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
				<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Beauty emakeup: A deep makeup transfer system</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hefei</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
				<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="701" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rescribe: Authoring and Automatically Editing Audio Descriptions</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jefrey P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 33rd Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="747" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">How Blind and Visually Impaired Composers, Producers, and Songwriters Leverage and Adapt Music Technology</title>
		<author>
			<persName><forename type="first">William</forename><surname>Christopher Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Yixuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabiha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Inclusive beauty: how buying and using cosmetics can be made more accessible for the visually impaired (VI) and blind consumer</title>
		<author>
			<persName><forename type="first">Akriti</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriela</forename><surname>Daniels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cosmetics and Toiletries</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="M4" to="M15" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Inclusive design: Procter &amp; Gamble updates Herbal Essences shampoo bottles for blind customers -Quartz</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Quito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">updatesherbal-essences-shampoo-bottles-for-blind-customers</title>
		<idno>08/10/2021</idno>
		<ptr target="https://qz.com/quartzy/1418531/inclusive-design-procter-gamble-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">The Best and Worst Beauty Packaging for Blind People, According to Molly Burke -Watch Video | Allure</title>
		<author>
			<persName><forename type="first">Marci</forename><surname>Robin</surname></persName>
		</author>
		<ptr target="https://www.allure.com/story/molly-burke-universal-design-beauty-products-blind-accessibility" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Accessed on 09/07/2021</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Foucault, femininity, and the modernization of patriarchal power</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Bartky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Boston: Northeastern UP</title>
		<editor>Refections on Resistance. Ed. Irene Diamond and Lee Quinby</editor>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Navcog3: An evaluation of a smartphone-based blind indoor navigation assistant with semantic features in a large-scale environment</title>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uran</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kakuya</forename><surname>Naito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironobu</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieko</forename><surname>Asakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 19th International ACM SIGACCESS Conference on and Accessibility</title>
				<meeting>19th International ACM SIGACCESS Conference on and Accessibility</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Exploring the community of blind or visually impaired people on youtube</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunggu</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<meeting>the 19th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="371" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Understanding blind or visually impaired people on youtube through qualitative analysis of videos</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunggu</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM International Conference on Interactive Experiences for TV and Online Video</title>
				<meeting>the 2018 ACM International Conference on Interactive Experiences for TV and Online Video</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Understanding the community of blind or impaired vloggers on YouTube. Universal Access in the Information Society</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunggu</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Challenges and opportunities to improve the accessibility of for people with visual impairments as content Universal Access in the Information Society</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunggu</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">In the shadow of misperception: assistive technology use and social interactions</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><surname>Wobbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
				<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Self-conscious or self-confdent? A diary study conceptualizing the social accessibility of assistive technology</title>
		<author>
			<persName><forename type="first">Kristen Shinohara Jacob O</forename><surname>Wobbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing (TACCESS)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Beauty is designing packaging for the visually impaired | Vogue Business</title>
		<author>
			<persName><forename type="first">Arabelle</forename><surname>Sicardi</surname></persName>
		</author>
		<ptr target="https://www.voguebusiness.com/beauty/braille-beauty-&apos;22" />
		<imprint>
			<date type="published" when="2019-04-29">2019. April 29-May 5, 2022</date>
			<pubPlace>New Orleans, LA, USA packaging-loccitane</pubPlace>
		</imprint>
	</monogr>
	<note>Accessed on 06/14/2021</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Tilbury</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<ptr target="https://www.charlottetilbury.com/uk/secrets/category/how-tos/occasion.(Ac-cessedon12/20/2021" />
		<title level="m">Makeup Looks For All Occasions | Charlotte Tilbury</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Paint a Better Mood? Efects of Makeup Use on YouTube Beauty Infuencers&apos; Self-Esteem</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Copes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAGE Open</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2158244020933591</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Makeup Creativity Enhancement with an Augmented Reality Face Makeup System</title>
		<author>
			<persName><forename type="first">Bantita</forename><surname>Treepong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironori</forename><surname>Mitake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoichi</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Entertainment (CIE)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Automatic Generation of Two-Level Hierarchical Tutorials from Instructional Makeup Videos</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peggy</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users</title>
		<author>
			<persName><forename type="first">Ruolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingrui</forename><forename type="middle">Ray</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang'anthony'</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Blindness and vision impairment</title>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Pray before you step out&quot; describing personal and situational blind navigation behaviors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><forename type="middle">K</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><surname>Kane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<meeting>the 15th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Human-in-the-Loop Machine Learning to Increase Video Accessibility for Visually Impaired and Blind Users</title>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Beste F Yuksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umang</forename><surname>Fazli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishali</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo</forename><forename type="middle">Jung</forename><surname>Bisht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">Junhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Jung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue-Ting</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilmi</forename><surname>Miele</surname></persName>
		</author>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM Designing Interactive Systems Conference</title>
				<meeting>the 2020 ACM Designing Interactive Systems Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="47" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A web navigation tool for the blind</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Zajicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international ACM conference on Assistive technologies</title>
				<meeting>the third international ACM conference on Assistive technologies</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="204" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">The efect of computer-generated descriptions on photo-sharing experiences of people with visual impairments</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaomei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsay</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiri</forename><surname>Azenkot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017">2017. 2017</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
